{
  "hash": "8d9e1f0d352e9ebe0cccef9898042803",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"T-Tests\"\nsubtitle: A Year 1 Textbook Chapter\nformat:\n  html: \n    code-tools: true\n    code-fold: true\n    code-annotations: hover\n    title-block-banner: true\n\nfilters: \n  - webr\n  \nexecute: \n  warning: false\n---\n\n\n\n\n\n# Chapter: T-Tests in R\n\n::: Content below origially written by Matthew Crump under a CC-BY-NC-SA\nLicence. Original Content Chapter 6 of\nhttps://www.crumplab.com/statistics/06-ttests.html :::\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\nBack in the day, William Sealy Gosset got a job working for Guinness\nBreweries. They make the famous Irish stout called Guinness. What\nhappens next went something like this (total fabrication, but mostly on\npoint).\n\nGuinness wanted all of their beers to be the best beers. No mistakes, no\nbad beers. They wanted to improve their quality control so that when\nGuinness was poured anywhere in the world, it would always comes out\nfantastic: 5 stars out of 5 every time, the best.\n\nGuinness had some beer tasters, who were super-experts. Every time they\ntasted a Guinness from the factory that wasn't 5 out of 5, they knew\nright away.\n\nBut, Guinness had a big problem. They would make a keg of beer, and they\nwould want to know if every single pint that would come out would be a 5\nout of 5. So, the beer tasters drank pint after pint out of the keg,\nuntil it was gone. Some kegs were all 5 out of 5s. Some weren't,\nGuinness needed to fix that. But, the biggest problem was that, after\nthe testing, there **was no beer left to sell**, the testers drank it\nall (remember I'm making this part up to illustrate a point, they\nprobably still had beer left to sell).\n\nGuinness had a sampling and population problem. They wanted to know that\nthe entire population of the beers they made were all 5 out of 5 stars.\nBut, if they sampled the entire population, they would drink all of\ntheir beer, and wouldn't have any left to sell.\n\nEnter William Sealy Gosset. Gosset figured out the solution to the\nproblem. He asked questions like this:\n\n1.  How many samples do I need to take to know the whole population is 5\n    out of 5?\n\n2.  What's the fewest amount of samples I need to take to know the\n    above, that would mean Guinness could test fewer beers for quality,\n    sell more beers for profit, and make the product testing time\n    shorter.\n\nGosset solved those questions, and he invented something called the\n*Student's t-test*. Gosset was working for Guinness, and could be fired\nfor releasing trade-secrets that he invented (the t-test). But, Gosset\npublished the work anyways, under a pseudonym [@Student1908]. He called\nhimself Student, hence Student's t-test. Now you know the rest of the\nstory.\n\nIt turns out this was a very nice thing for Gosset to have done. t-tests\nare used all the time, and they are useful, that's why they are used. In\nthis chapter we learn how they work.\n\nYou'll be surprised to learn that what we've already talked about, (the\nCrump Test, and the Randomization Test), are both very very similar to\nthe t-test. So, in general, you have already been thinking about the\nthings you need to think about to understand t-tests. You're probably\nwondering what is this $t$, what does $t$ mean? We will tell you. Before\nwe tell what it means, we first tell you about one more idea.\n\n## Check your confidence in your mean\n\nWe've talked about getting a sample of data. We know we can find the\nmean, we know we can find the standard deviation. We know we can look at\nthe data in a histogram. These are all useful things to do for us to\nlearn something about the properties of our data.\n\nYou might be thinking of the mean and standard deviation as very\ndifferent things that we would not put together. The mean is about\ncentral tendency (where most of the data is), and the standard deviation\nis about variance (where most of the data isn't). Yes, they are\ndifferent things, but we can use them together to create useful new\nthings.\n\nWhat if I told you my sample mean was 50, and I told you nothing else\nabout my sample. Would you be confident that most of the numbers were\nnear 50? Would you wonder if there was a lot of variability in the\nsample, and many of the numbers were very different from 50. You should\nwonder all of those things. The mean alone, just by itself, doesn't tell\nyou anything about well the mean represents all of the numbers in the\nsample.\n\nIt could be a representative number, when the standard deviation is very\nsmall, and all the numbers are close to 50. It could be a\nnon-representative number, when the standard deviation is large, and\nmany of the numbers are not near 50. You need to know the standard\ndeviation in order to be confident in how well the mean represents the\ndata.\n\nHow can we put the mean and the standard deviation together, to give us\na new number that tells us about confidence in the mean?\n\nWe can do this using a ratio:\n\n$\\frac{mean}{\\text{standard deviation}}$\n\nThink about what happens here. We are dividing a number by a number.\nLook at what happens:\n\n$\\frac{number}{\\text{same number}} = 1$\n\n$\\frac{number}{\\text{smaller number}} = \\text{big number}$\n\ncompared to:\n\n$\\frac{number}{\\text{bigger number}} = \\text{smaller number}$\n\nImagine we have a mean of 50, and a truly small standard deviation of 1.\nWhat do we get with our formula?\n\n$\\frac{50}{1} = 50$\n\nImagine we have a mean of 50, and a big standard deviation of 100. What\ndo we get with our formula?\n\n$\\frac{50}{100} = 0.5$\n\nNotice, when we have a mean paired with a small standard deviation, our\nformula gives us a big number, like 50. When we have a mean paired with\na large standard deviation, our formula gives us a small number, like\n0.5. These numbers can tell us something about confidence in our mean,\nin a general way. We can be 50 confident in our mean in the first case,\nand only 0.5 (not at a lot) confident in the second case.\n\nWhat did we do here? We created a descriptive statistic by dividing the\nmean by the standard deviation. And, we have a sense of how to interpret\nthis number, when it's big we're more confident that the mean represents\nall of the numbers, when it's small we are less confident. This is a\nuseful kind of number, a ratio between what we think about our sample\n(the mean), and the variability in our sample (the standard deviation).\nGet used to this idea. Almost everything that follows in this textbook\nis based on this kind of ratio. We will see that our ratio turns into\ndifferent kinds of \"statistics\", and the ratios will look like this in\ngeneral:\n\n$\\text{name of statistic} = \\frac{\\text{measure of what we know}}{\\text{measure of what we don't know}}$\n\nor, to say it using different words:\n\n$\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}$\n\nIn fact, this is the general formula for the t-test. Big surprise!\n\n## One-sample t-test: A new t-test\n\nNow we are ready to talk about t-test. We will talk about three of them.\nWe start with the one-sample t-test.\n\nCommonly, the one-sample t-test is used to estimate the chances that\nyour sample came from a particular population. Specifically, you might\nwant to know whether the mean that you found from your sample, could\nhave come from a particular population having a particular mean.\n\nStraight away, the one-sample t-test becomes a little confusing (and I\nhaven't even described it yet). Officially, it uses known parameters\nfrom the population, like the mean of the population and the standard\ndeviation of the population. However, most times you don't know those\nparameters of the population! So, you have to estimate them from your\nsample. Remember from the chapters on descriptive statistics and\nsampling, our sample mean is an unbiased estimate of the population\nmean. And, our sample standard deviation (the one where we divide by\nn-1) is an unbiased estimate of the population standard deviation. When\nGosset developed the t-test, he recognized that he could use these\nestimates from his samples, to make the t-test. Here is the formula for\nthe one sample t-test, we first use words, and then become more\nspecific:\n\n### Formulas for one-sample t-test\n\n$\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}$\n\n$\\text{t} = \\frac{\\text{measure of effect}}{\\text{measure of error}}$\n\n$\\text{t} = \\frac{\\text{Mean difference}}{\\text{standard error}}$\n\n$\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}}$\n\n$\\text{t} = \\frac{\\text{Sample Mean  - Population Mean}}{\\text{Sample Standard Error}}$\n\n$\\text{Estimated Standard Error} = \\text{Standard Error of Sample} = \\frac{s}{\\sqrt{N}}$\n\nWhere, s is the sample standard deviation.\n\nSome of you may have gone cross-eyed looking at all of this. Remember,\nwe've seen it before when we divided our mean by the standard deviation\nin the first bit. The t-test is just a measure of a sample mean, divided\nby the standard error of the sample mean. That is it.\n\n### What does t represent?\n\n$t$ gives us a measure of confidence, just like our previous ratio for\ndividing the mean by a standard deviations. The only difference with\n$t$, is that we divide by the standard error of mean (remember, this is\nalso a standard deviation, it is the standard deviation of the sampling\ndistribution of the mean)\n\n::: callout-note\nWhat does the t in t-test stand for? Apparently nothing. Gosset\noriginally labelled it z. And, Fisher later called it t, perhaps because\nt comes after s, which is often used for the sample standard deviation.\n:::\n\n$t$ is a property of the data that you collect. You compute it with a\nsample mean, and a sample standard error (there's one more thing in the\none-sample formula, the population mean, which we get to in a moment).\nThis is why we call $t$, a sample-statistic. It's a statistic we compute\nfrom the sample.\n\nWhat kinds of numbers should we expect to find for these $ts$? How could\nwe figure that out?\n\nLet's start small and work through some examples. Imagine your sample\nmean is 5. You want to know if it came from a population that also has a\nmean of 5. In this case, what would $t$ be? It would be zero: we first\nsubtract the sample mean from the population mean, $5-5=0$. Because the\nnumerator is 0, $t$ will be zero. So, $t$ = 0, occurs, when there is no\ndifference.\n\nLet's say you take another sample, do you think the mean will be 5 every\ntime, probably not. Let's say the mean is 6. So, what can $t$ be here?\nIt will be a positive number, because 6-5= +1. But, will $t$ be +1? That\ndepends on the standard error of the sample. If the standard error of\nthe sample is 1, then $t$ could be 1, because 1/1 = 1.\n\nIf the sample standard error is smaller than 1, what happens to $t$? It\nget's bigger right? For example, 1 divided by 0.5 = 2. If the sample\nstandard error was 0.5, $t$ would be 2. And, what could we do with this\ninformation? Well, it be like a measure of confidence. As $t$ get's\nbigger we could be more confident in the mean difference we are\nmeasuring.\n\nCan $t$ be smaller than 1? Sure, it can. If the sample standard error is\nbig, say like 2, then $t$ will be smaller than one (in our case), e.g.,\n1/2 = .5. The direction of the difference between the sample mean and\npopulation mean, can also make the $t$ become negative. What if our\nsample mean was 4. Well, then $t$ will be negative, because the mean\ndifference in the numerator will be negative, and the number in the\nbottom (denominator) will always be positive (remember why, it's the\nstandard error, computed from the sample standard deviation, which is\nalways positive because of the squaring that we did.).\n\nSo, that is some intuitions about what the kinds of values t can take.\n$t$ can be positive or negative, and big or small.\n\nLet's do one more thing to build our intuitions about what $t$ can look\nlike. How about we sample some numbers and then measure the sample mean\n**and** the standard error of the mean, and then plot those two things\nagainst each each. This will show us how a sample mean typically varies\nwith respect to the standard error of the mean.\n\nIn @fig-6sampleMSEM, I pulled 1,000 samples of $N = 10$ from a normal\ndistribution (mean = 0, sd = 1). Each time I measured the mean and\nstandard error of the sample. That gave two descriptive statistics for\neach sample, letting us plot each sample as dot in a scatter plot.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_mean <- length(1000)\nsample_se <- length(1000)\n\nfor (i in 1:1000) {\n  s <- rnorm(10, 0, 1)\n  sample_mean[i] <- mean(s)\n  sample_se[i] <- sd(s) / sqrt(length(s))\n}\n\nplot(sample_mean, sample_se)\n```\n\n::: {.cell-output-display}\n![A scatter plot with sample mean on the x-axis, and standard error of the mean on the y-axis](chapter_files/figure-html/fig-6sampleMSEM-1.png){#fig-6sampleMSEM width=672}\n:::\n:::\n\n\n\n\n\n```{webr-r}\nsample_mean <- length(1000)\nsample_se <- length(1000)\n\nfor (i in 1:1000) {\n  s <- rnorm(10, 0, 1)\n  sample_mean[i] <- mean(s)\n  sample_se[i] <- sd(s) / sqrt(length(s))\n}\n\nplot(sample_mean, sample_se)\n```\n\n```{webr-r}\n# Initialize vectors to store the sample means and standard errors, each of length 1000\nsample_mean <- numeric(1000)  # creates a numeric vector of length 1000 filled with zeros\nsample_se <- numeric(1000)    # creates a numeric vector of length 1000 filled with zeros\n\n# Run a loop 1000 times to generate samples, calculate their means, and standard errors\nfor (i in 1:1000) { # <1>\n  # Generate a sample of 10 random numbers from a normal distribution with mean 0 and standard deviation 1\n  s <- rnorm(10, 0, 1) # <2>\n  \n  # Calculate the mean of the sample and store it in the ith position of the sample_mean vector\n  sample_mean[i] <- mean(s)\n  \n  # Calculate the standard error of the sample mean and store it in the ith position of the sample_se vector\n  sample_se[i] <- sd(s) / sqrt(length(s))\n}\n\n# Plot the sample means against their corresponding standard errors\nplot(sample_mean, sample_se)\n\n```\n\n1.  Take `penguins`, and then,\n2.  add new columns for the bill ratio and bill area.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins |>                                      # <1>\n  mutate(                                        # <2>\n    bill_ratio = bill_depth_mm / bill_length_mm, # <2>\n    bill_area  = bill_depth_mm * bill_length_mm  # <2>\n  )                                              # <2>\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 344 × 10\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 4 more variables: sex <fct>, year <int>, bill_ratio <dbl>, bill_area <dbl>\n```\n\n\n:::\n:::\n\n\n\n\n\n1.  Take `penguins`, and then,\n2.  add new columns for the bill ratio and bill area.\n\n\n\n\n\n````{=html}\n<!-- ```{r}\nlibrary(countdown)\n\ncountdown(minutes = 0, seconds = 15)\n``` -->\n````\n\n\n\n\n\n::: to-webr\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins |>                                      # <1>\n  mutate(                                        # <2>\n    bill_ratio = bill_depth_mm / bill_length_mm, # <2>\n    bill_area  = bill_depth_mm * bill_length_mm  # <2>\n  )         \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 344 × 10\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 4 more variables: sex <fct>, year <int>, bill_ratio <dbl>, bill_area <dbl>\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\nWhat we get is a cloud of dots. You might notice the cloud has a\ncircular quality. There's more dots in the middle, and fewer dots as\nthey radiate out from the middle. The dot cloud shows us the general\nrange of the sample mean, for example most of the dots are in between -1\nand 1. Similarly, the range for the sample standard error is roughly\nbetween .2 and .5. Remember, each dot represents one sample.\n\nWe can look at the same data a different way. For example, rather than\nusing a scatter plot, we can divide the mean for each dot by the\nstandard error for each dot. @fig-6thisexample shows the result in a\nhistogram.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(sample_mean/sample_se, breaks=30)\n```\n\n::: {.cell-output-display}\n![A histogram of the sample means divided by the sample standard errors, this is a t-distribution.](chapter_files/figure-html/fig-6thisexample-1.png){#fig-6thisexample width=672}\n:::\n:::\n\n\n\n\n\nInteresting, we can see the histogram is shaped like a normal curve. It\nis centered on 0, which is the most common value. As values become more\nextreme, they become less common. If you remember, our formula for $t$,\nwas the mean divided by the standard error of the mean. That's what we\ndid here. This histogram is showing you a $t$-distribution.\n\n### Calculating t from data\n\nLet's briefly calculate a t-value from a small sample. Let's say we had\n10 students do a true/false quiz with 5 questions on it. There's a 50%\nchance of getting each answer correct.\n\nEvery student completes the 5 questions, we grade them, and then we find\ntheir performance (mean percent correct). What we want to know is\nwhether the students were guessing. If they were all guessing, then the\nsample mean should be about 50%, it shouldn't be different from chance,\nwhich is 50%. Let's look at @tbl-tsmall.\n\n\n\n\n\n::: {#tbl-tsmall .cell tbl-cap='Calculating the t-value for a one-sample test.'}\n::: {.cell-output-display}\n\n\n|students |scores |mean |Difference_from_Mean |Squared_Deviations |\n|:--------|:------|:----|:--------------------|:------------------|\n|1        |50     |61   |-11                  |121                |\n|2        |70     |61   |9                    |81                 |\n|3        |60     |61   |-1                   |1                  |\n|4        |40     |61   |-21                  |441                |\n|5        |80     |61   |19                   |361                |\n|6        |30     |61   |-31                  |961                |\n|7        |90     |61   |29                   |841                |\n|8        |60     |61   |-1                   |1                  |\n|9        |70     |61   |9                    |81                 |\n|10       |60     |61   |-1                   |1                  |\n|Sums     |610    |610  |0                    |2890               |\n|Means    |61     |61   |0                    |289                |\n|         |       |     |sd                   |17.92              |\n|         |       |     |SEM                  |5.67               |\n|         |       |     |t                    |1.94003527336861   |\n\n\n:::\n:::\n\n\n\n\n\nYou can see the `scores` column has all of the test scores for each of\nthe 10 students. We did the things we need to do to compute the standard\ndeviation.\n\nRemember the sample standard deviation is the square root of the sample\nvariance, or:\n\n$\\text{sample standard deviation} = \\sqrt{\\frac{\\sum_{i}^{n}({x_{i}-\\bar{x})^2}}{N-1}}$\n\n$\\text{sd} = \\sqrt{\\frac{2890}{10-1}} = 17.92$\n\nThe standard error of the mean, is the standard deviation divided by the\nsquare root of N\n\n$\\text{SEM} = \\frac{s}{\\sqrt{N}} = \\frac{17.92}{10} = 5.67$\n\n$t$ is the difference between our sample mean (61), and our population\nmean (50, assuming chance), divided by the standard error of the mean.\n\n$\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}} = \\frac{\\bar{X}-u}{SEM} = \\frac{61-50}{5.67} = 1.94$\n\nAnd, that is you how calculate $t$, by hand. It's a pain. I was annoyed\ndoing it this way. In the lab, you learn how to calculate $t$ using\nsoftware, so it will just spit out $t$. For example in R, all you have\nto do is this:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(scores, mu=50)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  scores\nt = 1.9412, df = 9, p-value = 0.08415\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 48.18111 73.81889\nsample estimates:\nmean of x \n       61 \n```\n\n\n:::\n:::\n\n\n\n\n\n### How does t behave?\n\nIf $t$ is just a number that we can compute from our sample (it is),\nwhat can we do with it? How can we use $t$ for statistical inference?\n\nRemember back to the chapter on sampling and distributions, that's where\nwe discussed the sampling distribution of the sample mean. Remember, we\nmade a lot of samples, then computed the mean for each sample, then we\nplotted a histogram of the sample means. Later, in that same section, we\nmentioned that we could generate sampling distributions for any\nstatistic. For each sample, we could compute the mean, the standard\ndeviation, the standard error, and now even $t$, if we wanted to. We\ncould generate 10,000 samples, and draw four histograms, one for each\nsampling distribution for each statistic.\n\nThis is exactly what I did, and the results are shown in the four panels\nof @fig-6sampdists below. I used a sample size of 20, and drew random\nobservations for each sample from a normal distribution, with mean = 0,\nand standard deviation = 1. Let's look at the sampling distributions for\neach of the statistics. $t$ was computed assuming with the population\nmean assumed to be 0.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_df <- data.frame()\nfor (i in 1:10000) {\n  sample <- rnorm(20, 0, 1)\n  sample_mean <- mean(sample)\n  sample_sd <- sd(sample)\n  sample_se <- sd(sample) / sqrt(length(sample))\n  sample_t <- as.numeric(t.test(sample, mu = 0)$statistic)\n  t_df <- data.frame(i, sample_mean, sample_sd, sample_se, sample_t)\n  all_df <- rbind(all_df, t_df)\n}\n\nlibrary(ggpubr)\na <- ggplot(all_df, aes(x = sample_mean)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nb <- ggplot(all_df, aes(x = sample_sd)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nc <- ggplot(all_df, aes(x = sample_se)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nd <- ggplot(all_df, aes(x = sample_t)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\n\nggarrange(a, b, c, d,\n          ncol = 2, nrow = 2)\n```\n\n::: {.cell-output-display}\n![Sampling distributions for the mean, standard deviation, standard error of the mean, and $t$.](chapter_files/figure-html/fig-6sampdists-1.png){#fig-6sampdists width=100%}\n:::\n:::\n\n\n\n\n\nWe see four sampling distributions. This is how statistical summaries of\nthese summaries behave. We have used the word chance windows before.\nThese are four chance windows, measuring different aspects of the\nsample. In this case, all of the samples came from the same normal\ndistribution. Because of sampling error, each sample is not identical.\nThe means are not identical, the standard deviations are not identical,\nsample standard error of the means are not identical, and the $t$s of\nthe samples are not identical. They all have some variation, as shown by\nthe histograms. This is how samples of size 20 behave.\n\nWe can see straight away, that in this case, we are unlikely to get a\nsample mean of 2. That's way outside the window. The range for the\nsampling distribution of the mean is around -.5 to +.5, and is centered\non 0 (the population mean, would you believe!).\n\nWe are unlikely to get sample standard deviations of between .6 and 1.5,\nthat is a different range, specific to the sample standard deviation.\n\nSame thing with the sample standard error of the mean, the range here is\neven smaller, mostly between .1, and .3. You would rarely find a sample\nwith a standard error of the mean greater than .3. Virtually never would\nyou find one of say 1 (for this situation).\n\nNow, look at $t$. It's range is basically between -3 and +3 here. 3s\nbarely happen at all. You pretty much never see a 5 or -5 in this\nsituation.\n\nAll of these sampling windows are chance windows, and they can all be\nused in the same way as we have used similar sampling distributions\nbefore (e.g., Crump Test, and Randomization Test) for statistical\ninference. For all of them we would follow the same process:\n\n1.  Generate these distributions\n2.  Look at your sample statistics for the data you have (mean, SD, SEM,\n    and $t$)\n3.  Find the likelihood of obtaining that value or greater\n4.  Obtain that probability\n5.  See if you think your sample statistics were probable or improbable.\n\nWe'll formalize this in a second. I just want you to know that what you\nwill be doing is something that you have already done before. For\nexample, in the Crump test and the Randomization test we focused on the\ndistribution of mean differences. We could do that again here, but\ninstead, we will focus on the distribution of $t$ values. We then apply\nthe same kinds of decision rules to the $t$ distribution, as we did for\nthe other distributions. Below you will see a graph you have already\nseen, except this time it is a distribution of $t$s, not mean\ndifferences:\n\nRemember, if we obtained a single $t$ from one sample we collected, we\ncould consult the chance window in @fig-6tdecision below to find out\nwhether the $t$ we obtained from the sample was likely or unlikely to\noccur by chance.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_t <- all_df$sample_t\n\nggplot(all_df, aes(x = sample_t)) +\n  annotate(\n    \"rect\",\n    xmin = min(sample_t),\n    xmax = max(sample_t),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"red\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = min(sample_t),\n    xmax = -1.94,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = 1.94,\n    xmax = max(sample_t),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  geom_rect(aes(\n    xmin = -Inf,\n    xmax = min(sample_t),\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_rect(aes(\n    xmin = max(sample_t),\n    xmax = Inf,\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_histogram(bins = 50, color = \"white\") +\n  theme_classic() +\n  geom_vline(xintercept = min(sample_t)) +\n  geom_vline(xintercept = max(sample_t)) +\n  geom_vline(xintercept = -1.94) +\n  geom_vline(xintercept = 1.94) +\n  xlim(-8, 8) +\n  geom_label(data = data.frame(x = 0, y = 250, label = \"CHANCE\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -7, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 7, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  #  geom_label(data = data.frame(x = min(sample_t), y = 600,\n  #                              label = paste0(\"min \\n\",round(min(sample_t)))),\n  #                             aes(x = x, y = y, label = label))+\n  #geom_label(data = data.frame(x = max(sample_t), y = 600,\n  #                            label = paste0(\"max \\n\",round(max(sample_t)))),\n  #                           aes(x = x, y = y, label = label))+\n  geom_label(data = data.frame(x = -4, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 4, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  xlab(\"mean sample_t\")\n```\n\n::: {.cell-output-display}\n![Applying decision criteria to the $t$-distribution. Histogram of $t$s from samples (n=20) drawn from the same normal distribution (u=0, sd=1)](chapter_files/figure-html/fig-6tdecision-1.png){#fig-6tdecision width=672}\n:::\n:::\n\n\n\n\n\n### Making a decision\n\nFrom our early example involving the TRUE/FALSE quizzes, we are now\nready to make some kind of decision about what happened there. We found\na mean difference of 11. We found a $t$ =\n1.9411765. The probability of this $t$ or\nlarger occurring is $p$ = 0.0841503. We were\ntesting the idea that our sample mean of 61 could have\ncome from a normal distribution with mean = 50. The $t$ test tells us\nthat the $t$ for our sample, or a larger one, would happen with p =\n0.0841503. In other words, chance can do it a\nkind of small amount of time, but not often. In English, this means that\nall of the students could have been guessing, but it wasn't that likely\nthat were just guessing.\n\nThe next $t$-test is called a **paired samples t-test**. And, spoiler\nalert, we will find out that a paired samples t-test is actually a\none-sample t-test in disguise (WHAT!), yes it is. If the one-sample\n$t$-test didn't make sense to you, read the next section.\n\n## Paired-samples t-test\n\nFor me (Crump), many analyses often boil down to a paired samples\nt-test. It just happens that many things I do reduce down to a test like\nthis.\n\nI am a cognitive psychologist, I conduct research about how people do\nthings like remember, pay attention, and learn skills. There are lots of\nPsychologists like me, who do very similar things.\n\nWe all often conduct the same kinds of experiments. They go like this,\nand they are called **repeated measures** designs. They are called\n**repeated measures** designs, because we measure how one person does\nsomething more than once, we **repeat** the measure.\n\nSo, I might measure somebody doing something in condition A, and measure\nthe same person doing something in Condition B, and then I see that same\nperson does different things in the two conditions. I **repeatedly\nmeasure** the same person in both conditions. I am interested in whether\nthe experimental manipulation changes something about how people perform\nthe task in question.\n\n### Mehr, Song, and Spelke (2016)\n\nWe will introduce the paired-samples t-test with an example using real\ndata, from a real study. @mehr20165 were interested in whether singing\nsongs to infants helps infants become more sensitive to social cues. For\nexample, infants might need to learn to direct their attention toward\npeople as a part of learning how to interact socially with people.\nPerhaps singing songs to infants aids this process of directing\nattention. When an infant hears a familiar song, they might start to pay\nmore attention to the person singing that song, even after they are done\nsinging the song. The person who sang the song might become more\nsocially important to the infant. You will learn more about this study\nin the lab for this week. This example, prepares you for the lab\nactivities. Here is a brief summary of what they did.\n\nFirst, parents were trained to sing a song to their infants. After many\ndays of singing this song to the infants, a parent came into the lab\nwith their infant. In the first session, parents sat with their infants\non their knees, so the infant could watch two video presentations. There\nwere two videos. Each video involved two unfamiliar new people the\ninfant had never seen before. Each new person in the video (the singers)\nsang one song to the infant. One singer sang the \"familiar\" song the\ninfant had learned from their parents. The other singer sang an\n\"unfamiliar\" song the infant had not hear before.\n\nThere were two really important measurement phases: the baseline phase,\nand the test phase.\n\nThe baseline phase occurred before the infants saw and heard each singer\nsing a song. During the baseline phase, the infants watched a video of\nboth singers at the same time. The researchers recorded the proportion\nof time that the infant looked at each singer. The baseline phase was\nconducted to determine whether infants had a preference to look at\neither person (who would later sing them a song).\n\nThe test phase occurred **after** infants saw and heard each song, sung\nby each singer. During the test phase, each infant had an opportunity to\nwatch silent videos of both singers. The researchers measured the\nproportion of time the infants spent looking at each person. The\nquestion of interest, was whether the infants would spend a greater\nproportion of time looking at the singer who sang the familiar song,\ncompared to the singer who sang the unfamiliar song.\n\nThere is more than one way to describe the design of this study. We will\ndescribe it like this. It was a repeated measures design, with one\nindependent (manipulation) variable called Viewing phase: Baseline\nversus Test. There was one dependent variable (the measurement), which\nwas proportion looking time (to singer who sung familiar song). This was\na repeated measures design because the researchers measured proportion\nlooking time twice (they repeated the measure), once during baseline\n(before infants heard each singer sing a song), and again during test\n(after infants head each singer sing a song).\n\nThe important question was whether infants would change their looking\ntime, and look more at the singer who sang the familiar song during the\ntest phase, than they did during the baseline phase. This is a question\nabout a change within individual infants. In general, the possible\noutcomes for the study are:\n\n1.  No change: The difference between looking time toward the singer of\n    the familiar song during baseline and test is zero, no difference.\n\n2.  Positive change: Infants will look longer toward the singer of the\n    familiar song during the test phase (after they saw and heard the\n    singers), compared to the baseline phase (before they saw and heard\n    the singers). This is a positive difference if we use the formula:\n    Test Phase Looking time - Baseline phase looking time (to familiar\n    song singer).\n\n3.  Negative change: Infants will look longer toward the singer of the\n    unfamiliar song during the test phase (after they saw and heard the\n    singers), compared to the baseline phase (before they saw and heard\n    the singers). This is a negative difference if we use the same\n    formula: Test Phase Looking time - Baseline phase looking time (to\n    familiar song singer).\n\n### The data\n\nLet's take a look at the data for the first 5 infants in the study. This\nwill help us better understand some properties of the data before we\nanalyze it. We will see that the data is structured in a particular way\nthat we can take advantage of with a paired samples t-test. Note, we\nlook at the first 5 infants to show how the computations work. The\nresults of the paired-samples t-test change when we use all of the data\nfrom the study.\n\nHere is a table of the data:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| infant| Baseline| Test|\n|------:|--------:|----:|\n|      1|     0.44| 0.60|\n|      2|     0.41| 0.68|\n|      3|     0.75| 0.72|\n|      4|     0.44| 0.28|\n|      5|     0.47| 0.50|\n\n\n:::\n:::\n\n\n\n\n\nThe table shows proportion looking times toward the singer of the\nfamiliar song during the Baseline and Test phases. Notice there are five\ndifferent infants, (1 to 5). Each infant is measured twice, once during\nthe Baseline phase, and once during the Test phase. To repeat from\nbefore, this is a repeated-measures design, because the infants are\nmeasured repeatedly (twice in this case). Or, this kind of design is\nalso called a **paired-samples** design. Why? because each participant\ncomes with a pair of samples (two samples), one for each level of the\ndesign.\n\nGreat, so what are we really interested in here? We want to know if the\nmean looking time toward the singer of the familiar song for the Test\nphase is higher than the Baseline phase. We are comparing the two sample\nmeans against each other and looking for a difference. We already know\nthat differences could be obtained by chance alone, simply because we\ntook two sets of samples, and we know that samples can be different. So,\nwe are interested in knowing whether chance was likely or unlikely to\nhave produced any difference we might observe.\n\nIn other words, we are interested in looking at the difference scores\nbetween the baseline and test phase for each infant. The question here\nis, for each infant, did their proportion looking time to the singer of\nthe familiar song, increase during the test phase as compared to the\nbaseline phase.\n\n### The difference scores\n\nLet's add the difference scores to the table of data so it is easier to\nsee what we are talking about. The first step in creating difference\nscores is to decide how you will take the difference, there are two\noptions:\n\n1.  Test phase score - Baseline Phase Score\n2.  Baseline phase score - Test Phase score\n\nLet's use the first formula. Why? Because it will give us positive\ndifferences when the test phase score is higher than the baseline phase\nscore. This makes a positive score meaningful with respect to the study\ndesign, we know (because we defined it to be this way), that positive\nscores will refer to longer proportion looking times (to singer of\nfamiliar song) during the test phase compared to the baseline phase.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaired_sample_df <- cbind(paired_sample_df, \n                          differences = (paired_sample_df$Test-\n                                           paired_sample_df$Baseline))\nknitr::kable(paired_sample_df)\n```\n\n::: {.cell-output-display}\n\n\n| infant| Baseline| Test| differences|\n|------:|--------:|----:|-----------:|\n|      1|     0.44| 0.60|        0.16|\n|      2|     0.41| 0.68|        0.27|\n|      3|     0.75| 0.72|       -0.03|\n|      4|     0.44| 0.28|       -0.16|\n|      5|     0.47| 0.50|        0.03|\n\n\n:::\n:::\n\n\n\n\n\nThere we have it, the difference scores. The first thing we can do here\nis look at the difference scores, and ask how many infants showed the\neffect of interest. Specifically, how many infants showed a positive\ndifference score. We can see that three of five infants showed a\npositive difference (they looked more at the singer of the familiar song\nduring the test than baseline phase), and two the infants showed the\nopposite effect (negative difference, they looked more at the singer of\nthe familiar song during baseline than test).\n\n### The mean difference\n\nAs we have been discussing, the effect of interest in this study is the\nmean difference between the baseline and test phase proportion looking\ntimes. We can calculate the **mean difference**, by finding the **mean\nof the difference scores**. Let's do that, in fact, for fun let's\ncalculate the mean of the baseline scores, the test scores, and the\ndifference scores.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaired_sample_df <- paired_sample_df %>%\n   rbind(c(\"Sums\",colSums(paired_sample_df[1:5,2:4]))) %>%\n   rbind(c(\"Means\",colMeans(paired_sample_df[1:5,2:4])))\n  \nknitr::kable(paired_sample_df)\n```\n\n::: {.cell-output-display}\n\n\n|infant |Baseline |Test  |differences |\n|:------|:--------|:-----|:-----------|\n|1      |0.44     |0.6   |0.16        |\n|2      |0.41     |0.68  |0.27        |\n|3      |0.75     |0.72  |-0.03       |\n|4      |0.44     |0.28  |-0.16       |\n|5      |0.47     |0.5   |0.03        |\n|Sums   |2.51     |2.78  |0.27        |\n|Means  |0.502    |0.556 |0.054       |\n\n\n:::\n:::\n\n\n\n\n\nWe can see there was a positive mean difference of 0.054, between the\ntest and baseline phases.\n\nCan we rush to judgment and conclude that infants are more socially\nattracted to individuals who have sung them a familiar song? I would\nhope not based on this very small sample. First, the difference in\nproportion looking isn't very large, and of course we recognize that\nthis difference could have been produced by chance.\n\nWe will more formally evaluate whether this difference could have been\ncaused by chance with the paired-samples t-test. But, before we do that,\nlet's again calculate $t$ and discuss what $t$ tells us over and above\nwhat our measure of the mean of the difference scores tells us.\n\n### Calculate t\n\nOK, so how do we calculate $t$ for a paired-samples $t$-test? Surprise,\nwe use the one-sample t-test formula that you already learned about!\nSpecifically, we use the one-sample $t$-test formula on the difference\nscores. We have one sample of difference scores (you can see they are in\none column), so we can use the one-sample $t$-test on the difference\nscores. Specifically, we are interested in comparing whether the mean of\nour difference scores came from a distribution with mean difference = 0.\nThis is a special distribution we refer to as the **null distribution**.\nIt is the distribution no differences. Of course, this **null\ndistribution** can produce differences due to to sampling error, but\nthose differences are not caused by any experimental manipulation, they\ncaused by the random sampling process.\n\nWe calculate $t$ in a moment. Let's now consider again why we want to\ncalculate $t$? Why don't we just stick with the mean difference we\nalready have?\n\nRemember, the whole concept behind $t$, is that it gives an indication\nof how confident we should be in our mean. Remember, $t$ involves a\nmeasure of the mean in the numerator, divided by a measure of variation\n(standard error of the sample mean) in the denominator. The resulting\n$t$ value is small when the mean difference is small, or when the\nvariation is large. So small $t$-values tell us that we shouldn't be\nthat confident in the estimate of our mean difference. Large $t$-values\noccur when the mean difference is large and/or when the measure of\nvariation is small. So, large $t$-values tell us that we can be more\nconfident in the estimate of our mean difference. Let's find $t$ for the\nmean difference scores. We use the same formulas as we did last time:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|infant |Baseline |Test  |differences |diff_from_mean |Squared_differences  |\n|:------|:--------|:-----|:-----------|:--------------|:--------------------|\n|1      |0.44     |0.6   |0.16        |0.106          |0.011236             |\n|2      |0.41     |0.68  |0.27        |0.216          |0.046656             |\n|3      |0.75     |0.72  |-0.03       |-0.084         |0.00705600000000001  |\n|4      |0.44     |0.28  |-0.16       |-0.214         |0.045796             |\n|5      |0.47     |0.5   |0.03        |-0.024         |0.000575999999999999 |\n|Sums   |2.51     |2.78  |0.27        |0              |0.11132              |\n|Means  |0.502    |0.556 |0.054       |0              |0.022264             |\n|       |         |      |            |sd             |0.167                |\n|       |         |      |            |SEM            |0.075                |\n|       |         |      |            |t              |0.72                 |\n\n\n:::\n:::\n\n\n\n\n\nIf we did this test using R, we would obtain almost the same numbers\n(there is a little bit of rounding in the table).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(differences,mu=0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  differences\nt = 0.72381, df = 4, p-value = 0.5092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.1531384  0.2611384\nsample estimates:\nmean of x \n    0.054 \n```\n\n\n:::\n:::\n\n\n\n\n\nHere is a quick write up of our t-test results, t(4) = .72, p = .509.\n\nWhat does all of that tell us? There's a few things we haven't gotten\ninto much yet. For example, the 4 represents degrees of freedom, which\nwe discuss later. The important part, the $t$ value should start to be a\nlittle bit more meaningful. We got a kind of small t-value didn't we.\nIt's .72. What can we tell from this value? First, it is positive, so we\nknow the mean difference is positive. The sign of the $t$-value is\nalways the same as the sign of the mean difference (ours was +0.054). We\ncan also see that the p-value was .509. We've seen p-values before. This\ntells us that our $t$ value or larger, occurs about 50.9% of the time...\nActually it means more than this. And, to understand it, we need to talk\nabout the concept of two-tailed and one-tailed tests.\n\n### Interpreting $t$s\n\nRemember what it is we are doing here. We are evaluating whether our\nsample data could have come from a particular kind of distribution. The\nnull distribution of no differences. This is the distribution of\n$t$-values that would occur for samples of size 5, with a mean\ndifference of 0, and a standard error of the sample mean of .075 (this\nis the SEM that we calculated from our sample). We can see what this\nparticular null-distribution looks like in @fig-6tnull.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange <- seq(-3, 3, .1)\nnull_distribution <- dt(range, 4, log = FALSE)\nplot_df <- data.frame(range, null_distribution)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  scale_x_continuous(breaks = (seq(-3, 3, .5))) +\n  geom_label(data = data.frame(x = -.7, y = .1, label = \"50% \\n (-)\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .7, y = .1, label = \"50% \\n (+)\"), aes(x = x, y = y, label = label)) +\n  geom_vline(xintercept = 0)+\n  theme_classic(base_size = 20)\n```\n\n::: {.cell-output-display}\n![A distribution of $t$-values that can occur by chance alone, when there is no difference between the sample and a population](chapter_files/figure-html/fig-6tnull-1.png){#fig-6tnull width=672}\n:::\n:::\n\n\n\n\n\nThe $t$-distribution above shows us the kinds of values $t$ will will\ntake by chance alone, when we measure the mean differences for pairs of\n5 samples (like our current). $t$ is most likely to be zero, which is\ngood, because we are looking at the distribution of no-differences,\nwhich should most often be 0! But, sometimes, due to sampling error, we\ncan get $t$s that are bigger than 0, either in the positive or negative\ndirection. Notice the distribution is symmetrical, a $t$ from the\nnull-distribution will be positive half of the time, and negative half\nof the time, that is what we would expect by chance.\n\nSo, what kind of information do we want know when we find a particular\n$t$ value from our sample? We want to know how likely the $t$ value like\nthe one we found occurs just by chance. This is actually a subtly\nnuanced kind of question. For example, any particular $t$ value doesn't\nhave a specific probability of occurring. When we talk about\nprobabilities, we are talking about ranges of probabilities. Let's\nconsider some probabilities. We will use the letter $p$, to talk about\nthe probabilities of particular $t$ values.\n\n1.  What is the probability that $t$ is zero or positive or negative?\n    The answer is p=1, or 100%. We will always have a $t$ value that is\n    zero or non-zero...Actually, if we can't compute the t-value, for\n    example when the standard deviation is undefined, I guess then we\n    would have a non-number. But, assuming we can calculate $t$, then it\n    will always be 0 or positive or negative.\n\n2.  What is the probability of $t$ = 0 or greater than 0? The answer is\n    p=.5, or 50%. 50% of $t$-values are 0 or greater.\n\n3.  What is the of $t$ = 0 or smaller than 0? The answer is p=.5, or\n    50%. 50% of $t$-values are 0 or smaller.\n\nWe can answer all of those questions just by looking at our\nt-distribution, and dividing it into two equal regions, the left side\n(containing 50% of the $t$ values), and the right side containing 50% of\nthe $t$-values).\n\nWhat if we wanted to take a more fine-grained approach, let's say we\nwere interested in regions of 10%. What kinds of $t$s occur 10% of the\ntime. We would apply lines like the following. Notice, the likelihood of\nbigger numbers (positive or negative) gets smaller, so we have to\nincrease the width of the bars for each of the intervals between the\nbars to contain 10% of the $t$-values, it looks like\n@fig-6percentregions.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange <- seq(-3, 3, .1)\nnull_distribution <- dt(range, 4, log = FALSE)\nplot_df <- data.frame(range, null_distribution)\n\nt_ps <- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = t_ps) +\n  theme_classic(base_size = 15) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  scale_x_continuous(breaks = round(t_ps, digits = 1)) +\n  geom_label(data = data.frame(x = -2.5, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -1.3, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 1.3, y = .2, label = \"10%\"), aes(x = x, y = y, label = label))\n```\n\n::: {.cell-output-display}\n![Splitting the t distribution up into regions each containing 10% of the $t$-values. The width between the bars narrows as they approach the center of the distribution, where there are more $t$-values.](chapter_files/figure-html/fig-6percentregions-1.png){#fig-6percentregions width=100%}\n:::\n:::\n\n\n\n\n\nConsider the probabilities ($p$) of $t$ for the different ranges.\n\n1.  $t$ \\<= -1.5 ($t$ is less than or equal to -1.5), $p$ = 10%\n2.  -1.5 \\>= $t$ \\<= -0.9 ($t$ is equal to or between -1.5 and -.9), $p$\n    = 10%\n3.  -.9 \\>= $t$ \\<= -0.6 ($t$ is equal to or between -.9 and -.6), $p$ =\n    10%\n4.  $t$ \\>= 1.5 ($t$ is greater than or equal to 1.5), $p$ = 10%\n\nNotice, that the $p$s are always 10%. $t$s occur in these ranges with\n10% probability.\n\n### Getting the p-values for $t$-values\n\nYou might be wondering where I am getting some of these values from. For\nexample, how do I know that 10% of $t$ values (for this null\ndistribution) have a value of approximately 1.5 or greater than 1.5? The\nanswer is I used R to tell me.\n\nIn most statistics textbooks the answer would be: there is a table at\nthe back of the book where you can look these things up...This textbook\nhas no such table. We could make one for you. And, we might do that.\nBut, we didn't do that yet...\n\nSo, where do these values come from, how can you figure out what they\nare? The complicated answer is that we are not going to explain the math\nbehind finding these values because, 1) the authors (some of us)\nadmittedly don't know the math well enough to explain it, and 2) it\nwould sidetrack us to much, 3) you will learn how to get these numbers\nin the lab with software, 4) you will learn how to get these numbers in\nlab without the math, just by doing a simulation, and 5) you can do it\nin R, or excel, or you can use an [online\ncalculator](http://www.socscistatistics.com/pvalues/tdistribution.aspx).\n\nThis is all to say that you can find the $t$s and their associated $p$s\nusing software. But, the software won't tell you what these values mean.\nThat's we are doing here. You will also see that software wants to know\na few more things from you, such as the degrees of freedom for the test,\nand whether the test is one-tailed or two tailed. We haven't explained\nany of these things yet. That's what we are going to do now. Note, we\nexplain degrees of freedom last. First, we start with a one-tailed test.\n\n### One-tailed tests\n\nA **one-tailed test** is sometimes also called a directional test. It is\ncalled a directional test, because a researcher might have a hypothesis\nin mind suggesting that the difference they observe in their means is\ngoing to have a particular direction, either a positive difference, or a\nnegative difference.\n\nTypically, a researcher would set an **alpha criterion**. The alpha\ncriterion describes a line in the sand for the researcher. Often, the\nalpha criterion is set at $p = .05$. What does this mean? @fig-6critT\nshows the $t$-distribution and the alpha criterion.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange <- seq(-3, 3, .1)\nnull_distribution <- dt(range, 4, log = FALSE)\nplot_df <- data.frame(range, null_distribution)\n\nt_ps <- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = qt(.95, 4, lower.tail = TRUE)) +\n  ggtitle(\"Critical t for one-tailed test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = qt(.95, 4, lower.tail = TRUE),\n    xmax = 3,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.95, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label))\n```\n\n::: {.cell-output-display}\n![The critical value of t for an alpha criterion of 0.05. 5% of all ts are at this value or larger](chapter_files/figure-html/fig-6critT-1.png){#fig-6critT width=672}\n:::\n:::\n\n\n\n\n\nThe figure shows that $t$ values of +2.13 or greater occur 5% of the\ntime. Because the t-distribution is symmetrical, we also know that $t$\nvalues of -2.13 or smaller also occur 5% of the time. Both of these\nproperties are true under the null distribution of no differences. This\nmeans, that when there really are no differences, a researcher can\nexpect to find $t$ values of 2.13 or larger 5% of the time.\n\nLet's review and connect some of the terms:\n\n1.  **alpha criterion**: the criterion set by the researcher to make\n    decisions about whether they believe chance did or did not cause the\n    difference. The alpha criterion here is set to $p = .05$.\n\n2.  **Critical** $t$. The critical $t$ is the $t$-value associated with\n    the alpha-criterion. In this case for a one-tailed test, it is the\n    $t$ value where 5% of all $t$s are this number or greater. In our\n    example, the critical $t$ is 2.13. 5% of all $t$ values (with\n    degrees of freedom = 4) are +2.13, or greater than +2.13.\n\n3.  **Observed** $t$. The observed $t$ is the one that you calculated\n    from your sample. In our example about the infants, the observed $t$\n    was $t$ (4) = 0.72.\n\n4.  **p-value**. The $p$-value is the probability of obtaining the\n    observed $t$ value or larger. Now, you could look back at our\n    previous example, and find that the $p$-value for $t$ (4) = .72, was\n    $p = .509$ . HOWEVER, this p-value was not calculated for a\n    one-directional test...(we talk about what .509 means in the next\n    section).\n\n@fig-6tonedirection shows what the $p$-value for $t$ (4) = .72 using a\none-directional test would would look like:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange <- seq(-3, 3, .1)\nnull_distribution <- dt(range, 4, log = FALSE)\nplot_df <- data.frame(range, null_distribution)\n\nt_ps <- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = .72) +\n  geom_vline(xintercept = qt(.95, 4, lower.tail = TRUE)) +\n  ggtitle(\"t value and p-range for one-directional test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = .72,\n    xmax = 3,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = qt(.95, 4, lower.tail = TRUE),\n    xmax = 3,\n    ymin = 0,\n    ymax = .25,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.95, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .72, y = .1,\n                               label = \"Observed t\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .72, y = .05,\n                               label = \".72, p=\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = 1.5,\n    y = .05,\n    label = round(pt(.72, 4, lower.tail = FALSE), digits =\n                    3)\n  ),\n  aes(x = x, y = y, label = label))\n```\n\n::: {.cell-output-display}\n![A case where the observed value of t is much less than the critical value for a one-directional t-test.](chapter_files/figure-html/fig-6tonedirection-1.png){#fig-6tonedirection width=672}\n:::\n:::\n\n\n\n\n\nLet's take this one step at a time. We have located the observed $t$ of\n.72 on the graph. We shaded the right region all grey. What we see is\nthat the grey region represents .256 or 25.6% of all $t$ values. In\nother words, 25.6% of $t$ values are .72 or larger than .72. You could\nexpect, by chance alone, to a find a $t$ value of .72 or larger, 25.6%\nof the time. That's fairly often. We did find a $t$ value of .72. Now\nthat you know this kind of $t$ value or larger occurs 25.6% of the time,\nwould you be confident that the mean difference was not due to chance?\nProbably not, given that chance can produce this difference fairly\noften.\n\nFollowing the \"standard\" decision making procedure, we would claim that\nour $t$ value was **not statistically significant**, because it was not\nlarge enough. If our observed value was larger than the critical $t$\n(larger than 2.13), defined by our alpha criterion, then we would claim\nthat our $t$ value was **statistically significant**. This would be\nequivalent to saying that we believe it is unlikely that the difference\nwe observed was due to chance. In general, for any observed $t$ value,\nthe associated $p$-value tells you how likely a $t$ of the observed size\nor larger would be observed. The $p$-value **always** refers to a\n**range** of $t$-values, never to a single $t$-value. Researchers use\nthe alpha criterion of .05, as a matter of convenience and convention.\nThere are other ways to interpret these values that do not rely on a\nstrict (significant versus not) dichotomy.\n\n### Two-tailed tests\n\nOK, so that was one-tailed tests... What are two tailed tests? The\n$p$-value that we originally calculated from our paired-samples $t$-test\nwas for a 2-tailed test. Often, the default is that the $p$-value is for\na two-tailed test.\n\nThe two-tailed test, is asking a more general question about whether a\ndifference is likely to have been produced by chance. The question is:\nwhat is probability of any difference. It is also called a\n**non-directional** test, because here we don't care about the direction\nor sign of the difference (positive or negative), we just care if there\nis any kind of difference.\n\nThe same basic things as before are involved. We define an alpha\ncriterion ($\\alpha = 0.05$). And, we say that any observed $t$ value\nthat has a probability of $p$ \\<.05 ($p$ is less than .05) will be\ncalled **statistically significant**, and ones that are more likely ($p$\n\\>.05, $p$ is greater than .05) will be called null-results, or not\nstatistically significant. The only difference is how we draw the alpha\nrange. Before it was on the right side of the $t$ distribution (we were\nconducting a one-sided test remember, so we were only interested in one\nside).\n\n@fig-6twotailedt shows what the most extreme 5% of the $t$-values are\nwhen we ignore their sign (whether they are positive or negative).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange <- seq(-4, 4, .1)\nnull_distribution <- dt(range, 4, log = FALSE)\nplot_df <- data.frame(range, null_distribution)\n\nt_ps <- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = qt(.975, 4, lower.tail = TRUE)) +\n  geom_vline(xintercept = qt(.025, 4, lower.tail = TRUE)) +\n  ggtitle(\"Critical ts for two-tailed test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = qt(.975, 4, lower.tail = TRUE),\n    xmax = 4,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 3.5, y = .2, label = \"2.5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.975, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.975, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.975, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label)) +\n  annotate(\n    \"rect\",\n    xmin = -4,\n    xmax = qt(.025, 4, lower.tail = TRUE),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = -3.5, y = .2, label = \"2.5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.025, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.025, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.025, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label))\n```\n\n::: {.cell-output-display}\n![Critical values for a two-tailed test. Each line represents the location where 2.5% of all $t$s are larger or smaller than critical value. The total for both tails is 5%](chapter_files/figure-html/fig-6twotailedt-1.png){#fig-6twotailedt width=672}\n:::\n:::\n\n\n\n\n\nHere is what we are seeing. A distribution of no differences (the null,\nwhich is what we are looking at), will produce $t$s that are 2.78 or\ngreater 2.5% of the time, and $t$s that are -2.78 or smaller 2.5% of the\ntime. 2.5% + 2.5% is a total of 5% of the time. We could also say that\n$t$s larger than +/- 2.78 occur 5% of the time.\n\nAs a result, the critical $t$ value is (+/-) 2.78 for a two-tailed test.\nAs you can see, the two-tailed test is blind to the direction or sign of\nthe difference. Because of this, the critical $t$ value is also higher\nfor a two-tailed test, than for the one-tailed test that we did earlier.\nHopefully, now you can see why it is called a two-tailed test. There are\ntwo tails of the distribution, one on the left and right, both shaded in\ngreen.\n\n### One or two tailed, which one?\n\nNow that you know there are two kinds of tests, one-tailed, and\ntwo-tailed, which one should you use? There is some conventional wisdom\non this, but also some debate. In the end, it is up to you to be able to\njustify your choice and why it is appropriate for you data. That is the\nreal answer.\n\nThe conventional answer is that you use a one-tailed test when you have\na theory or hypothesis that is making a directional prediction (the\ntheory predicts that the difference will be positive, or negative).\nSimilarly, use a two-tailed test when you are looking for any\ndifference, and you don't have a theory that makes a directional\nprediction (it just makes the prediction that there will be a\ndifference, either positive or negative).\n\nAlso, people appear to choose one or two-tailed tests based on how risky\nthey are as researchers. If you always ran one-tailed tests, your\ncritical $t$ values for your set alpha criterion would always be smaller\nthan the critical $t$s for a two-tailed test. Over the long run, you\nwould make more type I errors, because the criterion to detect an effect\nis a lower bar for one than two tailed tests.\n\n> Remember type 1 errors occur when you reject the idea that chance\n> could have caused your difference. You often never know when you make\n> this error. It happens anytime that sampling error was the actual\n> cause of the difference, but a researcher dismisses that possibility\n> and concludes that their manipulation caused the difference.\n\nSimilarly, if you always ran two-tailed tests, even when you had a\ndirectional prediction, you would make fewer type I errors over the long\nrun, because the $t$ for a two-tailed test is higher than the $t$ for a\none-tailed test. It seems quite common for researchers to use a more\nconservative two-tailed test, even when they are making a directional\nprediction based on theory. In practice, researchers tend to adopt a\nstandard for reporting that is common in their field. Whether or not the\npractice is justifiable can sometimes be an open question. The important\ntask for any researcher, or student learning statistics, is to be able\nto justify their choice of test.\n\n### Degrees of freedom\n\nBefore we finish up with paired-samples $t$-tests, we should talk about\ndegrees of freedom. Our sense is that students don't really understand\ndegrees of freedom very well. If you are reading this textbook, you are\nprobably still wondering what is degrees of freedom, seeing as we\nhaven't really talked about it all.\n\nFor the $t$-test, there is a formula for degrees of freedom. For the\none-sample and paired sample $t$-tests, the formula is:\n\n$\\text{Degrees of Freedom} = \\text{df} = n-1$. Where n is the number of\nsamples in the test.\n\nIn our paired $t$-test example, there were 5 infants. Therefore, degrees\nof freedom = 5-1 = 4.\n\nOK, that's a formula. Who cares about degrees of freedom, what does the\nnumber mean? And why do we report it when we report a $t$-test... you've\nprobably noticed the number in parentheses e.g., $t$(4)=.72, the 4 is\nthe $df$, or degrees of freedom.\n\nDegrees of freedom is both a concept, and a correction. The concept is\nthat if you estimate a property of the numbers, and you use this\nestimate, you will be forcing some constraints on your numbers.\n\nConsider the numbers: 1, 2, 3. The mean of these numbers is 2. Now,\nlet's say I told you that the mean of three numbers is 2. Then, how many\nof these three numbers have freedom? Funny question right. What we mean\nis, how many of the three numbers could be any number, or have the\nfreedom to be any number.\n\nThe first two numbers could be any number. But, once those two numbers\nare set, the final number (the third number), MUST be a particular\nnumber that makes the mean 2. The first two numbers have freedom. The\nthird number has no freedom.\n\nTo illustrate. Let's freely pick two numbers: 51 and -3. I used my\npersonal freedom to pick those two numbers. Now, if our three numbers\nare 51, -3, and x, and the mean of these three numbers is 2. There is\nonly one solution, x has to be -42, otherwise the mean won't be 2. This\nis one way to think about degrees of freedom. The degrees of freedom for\nthese three numbers is n-1 = 3-1= 2, because 2 of the numbers can be\nfree, but the last number has no freedom, it becomes fixed after the\nfirst two are decided.\n\nNow, statisticians often apply degrees of freedom to their calculations,\nespecially when a second calculation relies on an estimated value. For\nexample, when we calculate the standard deviation of a sample, we first\ncalculate the mean of the sample right! By estimating the mean, we are\nfixing an aspect of our sample, and so, our sample now has n-1 degrees\nof freedom when we calculate the standard deviation (remember for the\nsample standard deviation, we divide by n-1...there's that n-1 again.)\n\n#### Simulating how degrees of freedom affects the $t$ distribution\n\nThere are at least two ways to think the degrees of freedom for a\n$t$-test. For example, if you want to use math to compute aspects of the\n$t$ distribution, then you need the degrees of freedom to plug in to the\nformula... If you want to see the formulas I'm talking about, scroll\ndown on the [$t$-test wikipedia\npage](https://en.wikipedia.org/wiki/Student%27s_t-distribution) and look\nfor the probability density or cumulative distribution functions...We\nthink that is quite scary for most people, and one reason why degrees of\nfreedom are not well-understood.\n\nIf we wanted to simulate the $t$ distribution we could more easily see\nwhat influence degrees of freedom has on the shape of the distribution.\nRemember, $t$ is a sample statistic, it is something we measure from the\nsample. So, we could simulate the process of measuring $t$ from many\ndifferent samples, then plot the histogram of $t$ to show us the\nsimulated $t$ distribution.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nts <- c(rt(10000, 4), rt(10000, 100))\ndfs <- as.factor(rep(c(4, 100), each = 10000))\n\nt_df <- data.frame(dfs, ts)\nt_df <- t_df[abs(t_df$ts) < 5, ]\n\nggplot(t_df, aes(x = ts, group = dfs, color = dfs)) +\n  geom_histogram() +\n  theme_classic() +\n  facet_wrap( ~ dfs) +\n  theme_classic(base_size=15)\n```\n\n::: {.cell-output-display}\n![The width of the t distribution shrinks as sample size and degrees of freedom (from 4 to 100) increases.](chapter_files/figure-html/fig-6dft-1.png){#fig-6dft width=100%}\n:::\n:::\n\n\n\n\n\nIn @fig-6dft notice that the red distribution for $df = 4$, is a little\nbit shorter, and a little bit wider than the bluey-green distribution\nfor $df = 100$. As degrees of freedom increase the $t$ distribution gets\ntaller (in the middle), and narrower in the range. It get's more peaky.\nCan you guess the reason for this? Remember, we are estimating a sample\nstatistic, and degrees of freedom is really just a number that refers to\nthe number of subjects (well minus one). And, we already know that as we\nincrease $n$, our sample statistics become better estimates (less\nvariance) of the distributional parameters they are estimating. So, $t$\nbecomes a better estimate of it's \"true\" value as sample size increase,\nresulting in a more narrow distribution of $t$s.\n\nThere is a slightly different $t$ distribution for every degrees of\nfreedom, and the critical regions associated with 5% of the extreme\nvalues are thus slightly different every time. This is why we report the\ndegrees of freedom for each t-test, they define the distribution of $t$\nvalues for the sample-size in question. Why do we use n-1 and not n?\nWell, we calculate $t$ using the sample standard deviation to estimate\nthe standard error or the mean, that estimate uses n-1 in the\ndenominator, so our $t$ distribution is built assuming n-1. That's\nenough for degrees of freedom...\n\n## The paired samples t-test strikes back\n\nYou must be wondering if we will ever be finished talking about paired\nsamples t-tests... why are we doing round 2, oh no! Don't worry, we're\njust going to 1) remind you about what we were doing with the infant\nstudy, and 2) do a paired samples t-test on the entire data set and\ndiscuss.\n\nRemember, we were wondering if the infants would look longer toward the\nsinger who sang the familiar song during the test phase compared to the\nbaseline phase. We showed you data from 5 infants, and walked through\nthe computations for the $t$-test. As a reminder, it looked like this:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|infant |Baseline |Test  |differences |diff_from_mean |Squared_differences  |\n|:------|:--------|:-----|:-----------|:--------------|:--------------------|\n|1      |0.44     |0.6   |0.16        |0.106          |0.011236             |\n|2      |0.41     |0.68  |0.27        |0.216          |0.046656             |\n|3      |0.75     |0.72  |-0.03       |-0.084         |0.00705600000000001  |\n|4      |0.44     |0.28  |-0.16       |-0.214         |0.045796             |\n|5      |0.47     |0.5   |0.03        |-0.024         |0.000575999999999999 |\n|Sums   |2.51     |2.78  |0.27        |0              |0.11132              |\n|Means  |0.502    |0.556 |0.054       |0              |0.022264             |\n|       |         |      |            |sd             |0.167                |\n|       |         |      |            |SEM            |0.075                |\n|       |         |      |            |t              |0.72                 |\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  round(differences, digits = 2)\nt = 0.72381, df = 4, p-value = 0.5092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.1531384  0.2611384\nsample estimates:\nmean of x \n    0.054 \n```\n\n\n:::\n:::\n\n\n\n\n\nLet's write down the finding one more time: The mean difference was\n0.054, $t$(4) = .72, $p$ =.509. We can also now confirm, that the\n$p$-value was from a two-tailed test. So, what does this all really\nmean.\n\nWe can say that a $t$ value with an absolute of .72 or larger occurs\n50.9% of the time. More precisely, the distribution of no differences\n(the null), will produce a $t$ value this large or larger 50.9% of the\ntime. In other words, chance alone good have easily produced the $t$\nvalue from our sample, and the mean difference we observed or .054,\ncould easily have been a result of chance.\n\nLet's quickly put all of the data in the $t$-test, and re-run the test\nusing all of the infant subjects.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaired_sample_df <-  data.frame(infant=1:32, \n                               Baseline = round(experiment_one$Baseline_Proportion_Gaze_to_Singer[1:32],digits=2), \n                               Test = round(experiment_one$Test_Proportion_Gaze_to_Singer[1:32], digits=2))\n\ndifferences <-  paired_sample_df$Test-paired_sample_df$Baseline\nt.test(differences,mu=0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  differences\nt = 2.4388, df = 31, p-value = 0.02066\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.01192088 0.13370412\nsample estimates:\nmean of x \n0.0728125 \n```\n\n\n:::\n:::\n\n\n\n\n\nNow we get a very different answer. We would summarize the results\nsaying the mean difference was .073, t(31) = 2.44, p = 0.020. How many\ntotal infants were their? Well the degrees of freedom was 31, so there\nmust have been 32 infants in the study. Now we see a much smaller\n$p$-value. This was also a two-tailed test, so we that observing a $t$\nvalue of 2.4 or greater (absolute value) only occurs 2% of the time. In\nother words, the distribution of no differences will produce the\nobserved t-value very rarely. So, it is unlikely that the observed mean\ndifference of .073 was due to chance (it could have been due to chance,\nbut that is very unlikely). As a result, we can be somewhat confident in\nconcluding that something about seeing and hearing a unfamiliar person\nsing a familiar song, causes an infant to draw their attention toward\nthe singer, and this potentially benefits social learning on the part of\nthe infant.\n\n## Independent samples t-test: The return of the t-test?\n\nIf you've been following the Star Wars references, we are on last movie\n(of the original trilogy)... the independent t-test. This is were\nbasically the same story plays out as before, only slightly different.\n\nRemember there are different $t$-tests for different kinds of research\ndesigns. When your design is a **between-subjects** design, you use an\n**independent samples t-test**. Between-subjects design involve\ndifferent people or subjects in each experimental condition. If there\nare two conditions, and 10 people in each, then there are 20 total\npeople. And, there are no paired scores, because every single person is\nmeasured once, not twice, no repeated measures. Because there are no\nrepeated measures we can't look at the difference scores between\nconditions one and two. The scores are not paired in any meaningful way,\nto it doesn't make sense to subtract them. So what do we do?\n\nThe logic of the independent samples t-test is the very same as the\nother $t$-tests. We calculated the means for each group, then we find\nthe difference. That goes into the numerator of the t formula. Then we\nget an estimate of the variation for the denominator. We divide the mean\ndifference by the estimate of the variation, and we get $t$. It's the\nsame as before.\n\nThe only wrinkle here is what goes into the denominator? How should we\ncalculate the estimate of the variance? It would be nice if we could do\nsomething very straightforward like this, say for an experiment with two\ngroups A and B:\n\n$t = \\frac{\\bar{A}-\\bar{B}}{(\\frac{SEM_A+SEM_B}{2})}$\n\nIn plain language, this is just:\n\n1.  Find the mean difference for the top part\n2.  Compute the SEM (standard error of the mean) for each group, and\n    average them together to make a single estimate, pooling over both\n    samples.\n\nThis would be nice, but unfortunately, it turns out that finding the\naverage of two standard errors of the mean is not the best way to do it.\nThis would create a biased estimator of the variation for the\nhypothesized distribution of no differences. We won't go into the math\nhere, but instead of the above formula, we an use a different one that\ngives as an **unbiased estimate of the pooled standard error of the\nsample mean**. Our new and improved $t$ formula would look like this:\n\n$t = \\frac{\\bar{X_A}-\\bar{X_B}}{s_p * \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_B}}}$\n\nand, $s_p$, which is the pooled sample standard deviation is defined as,\nnote the \\$s\\$es in the formula are variances:\n\n$s_p = \\sqrt{\\frac{(n_A-1)s_A^2 + (n_B-1)s^2_B}{n_A +n_B -2}}$\n\nBelieve you me, that is so much more formula than I wanted to type out.\nShall we do one independent $t$-test example by hand, just to see the\ncomputations? Let's do it...but in a slightly different way than you\nexpect. I show the steps using R. I made some fake scores for groups A\nand B. Then, I followed all of the steps from the formula, but made R do\neach of the calculations. This shows you the needed steps by following\nthe code. At the end, I print the $t$-test values I computed \"by hand\",\nand then the $t$-test value that the R software outputs using the\n$t$-test function. You should be able to get the same values for $t$, if\nyou were brave enough to compute $t$ by hand.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## By \"hand\" using R r code\na <- c(1,2,3,4,5)\nb <- c(3,5,4,7,9)\n\nmean_difference <- mean(a)-mean(b) # compute mean difference\n\nvariance_a <- var(a) # compute variance for A\nvariance_b <- var(b) # compute variance for B\n\n# Compute top part and bottom part of sp formula\n\nsp_numerator <- (4*variance_a + 4* variance_b) \nsp_denominator <- 5+5-2\nsp <- sqrt(sp_numerator/sp_denominator) # compute sp\n\n\n# compute t following formulat\n\nt <- mean_difference / ( sp * sqrt( (1/5) +(1/5) ) )\n\nt # print results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -2.017991\n```\n\n\n:::\n\n```{.r .cell-code}\n# using the R function t.test\nt.test(a,b, paired=FALSE, var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  a and b\nt = -2.018, df = 8, p-value = 0.0783\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -5.5710785  0.3710785\nsample estimates:\nmean of x mean of y \n      3.0       5.6 \n```\n\n\n:::\n:::\n\n\n\n\n\n## Simulating data for t-tests\n\nAn \"advanced\" topic for $t$-tests is the idea of using R to conduct\nsimulations for $t$-tests.\n\nIf you recall, $t$ is a property of a sample. We calculate $t$ from our\nsample. The $t$ distribution is the hypothetical behavior of our sample.\nThat is, if we had taken thousands upon thousands of samples, and\ncalculated $t$ for each one, and then looked at the distribution of\nthose $t$'s, we would have the sampling distribution of $t$!\n\nIt can be very useful to get in the habit of using R to simulate data\nunder certain conditions, to see how your sample data, and things like\n$t$ behave. Why is this useful? It mainly prepares you with some\nintuitions about how sampling error (random chance) can influence your\nresults, given specific parameters of your design, such as sample-size,\nthe size of the mean difference you expect to find in your data, and the\namount of variation you might find. These methods can be used formally\nto conduct power-analyses. Or more informally for data sense.\n\n### Simulating a one-sample t-test\n\nHere are the steps you might follow to simulate data for a one sample\n$t$-test.\n\n1.  Make some assumptions about what your sample (that you might be\n    planning to collect) might look like. For example, you might be\n    planning to collect 30 subjects worth of data. The scores of those\n    data points might come from a normal distribution (mean = 50, sd =\n    10).\n\n2.  sample simulated numbers from the distribution, then conduct a\n    $t$-test on the simulated numbers. Save the statistics you want\n    (such as $t$s and $p$s), and then see how things behave.\n\nLet's do this a couple different times. First, let's simulate samples\nwith N = 30, taken from a normal (mean= 50, sd = 25). We'll do a\nsimulation with 1000 simulations. For each simulation, we will compare\nthe sample mean with a population mean of 50. There should be no\ndifference on average here. @fig-6nullt is the null distribution that we\nare simulating.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# steps to create fake data from a distribution\n# and conduct t-tests on the simulated data\nsave_ps <- length(1000)\nsave_ts <- length(1000)\nfor (i in 1:1000) {\n  my_sample <- rnorm(n = 30, mean = 50, sd = 25)\n  t_test <- t.test (my_sample, mu = 50)\n  save_ps[i] <- t_test$p.value\n  save_ts[i] <- t_test$statistic\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#plot histograms of t and p values for 1000 simulations\nhist(save_ts)\n```\n\n::: {.cell-output-display}\n![The distribution of $t$-values under the null. These are the $t$ values that are produced by chance alone.](chapter_files/figure-html/fig-6nullt-1.png){#fig-6nullt width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(save_ps)\n```\n\n::: {.cell-output-display}\n![The distribution of $p$-values that are observed is flat under the null.](chapter_files/figure-html/fig-6flatp-1.png){#fig-6flatp width=672}\n:::\n:::\n\n\n\n\n\nNeat. We see both a $t$ distribution, that looks like $t$ distribution\nas it should. And we see the $p$ distribution. This shows us how often\nwe get $t$ values of particular sizes. You may find it interesting that\nthe $p$-distribution is flat under the null, which we are simulating\nhere. This means that you have the same chances of a getting a $t$ with\na p-value between 0 and 0.05, as you would for getting a $t$ with a\np-value between .90 and .95. Those ranges are both ranges of 5%, so\nthere are an equal amount of $t$ values in them by definition.\n\nHere's another way to do the same simulation in R, using the replicate\nfunction, instead a for loop:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated_ts <- replicate(1000,\n                          t.test(rnorm(30, 50, 25), mu = 50)$statistic)\nhist(simulated_ts)\n```\n\n::: {.cell-output-display}\n![Simulating $t$s in R.](chapter_files/figure-html/fig-6simtsRep-1.png){#fig-6simtsRep width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated_ps <- replicate(1000,\n                          t.test(rnorm(30, 50, 25), mu = 50)$p.value)\nhist(simulated_ps)\n```\n\n::: {.cell-output-display}\n![Simulating $p$s in R.](chapter_files/figure-html/fig-6simpsRep-1.png){#fig-6simpsRep width=672}\n:::\n:::\n\n\n\n\n\n### Simulating a paired samples t-test\n\nThe code below is set up to sample 10 scores for condition A and B from\nthe same normal distribution. The simulation is conducted 1000 times,\nand the $t$s and $p$s are saved and plotted for each.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_ps <- length(1000)\nsave_ts <- length(1000)\nfor ( i in 1:1000 ){\n  condition_A <- rnorm(10,10,5)\n  condition_B <- rnorm(10,10,5)\n  differences <- condition_A - condition_B\n  t_test <- t.test(differences, mu=0)\n  save_ps[i] <- t_test$p.value\n  save_ts[i] <- t_test$statistic\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(save_ts)\n```\n\n::: {.cell-output-display}\n![1000 simulated ts from the null distribution](chapter_files/figure-html/fig-6simps1000-1.png){#fig-6simps1000 width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(save_ps)\n```\n\n::: {.cell-output-display}\n![1000 simulated ps from the null distribution](chapter_files/figure-html/fig-6simts1000-1.png){#fig-6simts1000 width=672}\n:::\n:::\n\n\n\n\n\nAccording to the simulation. When there are no differences between the\nconditions, and the samples are being pulled from the very same\ndistribution, you get these two distributions for $t$ and $p$. These\nagain show how the null distribution of no differences behaves.\n\nFor any of these simulations, if you rejected the null-hypothesis (that\nyour difference was only due to chance), you would be making a type I\nerror. If you set your alpha criteria to $\\alpha = .05$, we can ask how\nmany type I errors were made in these 1000 simulations. The answer is:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(save_ps[save_ps<.05])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 49\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(save_ps[save_ps<.05])/1000\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.049\n```\n\n\n:::\n:::\n\n\n\n\n\nWe happened to make 49. The expectation\nover the long run is 5% type I error rates (if your alpha is .05).\n\nWhat happens if there actually is a difference in the simulated data,\nlet's set one condition to have a larger mean than the other:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_ps <- length(1000)\nsave_ts <- length(1000)\nfor ( i in 1:1000 ){\n  condition_A <- rnorm(10,10,5)\n  condition_B <- rnorm(10,13,5)\n  differences <- condition_A - condition_B\n  t_test <- t.test(differences, mu=0)\n  save_ps[i] <- t_test$p.value\n  save_ts[i] <- t_test$statistic\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(save_ts)\n```\n\n::: {.cell-output-display}\n![1000 ts when there is a true difference](chapter_files/figure-html/fig-6simtruets-1.png){#fig-6simtruets width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(save_ps)\n```\n\n::: {.cell-output-display}\n![1000 ps when there is a true difference](chapter_files/figure-html/fig-6simtrueps-1.png){#fig-6simtrueps width=672}\n:::\n:::\n\n\n\n\n\nNow you can see that the $p$-value distribution is skewed to the left.\nThis is because when there is a true effect, you will get p-values that\nare less than .05 more often. Or, rather, you get larger $t$ values than\nyou normally would if there were no differences.\n\nIn this case, we wouldn't be making a type I error if we rejected the\nnull when p was smaller than .05. How many times would we do that out of\nour 1000 experiments?\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(save_ps[save_ps<.05])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 251\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(save_ps[save_ps<.05])/1000\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.251\n```\n\n\n:::\n:::\n\n\n\n\n\nWe happened to get 251 simulations where p\nwas less than .05, that's only 0.251\nexperiments. If you were the researcher, would you want to run an\nexperiment that would be successful only\n0.251 of the time? I wouldn't. I would\nrun a better experiment.\n\nHow would you run a better simulated experiment? Well, you could\nincrease $n$, the number of subjects in the experiment. Let's increase\n$n$ from 10 to 100, and see what happens to the number of \"significant\"\nsimulated experiments.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_ps <- length(1000)\nsave_ts <- length(1000)\nfor ( i in 1:1000 ){\n  condition_A <- rnorm(100,10,5)\n  condition_B <- rnorm(100,13,5)\n  differences <- condition_A - condition_B\n  t_test <- t.test(differences, mu=0)\n  save_ps[i] <- t_test$p.value\n  save_ts[i] <- t_test$statistic\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(save_ts)\n```\n\n::: {.cell-output-display}\n![1000 ts for n =100, when there is a true effect](chapter_files/figure-html/fig-6simtsB-1.png){#fig-6simtsB width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(save_ps)\n\n\nlength(save_ps[save_ps<.05])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 993\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(save_ps[save_ps<.05])/1000\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.993\n```\n\n\n:::\n\n::: {.cell-output-display}\n![1000 ps for n =100, when there is a true effect](chapter_files/figure-html/fig-6simpsB-1.png){#fig-6simpsB width=672}\n:::\n:::\n\n\n\n\n\nCool, now almost all of the experiments show a $p$-value of less than\n.05 (using a two-tailed test, that's the default in R). See, you could\nuse this simulation process to determine how many subjects you need to\nreliably find your effect.\n\n### Simulating an independent samples t.test\n\nJust change the t.test function like so... this is for the null,\nassuming no difference between groups.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_ps <- length(1000)\nsave_ts <- length(1000)\nfor ( i in 1:1000 ){\n  group_A <- rnorm(10,10,5)\n  group_B <- rnorm(10,10,5)\n  t_test <- t.test(group_A, group_B, paired=FALSE, var.equal=TRUE)\n  save_ps[i] <- t_test$p.value\n  save_ts[i] <- t_test$statistic\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(save_ts)\n```\n\n::: {.cell-output-display}\n![1000 ts for n =100, when there is a true effect](chapter_files/figure-html/fig-6simtsC-1.png){#fig-6simtsC width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(save_ps)\n\n\nlength(save_ps[save_ps<.05])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 35\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(save_ps[save_ps<.05])/1000\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.035\n```\n\n\n:::\n\n::: {.cell-output-display}\n![1000 ps for n =100, when there is a true effect](chapter_files/figure-html/fig-6simpsC-1.png){#fig-6simpsC width=672}\n:::\n:::\n\n\n\n\n\n## Videos\n\n### One or Two tailed tests\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/PuoGsguuY30\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n\n</iframe>\n\n------------------------------------------------------------------------\n\nGordon Original \\## 1. Introduction\n\nThe t-test is a fundamental statistical method used in psychology to\ncompare the means of two groups. This chapter will explore the theory\nbehind t-tests, demonstrate practical applications using R, and provide\na structured write-up in APA style. Whether you're examining the\neffectiveness of a new therapy or comparing cognitive performance\nbetween groups, the t-test is an essential tool for psychological\nresearch.\n\n## 2. Main Content\n\n### 2.1 Theory of T-Tests\n\nA t-test evaluates whether the means of two groups are statistically\ndifferent from each other. The most common types are the independent\nsamples t-test, paired samples t-test, and one-sample t-test. Each type\ntests different hypotheses and has unique assumptions.\n\n#### 2.1.1 Independent Samples T-Test\n\nThis test compares the means of two independent groups to ascertain if\nthere is statistical evidence that the associated population means are\nsignificantly different.\n\n#### 2.1.2 Paired Samples T-Test\n\nThis test compares means from the same group at different times (e.g.,\nbefore and after a treatment) or matched pairs.\n\n#### 2.1.3 One-Sample T-Test\n\nThis test determines whether the mean of a single group is different\nfrom a known mean.\n\n### 2.2 Assumptions of T-Tests\n\n-   **Independence**: Observations within each group must be\n    independent.\n-   **Normality**: Data in each group should be approximately normally\n    distributed.\n-   **Homogeneity of Variance**: Variances in the two groups should be\n    roughly equal (for independent samples t-test).\n\n## 3. Methods and Measures\n\nTo conduct a t-test, you need to: 1. Formulate the null (H0) and\nalternative (H1) hypotheses. 2. Collect and organize data. 3. Check\nassumptions. 4. Calculate the t-statistic and degrees of freedom. 5.\nDetermine the p-value. 6. Make a decision to reject or fail to reject H0\nbased on the p-value.\n\n### Example Study\n\n**Research Question**: Does a new cognitive-behavioral therapy (CBT)\nimprove anxiety levels more than a placebo?\n\n**Hypotheses**: - H0: There is no difference in anxiety levels between\nthe CBT and placebo groups. - H1: The CBT group has lower anxiety levels\nthan the placebo group.\n\n**Measures**: Anxiety levels are measured using a standardized anxiety\ninventory.\n\n## 4. Analysis with R Code\n\nLet's use R for an independent samples t-test.\n\n### Data Setup\n\n```{webr-r}\n# Simulated data\nCBT <- c(23, 19, 21, 22, 20, 24, 25, 18, 22, 23)\nPlacebo <- c(30, 28, 27, 29, 31, 32, 33, 30, 29, 28)\n\n# Combine into a data frame\ndata <- data.frame(\n  group = factor(rep(c(\"CBT\", \"Placebo\"), each = 10)),\n  anxiety = c(CBT, Placebo)\n)\n\n# View the data\nprint(data)\n```\n\n### Conducting the T-Test\n\n```{webr-r}\n# Independent samples t-test\nt_test_results <- t.test(anxiety ~ group, data = data)\n\n# Display the results\nprint(t_test_results)\n```\n\n### R Output Interpretation\n\nThe `t.test` function provides the t-statistic, degrees of freedom, and\np-value. If the p-value is less than the significance level (e.g.,\n0.05), we reject H0.\n\n```         \n    Welch Two Sample t-test\n\ndata:  anxiety by group\nt = -8.6943, df = 17.564, p-value = 8.922e-08\nalternative hypothesis: true difference in means between group CBT and group Placebo is not equal to 0\n95 percent confidence interval:\n -9.936595 -6.063405\nsample estimates:\n    mean in group CBT mean in group Placebo \n                 21.7                  29.7\n```\n\n## 5. Debates and Controversies\n\n### 5.1 Assumption Violations\n\nViolating assumptions (e.g., normality, homogeneity of variance) can\nlead to incorrect conclusions. Robust alternatives or data\ntransformations may be necessary (Wilcox, 2012).\n\n### 5.2 P-Value Misinterpretation\n\nOverreliance on p-values can lead to misinterpretation of results. It’s\ncrucial to report effect sizes and confidence intervals (Cumming, 2014).\n\n### 5.3 Multiple Comparisons\n\nConducting multiple t-tests increases the risk of Type I errors. Methods\nlike Bonferroni correction adjust for this but can be overly\nconservative (Perneger, 1998).\n\n## 6. Conclusion\n\nT-tests are a vital statistical method in psychological research for\ncomparing means between groups. Understanding their theory, assumptions,\nand correct application in R can significantly enhance the rigor of your\nanalyses. Always ensure to check assumptions, consider effect sizes, and\nbe mindful of the limitations and debates surrounding t-tests.\n\n### References\n\nCumming, G. (2014). The new statistics: Why and how. *Psychological\nScience*, 25(1), 7-29.\n\nPerneger, T. V. (1998). What's wrong with Bonferroni adjustments. *BMJ*,\n316(7139), 1236-1238.\n\nWilcox, R. R. (2012). Modern statistics for the social and behavioral\nsciences: A practical introduction. *CRC Press*.\n\nThis chapter provides a comprehensive overview of conducting t-tests in\nR, essential for first-year psychology students. Future chapters will\ndelve into more advanced statistical methods to further enhance your\nanalytical skills.\n\n# Glossary\n\nCertainly! Here's a glossary of key terms related to 'T-Tests in R with\ntheory, practicals, and write-up in Psychology APA-style':\n\n1.  **T-Test**\n    -   **Definition**: A statistical test used to determine if there is\n        a significant difference between the means of two groups. It is\n        commonly used in hypothesis testing.\n2.  **Independent Samples T-Test**\n    -   **Definition**: A type of t-test used when comparing the means\n        of two independent groups to see if there is a significant\n        difference between them.\n3.  **Paired Samples T-Test**\n    -   **Definition**: A t-test used when comparing the means of two\n        related groups, such as measurements taken from the same\n        subjects at two different times.\n4.  **Null Hypothesis (H0)**\n    -   **Definition**: A hypothesis that there is no significant\n        difference between specified populations, any observed\n        difference being due to sampling or experimental error.\n5.  **Alternative Hypothesis (H1)**\n    -   **Definition**: A hypothesis that states there is a significant\n        difference between specified populations, contrary to the null\n        hypothesis.\n6.  **P-Value**\n    -   **Definition**: The probability that the observed results\n        occurred by chance. A p-value less than 0.05 is typically\n        considered statistically significant.\n7.  **Degrees of Freedom (df)**\n    -   **Definition**: The number of values in a calculation that are\n        free to vary. In t-tests, degrees of freedom are used to\n        determine the critical value from the t-distribution.\n8.  **Effect Size**\n    -   **Definition**: A measure of the magnitude of the difference\n        between groups. Common effect size metrics include Cohen's d.\n9.  **R Programming Language**\n    -   **Definition**: A programming language and environment commonly\n        used for statistical computing and graphics, including\n        performing t-tests.\n10. **Psychology APA-Style**\n    -   **Definition**: The writing and citation style prescribed by the\n        American Psychological Association, commonly used in psychology\n        for structuring research papers and reports.\n11. **Mean (M or μ)**\n    -   **Definition**: The average of a set of numbers, calculated by\n        summing them up and dividing by the count of numbers.\n12. **Standard Deviation (SD or σ)**\n    -   **Definition**: A measure of the amount of variation or\n        dispersion in a set of values.\n13. **Confidence Interval (CI)**\n    -   **Definition**: A range of values derived from the sample data\n        that is likely to contain the population parameter with a\n        specified level of confidence, usually 95%.\n14. **T-Distribution**\n    -   **Definition**: A type of probability distribution that is\n        symmetric and bell-shaped but has heavier tails than the normal\n        distribution. It is used in t-tests, especially when sample\n        sizes are small.\n15. **Data Frame**\n    -   **Definition**: A table or two-dimensional array-like structure\n        in R, where each column contains values of one variable and each\n        row contains one set of values from each column.\n\nThese terms will help students grasp the fundamental concepts and\ntechniques involved in performing and reporting t-tests in R, especially\nwithin the context of psychological research adhering to APA style.\n\n# Self-Assessment Quiz\n\n### Self-Assessment Quiz: T-Tests in R with Theory, Practicals, and Write-up in Psychology APA-style\n\n#### Multiple-Choice Questions\n\n**1. What is the primary purpose of a t-test in statistics?** a) To\ncompare three or more means\\\nb) To determine the relationship between two categorical variables\\\nc) To compare the means of two groups\\\nd) To analyze the variance within a single group\n\n**Answer: c) To compare the means of two groups**\n\n**2. In R, which function is commonly used to perform an independent\nt-test?** a) t_test()\\\nb) ttest()\\\nc) t.test()\\\nd) TTest()\n\n**Answer: c) t.test()**\n\n**3. What is the null hypothesis in an independent samples t-test?** a)\nThe means of the two groups are equal\\\nb) The means of the two groups are not equal\\\nc) The variances of the two groups are equal\\\nd) The variances of the two groups are not equal\n\n**Answer: a) The means of the two groups are equal**\n\n**4. Which of the following is NOT a requirement for conducting a\nt-test?** a) The data should be approximately normally distributed\\\nb) The samples should be independent\\\nc) The sample sizes must be equal\\\nd) The variances should be roughly equal (for independent t-tests)\n\n**Answer: c) The sample sizes must be equal**\n\n#### True/False Questions\n\n**5. A paired t-test is appropriate when comparing the means of two\nrelated groups.** **True**\n\n**6. In R, the `t.test()` function can only be used for independent\nt-tests.** **False**\n\n**7. The p-value in a t-test indicates the probability that the observed\ndifferences in sample means occurred by chance.** **True**\n\n**8. The degrees of freedom for an independent t-test with two samples\nof 15 participants each would be 28.** **False** (It's 28; df = n1 +\nn2 - 2, so 15 + 15 - 2 = 28)\n\n#### Short Answer Questions\n\n**9. Describe in a few sentences the steps to perform an independent\nt-test in R, including any necessary data preparation.**\n\n**Answer: To perform an independent t-test in R, follow these steps:** -\nLoad your data into R, typically using `read.csv()` or similar\nfunctions. - Inspect and clean your data to ensure it meets the\nassumptions of the t-test (e.g., checking for normality and\nindependence). - Use the `t.test()` function, specifying the two groups\nyou wish to compare. - Interpret the output, which includes the t-value,\ndegrees of freedom, and p-value, to determine whether to reject the null\nhypothesis.\n\n**10. Write an example of how to report the results of an independent\nt-test in APA style.**\n\n**Answer: An example APA-style write-up might look like this:** - \"An\nindependent samples t-test was conducted to compare the test scores for\nGroup A and Group B. There was a significant difference in scores\nbetween Group A (M = 85.3, SD = 5.2) and Group B (M = 78.4, SD = 6.1);\nt(28) = 3.45, p \\< .01. These results suggest that the intervention had\na significant effect on test scores.\"\n\n#### Answer Key\n\n1.  \n\n    c)  To compare the means of two groups\n\n2.  \n\n    c)  t.test()\n\n3.  \n\n    a)  The means of the two groups are equal\n\n4.  \n\n    c)  The sample sizes must be equal\n\n5.  True\n\n6.  False\n\n7.  True\n\n8.  False\n\n9.  Answer provided above.\n\n10. Answer provided above.\n\nThis quiz should help students test their understanding of the key\nconcepts related to t-tests in R, including theoretical understanding,\npractical application, and APA-style reporting.\n",
    "supporting": [
      "chapter_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}