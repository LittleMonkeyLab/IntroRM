[
  {
    "objectID": "webr.html",
    "href": "webr.html",
    "title": "webR in Quarto HTML Documents",
    "section": "",
    "text": "Each class session has an interactive lesson that you will work through after doing the readings and watching the lecture. These lessons are a central part of the class—they will teach you how to use {ggplot2} and other packages in the tidyverse to create beautiful and truthful visualizations with R.\nInteractive code sections look like this. Make changes in the text box and click on the green “Run Code” button to see the results. Sometimes there will be a tab with a hint or solution.\n\n\n\n\n\n\nYour turn\n\n\n\nModify the code here to show the relationship between health and wealth for 2002 instead of 2007.\n\n{{&lt; fa code &gt;}} Interactive editor{{&lt; fa lightbulb &gt;}} Hint\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHint: You’ll want to change something in the code that creates gapminder_filtered. The text in the subtitle won’t change automatically, so you’ll want to edit that too.\n\n\n\n\n\nIf you’re curious how this works, each interactive code section uses the amazing {quarto-webr} package to run R directly in your browser.\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{untitled,\n  author = {},\n  title = {webR in {Quarto} {HTML} {Documents}},\n  url = {http://introrm.littlemonkeylab.com/webr.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“webR in Quarto HTML Documents.” n.d. http://introrm.littlemonkeylab.com/webr.html.",
    "crumbs": [
      "webR in Quarto HTML Documents"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PS51020A Research Methods & Statistics",
    "section": "",
    "text": "HOMEPAGE CONTENT\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{untitled,\n  author = {},\n  title = {PS51020A {Research} {Methods} \\& {Statistics}},\n  url = {http://introrm.littlemonkeylab.com/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“PS51020A Research Methods & Statistics.” n.d. http://introrm.littlemonkeylab.com/."
  },
  {
    "objectID": "week01/dataskills.html",
    "href": "week01/dataskills.html",
    "title": "Expectations",
    "section": "",
    "text": "stuff",
    "crumbs": [
      "Week01",
      "Expectations"
    ]
  },
  {
    "objectID": "week01/dataskills.html#what-we-expect-from-you",
    "href": "week01/dataskills.html#what-we-expect-from-you",
    "title": "Expectations",
    "section": "",
    "text": "stuff",
    "crumbs": [
      "Week01",
      "Expectations"
    ]
  },
  {
    "objectID": "week01/dataskills.html#what-you-can-expect-from-us",
    "href": "week01/dataskills.html#what-you-can-expect-from-us",
    "title": "Expectations",
    "section": "What you can expect from us!",
    "text": "What you can expect from us!",
    "crumbs": [
      "Week01",
      "Expectations"
    ]
  },
  {
    "objectID": "week01/lecture-slides.html#getting-up",
    "href": "week01/lecture-slides.html#getting-up",
    "title": "Lecture 01",
    "section": "Getting up",
    "text": "Getting up\n\nTurn off alarm\nGet out of bed",
    "crumbs": [
      "Week01",
      "Lecture 01"
    ]
  },
  {
    "objectID": "week01/lecture-slides.html#breakfast",
    "href": "week01/lecture-slides.html#breakfast",
    "title": "Lecture 01",
    "section": "Breakfast",
    "text": "Breakfast\n\nEat eggs\nDrink coffee",
    "crumbs": [
      "Week01",
      "Lecture 01"
    ]
  },
  {
    "objectID": "week01/lecture-slides.html#dinner",
    "href": "week01/lecture-slides.html#dinner",
    "title": "Lecture 01",
    "section": "Dinner",
    "text": "Dinner\n\nEat spaghetti\nDrink wine",
    "crumbs": [
      "Week01",
      "Lecture 01"
    ]
  },
  {
    "objectID": "week01/lecture-slides.html#going-to-sleep",
    "href": "week01/lecture-slides.html#going-to-sleep",
    "title": "Lecture 01",
    "section": "Going to sleep",
    "text": "Going to sleep\n\nGet in bed\nCount sheep",
    "crumbs": [
      "Week01",
      "Lecture 01"
    ]
  },
  {
    "objectID": "week01/06-ttests.html",
    "href": "week01/06-ttests.html",
    "title": "06-ttests",
    "section": "",
    "text": "Back in the day, William Sealy Gosset got a job working for Guinness Breweries. They make the famous Irish stout called Guinness. What happens next went something like this (total fabrication, but mostly on point).\nGuinness wanted all of their beers to be the best beers. No mistakes, no bad beers. They wanted to improve their quality control so that when Guinness was poured anywhere in the world, it would always comes out fantastic: 5 stars out of 5 every time, the best.\nGuinness had some beer tasters, who were super-experts. Every time they tasted a Guinness from the factory that wasn’t 5 out of 5, they knew right away.\nBut, Guinness had a big problem. They would make a keg of beer, and they would want to know if every single pint that would come out would be a 5 out of 5. So, the beer tasters drank pint after pint out of the keg, until it was gone. Some kegs were all 5 out of 5s. Some weren’t, Guinness needed to fix that. But, the biggest problem was that, after the testing, there was no beer left to sell, the testers drank it all (remember I’m making this part up to illustrate a point, they probably still had beer left to sell).\nGuinness had a sampling and population problem. They wanted to know that the entire population of the beers they made were all 5 out of 5 stars. But, if they sampled the entire population, they would drink all of their beer, and wouldn’t have any left to sell.\nEnter William Sealy Gosset. Gosset figured out the solution to the problem. He asked questions like this:\n\nHow many samples do I need to take to know the whole population is 5 out of 5?\nWhat’s the fewest amount of samples I need to take to know the above, that would mean Guinness could test fewer beers for quality, sell more beers for profit, and make the product testing time shorter.\n\nGosset solved those questions, and he invented something called the Student’s t-test. Gosset was working for Guinness, and could be fired for releasing trade-secrets that he invented (the t-test). But, Gosset published the work anyways, under a pseudonym [@Student1908]. He called himself Student, hence Student’s t-test. Now you know the rest of the story.\nIt turns out this was a very nice thing for Gosset to have done. t-tests are used all the time, and they are useful, that’s why they are used. In this chapter we learn how they work.\nYou’ll be surprised to learn that what we’ve already talked about, (the Crump Test, and the Randomization Test), are both very very similar to the t-test. So, in general, you have already been thinking about the things you need to think about to understand t-tests. You’re probably wondering what is this \\(t\\), what does \\(t\\) mean? We will tell you. Before we tell what it means, we first tell you about one more idea.\n\n\nWe’ve talked about getting a sample of data. We know we can find the mean, we know we can find the standard deviation. We know we can look at the data in a histogram. These are all useful things to do for us to learn something about the properties of our data.\nYou might be thinking of the mean and standard deviation as very different things that we would not put together. The mean is about central tendency (where most of the data is), and the standard deviation is about variance (where most of the data isn’t). Yes, they are different things, but we can use them together to create useful new things.\nWhat if I told you my sample mean was 50, and I told you nothing else about my sample. Would you be confident that most of the numbers were near 50? Would you wonder if there was a lot of variability in the sample, and many of the numbers were very different from 50. You should wonder all of those things. The mean alone, just by itself, doesn’t tell you anything about well the mean represents all of the numbers in the sample.\nIt could be a representative number, when the standard deviation is very small, and all the numbers are close to 50. It could be a non-representative number, when the standard deviation is large, and many of the numbers are not near 50. You need to know the standard deviation in order to be confident in how well the mean represents the data.\nHow can we put the mean and the standard deviation together, to give us a new number that tells us about confidence in the mean?\nWe can do this using a ratio:\n\\(\\frac{mean}{\\text{standard deviation}}\\)\nThink about what happens here. We are dividing a number by a number. Look at what happens:\n\\(\\frac{number}{\\text{same number}} = 1\\)\n\\(\\frac{number}{\\text{smaller number}} = \\text{big number}\\)\ncompared to:\n\\(\\frac{number}{\\text{bigger number}} = \\text{smaller number}\\)\nImagine we have a mean of 50, and a truly small standard deviation of 1. What do we get with our formula?\n\\(\\frac{50}{1} = 50\\)\nImagine we have a mean of 50, and a big standard deviation of 100. What do we get with our formula?\n\\(\\frac{50}{100} = 0.5\\)\nNotice, when we have a mean paired with a small standard deviation, our formula gives us a big number, like 50. When we have a mean paired with a large standard deviation, our formula gives us a small number, like 0.5. These numbers can tell us something about confidence in our mean, in a general way. We can be 50 confident in our mean in the first case, and only 0.5 (not at a lot) confident in the second case.\nWhat did we do here? We created a descriptive statistic by dividing the mean by the standard deviation. And, we have a sense of how to interpret this number, when it’s big we’re more confident that the mean represents all of the numbers, when it’s small we are less confident. This is a useful kind of number, a ratio between what we think about our sample (the mean), and the variability in our sample (the standard deviation). Get used to this idea. Almost everything that follows in this textbook is based on this kind of ratio. We will see that our ratio turns into different kinds of “statistics”, and the ratios will look like this in general:\n\\(\\text{name of statistic} = \\frac{\\text{measure of what we know}}{\\text{measure of what we don't know}}\\)\nor, to say it using different words:\n\\(\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\nIn fact, this is the general formula for the t-test. Big surprise!\n\n\n\nNow we are ready to talk about t-test. We will talk about three of them. We start with the one-sample t-test.\nCommonly, the one-sample t-test is used to estimate the chances that your sample came from a particular population. Specifically, you might want to know whether the mean that you found from your sample, could have come from a particular population having a particular mean.\nStraight away, the one-sample t-test becomes a little confusing (and I haven’t even described it yet). Officially, it uses known parameters from the population, like the mean of the population and the standard deviation of the population. However, most times you don’t know those parameters of the population! So, you have to estimate them from your sample. Remember from the chapters on descriptive statistics and sampling, our sample mean is an unbiased estimate of the population mean. And, our sample standard deviation (the one where we divide by n-1) is an unbiased estimate of the population standard deviation. When Gosset developed the t-test, he recognized that he could use these estimates from his samples, to make the t-test. Here is the formula for the one sample t-test, we first use words, and then become more specific:\n\n\n\\(\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\n\\(\\text{t} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\n\\(\\text{t} = \\frac{\\text{Mean difference}}{\\text{standard error}}\\)\n\\(\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}}\\)\n\\(\\text{t} = \\frac{\\text{Sample Mean  - Population Mean}}{\\text{Sample Standard Error}}\\)\n\\(\\text{Estimated Standard Error} = \\text{Standard Error of Sample} = \\frac{s}{\\sqrt{N}}\\)\nWhere, s is the sample standard deviation.\nSome of you may have gone cross-eyed looking at all of this. Remember, we’ve seen it before when we divided our mean by the standard deviation in the first bit. The t-test is just a measure of a sample mean, divided by the standard error of the sample mean. That is it.\n\n\n\n\\(t\\) gives us a measure of confidence, just like our previous ratio for dividing the mean by a standard deviations. The only difference with \\(t\\), is that we divide by the standard error of mean (remember, this is also a standard deviation, it is the standard deviation of the sampling distribution of the mean)\n\n\n\n\n\n\nNote\n\n\n\nWhat does the t in t-test stand for? Apparently nothing. Gosset originally labelled it z. And, Fisher later called it t, perhaps because t comes after s, which is often used for the sample standard deviation.\n\n\n\\(t\\) is a property of the data that you collect. You compute it with a sample mean, and a sample standard error (there’s one more thing in the one-sample formula, the population mean, which we get to in a moment). This is why we call \\(t\\), a sample-statistic. It’s a statistic we compute from the sample.\nWhat kinds of numbers should we expect to find for these \\(ts\\)? How could we figure that out?\nLet’s start small and work through some examples. Imagine your sample mean is 5. You want to know if it came from a population that also has a mean of 5. In this case, what would \\(t\\) be? It would be zero: we first subtract the sample mean from the population mean, \\(5-5=0\\). Because the numerator is 0, \\(t\\) will be zero. So, \\(t\\) = 0, occurs, when there is no difference.\nLet’s say you take another sample, do you think the mean will be 5 every time, probably not. Let’s say the mean is 6. So, what can \\(t\\) be here? It will be a positive number, because 6-5= +1. But, will \\(t\\) be +1? That depends on the standard error of the sample. If the standard error of the sample is 1, then \\(t\\) could be 1, because 1/1 = 1.\nIf the sample standard error is smaller than 1, what happens to \\(t\\)? It get’s bigger right? For example, 1 divided by 0.5 = 2. If the sample standard error was 0.5, \\(t\\) would be 2. And, what could we do with this information? Well, it be like a measure of confidence. As \\(t\\) get’s bigger we could be more confident in the mean difference we are measuring.\nCan \\(t\\) be smaller than 1? Sure, it can. If the sample standard error is big, say like 2, then \\(t\\) will be smaller than one (in our case), e.g., 1/2 = .5. The direction of the difference between the sample mean and population mean, can also make the \\(t\\) become negative. What if our sample mean was 4. Well, then \\(t\\) will be negative, because the mean difference in the numerator will be negative, and the number in the bottom (denominator) will always be positive (remember why, it’s the standard error, computed from the sample standard deviation, which is always positive because of the squaring that we did.).\nSo, that is some intuitions about what the kinds of values t can take. \\(t\\) can be positive or negative, and big or small.\nLet’s do one more thing to build our intuitions about what \\(t\\) can look like. How about we sample some numbers and then measure the sample mean and the standard error of the mean, and then plot those two things against each each. This will show us how a sample mean typically varies with respect to the standard error of the mean.\nIn Figure 1, I pulled 1,000 samples of \\(N = 10\\) from a normal distribution (mean = 0, sd = 1). Each time I measured the mean and standard error of the sample. That gave two descriptive statistics for each sample, letting us plot each sample as dot in a scatter plot.\n\nsample_mean &lt;- length(1000)\nsample_se &lt;- length(1000)\n\nfor (i in 1:1000) {\n  s &lt;- rnorm(10, 0, 1)\n  sample_mean[i] &lt;- mean(s)\n  sample_se[i] &lt;- sd(s) / sqrt(length(s))\n}\n\nplot(sample_mean, sample_se)\n\n\n\n\n\n\n\nFigure 1: A scatter plot with sample mean on the x-axis, and standard error of the mean on the y-axis\n\n\n\n\n\nWhat we get is a cloud of dots. You might notice the cloud has a circular quality. There’s more dots in the middle, and fewer dots as they radiate out from the middle. The dot cloud shows us the general range of the sample mean, for example most of the dots are in between -1 and 1. Similarly, the range for the sample standard error is roughly between .2 and .5. Remember, each dot represents one sample.\nWe can look at the same data a different way. For example, rather than using a scatter plot, we can divide the mean for each dot by the standard error for each dot. Figure 2 shows the result in a histogram.\n\nhist(sample_mean/sample_se, breaks=30)\n\n\n\n\n\n\n\nFigure 2: A histogram of the sample means divided by the sample standard errors, this is a t-distribution.\n\n\n\n\n\nInteresting, we can see the histogram is shaped like a normal curve. It is centered on 0, which is the most common value. As values become more extreme, they become less common. If you remember, our formula for \\(t\\), was the mean divided by the standard error of the mean. That’s what we did here. This histogram is showing you a \\(t\\)-distribution.\n\n\n\nLet’s briefly calculate a t-value from a small sample. Let’s say we had 10 students do a true/false quiz with 5 questions on it. There’s a 50% chance of getting each answer correct.\nEvery student completes the 5 questions, we grade them, and then we find their performance (mean percent correct). What we want to know is whether the students were guessing. If they were all guessing, then the sample mean should be about 50%, it shouldn’t be different from chance, which is 50%. Let’s look at Table 1.\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\nTable 1: Calculating the t-value for a one-sample test.\n\n\n\n\n\n\nstudents\nscores\nmean\nDifference_from_Mean\nSquared_Deviations\n\n\n\n\n1\n50\n61\n-11\n121\n\n\n2\n70\n61\n9\n81\n\n\n3\n60\n61\n-1\n1\n\n\n4\n40\n61\n-21\n441\n\n\n5\n80\n61\n19\n361\n\n\n6\n30\n61\n-31\n961\n\n\n7\n90\n61\n29\n841\n\n\n8\n60\n61\n-1\n1\n\n\n9\n70\n61\n9\n81\n\n\n10\n60\n61\n-1\n1\n\n\nSums\n610\n610\n0\n2890\n\n\nMeans\n61\n61\n0\n289\n\n\n\n\n\nsd\n17.92\n\n\n\n\n\nSEM\n5.67\n\n\n\n\n\nt\n1.94003527336861\n\n\n\n\n\n\n\n\nYou can see the scores column has all of the test scores for each of the 10 students. We did the things we need to do to compute the standard deviation.\nRemember the sample standard deviation is the square root of the sample variance, or:\n\\(\\text{sample standard deviation} = \\sqrt{\\frac{\\sum_{i}^{n}({x_{i}-\\bar{x})^2}}{N-1}}\\)\n\\(\\text{sd} = \\sqrt{\\frac{2890}{10-1}} = 17.92\\)\nThe standard error of the mean, is the standard deviation divided by the square root of N\n\\(\\text{SEM} = \\frac{s}{\\sqrt{N}} = \\frac{17.92}{10} = 5.67\\)\n\\(t\\) is the difference between our sample mean (61), and our population mean (50, assuming chance), divided by the standard error of the mean.\n\\(\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}} = \\frac{\\bar{X}-u}{SEM} = \\frac{61-50}{5.67} = 1.94\\)\nAnd, that is you how calculate \\(t\\), by hand. It’s a pain. I was annoyed doing it this way. In the lab, you learn how to calculate \\(t\\) using software, so it will just spit out \\(t\\). For example in R, all you have to do is this:\n\nt.test(scores, mu=50)\n\n\n    One Sample t-test\n\ndata:  scores\nt = 1.9412, df = 9, p-value = 0.08415\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 48.18111 73.81889\nsample estimates:\nmean of x \n       61 \n\n\n\n\n\nIf \\(t\\) is just a number that we can compute from our sample (it is), what can we do with it? How can we use \\(t\\) for statistical inference?\nRemember back to the chapter on sampling and distributions, that’s where we discussed the sampling distribution of the sample mean. Remember, we made a lot of samples, then computed the mean for each sample, then we plotted a histogram of the sample means. Later, in that same section, we mentioned that we could generate sampling distributions for any statistic. For each sample, we could compute the mean, the standard deviation, the standard error, and now even \\(t\\), if we wanted to. We could generate 10,000 samples, and draw four histograms, one for each sampling distribution for each statistic.\nThis is exactly what I did, and the results are shown in the four panels of Figure 3 below. I used a sample size of 20, and drew random observations for each sample from a normal distribution, with mean = 0, and standard deviation = 1. Let’s look at the sampling distributions for each of the statistics. \\(t\\) was computed assuming with the population mean assumed to be 0.\n\nall_df &lt;- data.frame()\nfor (i in 1:10000) {\n  sample &lt;- rnorm(20, 0, 1)\n  sample_mean &lt;- mean(sample)\n  sample_sd &lt;- sd(sample)\n  sample_se &lt;- sd(sample) / sqrt(length(sample))\n  sample_t &lt;- as.numeric(t.test(sample, mu = 0)$statistic)\n  t_df &lt;- data.frame(i, sample_mean, sample_sd, sample_se, sample_t)\n  all_df &lt;- rbind(all_df, t_df)\n}\n\nlibrary(ggpubr)\na &lt;- ggplot(all_df, aes(x = sample_mean)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nb &lt;- ggplot(all_df, aes(x = sample_sd)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nc &lt;- ggplot(all_df, aes(x = sample_se)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nd &lt;- ggplot(all_df, aes(x = sample_t)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\n\nggarrange(a, b, c, d,\n          ncol = 2, nrow = 2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 3: Sampling distributions for the mean, standard deviation, standard error of the mean, and \\(t\\).\n\n\n\n\n\nWe see four sampling distributions. This is how statistical summaries of these summaries behave. We have used the word chance windows before. These are four chance windows, measuring different aspects of the sample. In this case, all of the samples came from the same normal distribution. Because of sampling error, each sample is not identical. The means are not identical, the standard deviations are not identical, sample standard error of the means are not identical, and the \\(t\\)s of the samples are not identical. They all have some variation, as shown by the histograms. This is how samples of size 20 behave.\nWe can see straight away, that in this case, we are unlikely to get a sample mean of 2. That’s way outside the window. The range for the sampling distribution of the mean is around -.5 to +.5, and is centered on 0 (the population mean, would you believe!).\nWe are unlikely to get sample standard deviations of between .6 and 1.5, that is a different range, specific to the sample standard deviation.\nSame thing with the sample standard error of the mean, the range here is even smaller, mostly between .1, and .3. You would rarely find a sample with a standard error of the mean greater than .3. Virtually never would you find one of say 1 (for this situation).\nNow, look at \\(t\\). It’s range is basically between -3 and +3 here. 3s barely happen at all. You pretty much never see a 5 or -5 in this situation.\nAll of these sampling windows are chance windows, and they can all be used in the same way as we have used similar sampling distributions before (e.g., Crump Test, and Randomization Test) for statistical inference. For all of them we would follow the same process:\n\nGenerate these distributions\nLook at your sample statistics for the data you have (mean, SD, SEM, and \\(t\\))\nFind the likelihood of obtaining that value or greater\nObtain that probability\nSee if you think your sample statistics were probable or improbable.\n\nWe’ll formalize this in a second. I just want you to know that what you will be doing is something that you have already done before. For example, in the Crump test and the Randomization test we focused on the distribution of mean differences. We could do that again here, but instead, we will focus on the distribution of \\(t\\) values. We then apply the same kinds of decision rules to the \\(t\\) distribution, as we did for the other distributions. Below you will see a graph you have already seen, except this time it is a distribution of \\(t\\)s, not mean differences:\nRemember, if we obtained a single \\(t\\) from one sample we collected, we could consult the chance window in Figure 4 below to find out whether the \\(t\\) we obtained from the sample was likely or unlikely to occur by chance.\n\nsample_t &lt;- all_df$sample_t\n\nggplot(all_df, aes(x = sample_t)) +\n  annotate(\n    \"rect\",\n    xmin = min(sample_t),\n    xmax = max(sample_t),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"red\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = min(sample_t),\n    xmax = -1.94,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = 1.94,\n    xmax = max(sample_t),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  geom_rect(aes(\n    xmin = -Inf,\n    xmax = min(sample_t),\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_rect(aes(\n    xmin = max(sample_t),\n    xmax = Inf,\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_histogram(bins = 50, color = \"white\") +\n  theme_classic() +\n  geom_vline(xintercept = min(sample_t)) +\n  geom_vline(xintercept = max(sample_t)) +\n  geom_vline(xintercept = -1.94) +\n  geom_vline(xintercept = 1.94) +\n  xlim(-8, 8) +\n  geom_label(data = data.frame(x = 0, y = 250, label = \"CHANCE\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -7, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 7, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  #  geom_label(data = data.frame(x = min(sample_t), y = 600,\n  #                              label = paste0(\"min \\n\",round(min(sample_t)))),\n  #                             aes(x = x, y = y, label = label))+\n  #geom_label(data = data.frame(x = max(sample_t), y = 600,\n  #                            label = paste0(\"max \\n\",round(max(sample_t)))),\n  #                           aes(x = x, y = y, label = label))+\n  geom_label(data = data.frame(x = -4, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 4, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  xlab(\"mean sample_t\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\nFigure 4: Applying decision criteria to the \\(t\\)-distribution. Histogram of \\(t\\)s from samples (n=20) drawn from the same normal distribution (u=0, sd=1)\n\n\n\n\n\n\n\n\nFrom our early example involving the TRUE/FALSE quizzes, we are now ready to make some kind of decision about what happened there. We found a mean difference of 11. We found a \\(t\\) = 1.9411765. The probability of this \\(t\\) or larger occurring is \\(p\\) = 0.0841503. We were testing the idea that our sample mean of 61 could have come from a normal distribution with mean = 50. The \\(t\\) test tells us that the \\(t\\) for our sample, or a larger one, would happen with p = 0.0841503. In other words, chance can do it a kind of small amount of time, but not often. In English, this means that all of the students could have been guessing, but it wasn’t that likely that were just guessing.\nThe next \\(t\\)-test is called a paired samples t-test. And, spoiler alert, we will find out that a paired samples t-test is actually a one-sample t-test in disguise (WHAT!), yes it is. If the one-sample \\(t\\)-test didn’t make sense to you, read the next section.\n\n\n\n\nFor me (Crump), many analyses often boil down to a paired samples t-test. It just happens that many things I do reduce down to a test like this.\nI am a cognitive psychologist, I conduct research about how people do things like remember, pay attention, and learn skills. There are lots of Psychologists like me, who do very similar things.\nWe all often conduct the same kinds of experiments. They go like this, and they are called repeated measures designs. They are called repeated measures designs, because we measure how one person does something more than once, we repeat the measure.\nSo, I might measure somebody doing something in condition A, and measure the same person doing something in Condition B, and then I see that same person does different things in the two conditions. I repeatedly measure the same person in both conditions. I am interested in whether the experimental manipulation changes something about how people perform the task in question.\n\n\nWe will introduce the paired-samples t-test with an example using real data, from a real study. @mehr20165 were interested in whether singing songs to infants helps infants become more sensitive to social cues. For example, infants might need to learn to direct their attention toward people as a part of learning how to interact socially with people. Perhaps singing songs to infants aids this process of directing attention. When an infant hears a familiar song, they might start to pay more attention to the person singing that song, even after they are done singing the song. The person who sang the song might become more socially important to the infant. You will learn more about this study in the lab for this week. This example, prepares you for the lab activities. Here is a brief summary of what they did.\nFirst, parents were trained to sing a song to their infants. After many days of singing this song to the infants, a parent came into the lab with their infant. In the first session, parents sat with their infants on their knees, so the infant could watch two video presentations. There were two videos. Each video involved two unfamiliar new people the infant had never seen before. Each new person in the video (the singers) sang one song to the infant. One singer sang the “familiar” song the infant had learned from their parents. The other singer sang an “unfamiliar” song the infant had not hear before.\nThere were two really important measurement phases: the baseline phase, and the test phase.\nThe baseline phase occurred before the infants saw and heard each singer sing a song. During the baseline phase, the infants watched a video of both singers at the same time. The researchers recorded the proportion of time that the infant looked at each singer. The baseline phase was conducted to determine whether infants had a preference to look at either person (who would later sing them a song).\nThe test phase occurred after infants saw and heard each song, sung by each singer. During the test phase, each infant had an opportunity to watch silent videos of both singers. The researchers measured the proportion of time the infants spent looking at each person. The question of interest, was whether the infants would spend a greater proportion of time looking at the singer who sang the familiar song, compared to the singer who sang the unfamiliar song.\nThere is more than one way to describe the design of this study. We will describe it like this. It was a repeated measures design, with one independent (manipulation) variable called Viewing phase: Baseline versus Test. There was one dependent variable (the measurement), which was proportion looking time (to singer who sung familiar song). This was a repeated measures design because the researchers measured proportion looking time twice (they repeated the measure), once during baseline (before infants heard each singer sing a song), and again during test (after infants head each singer sing a song).\nThe important question was whether infants would change their looking time, and look more at the singer who sang the familiar song during the test phase, than they did during the baseline phase. This is a question about a change within individual infants. In general, the possible outcomes for the study are:\n\nNo change: The difference between looking time toward the singer of the familiar song during baseline and test is zero, no difference.\nPositive change: Infants will look longer toward the singer of the familiar song during the test phase (after they saw and heard the singers), compared to the baseline phase (before they saw and heard the singers). This is a positive difference if we use the formula: Test Phase Looking time - Baseline phase looking time (to familiar song singer).\nNegative change: Infants will look longer toward the singer of the unfamiliar song during the test phase (after they saw and heard the singers), compared to the baseline phase (before they saw and heard the singers). This is a negative difference if we use the same formula: Test Phase Looking time - Baseline phase looking time (to familiar song singer).\n\n\n\n\nLet’s take a look at the data for the first 5 infants in the study. This will help us better understand some properties of the data before we analyze it. We will see that the data is structured in a particular way that we can take advantage of with a paired samples t-test. Note, we look at the first 5 infants to show how the computations work. The results of the paired-samples t-test change when we use all of the data from the study.\nHere is a table of the data:\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\n\n\n\ninfant\nBaseline\nTest\n\n\n\n\n1\n0.44\n0.60\n\n\n2\n0.41\n0.68\n\n\n3\n0.75\n0.72\n\n\n4\n0.44\n0.28\n\n\n5\n0.47\n0.50\n\n\n\n\n\nThe table shows proportion looking times toward the singer of the familiar song during the Baseline and Test phases. Notice there are five different infants, (1 to 5). Each infant is measured twice, once during the Baseline phase, and once during the Test phase. To repeat from before, this is a repeated-measures design, because the infants are measured repeatedly (twice in this case). Or, this kind of design is also called a paired-samples design. Why? because each participant comes with a pair of samples (two samples), one for each level of the design.\nGreat, so what are we really interested in here? We want to know if the mean looking time toward the singer of the familiar song for the Test phase is higher than the Baseline phase. We are comparing the two sample means against each other and looking for a difference. We already know that differences could be obtained by chance alone, simply because we took two sets of samples, and we know that samples can be different. So, we are interested in knowing whether chance was likely or unlikely to have produced any difference we might observe.\nIn other words, we are interested in looking at the difference scores between the baseline and test phase for each infant. The question here is, for each infant, did their proportion looking time to the singer of the familiar song, increase during the test phase as compared to the baseline phase.\n\n\n\nLet’s add the difference scores to the table of data so it is easier to see what we are talking about. The first step in creating difference scores is to decide how you will take the difference, there are two options:\n\nTest phase score - Baseline Phase Score\nBaseline phase score - Test Phase score\n\nLet’s use the first formula. Why? Because it will give us positive differences when the test phase score is higher than the baseline phase score. This makes a positive score meaningful with respect to the study design, we know (because we defined it to be this way), that positive scores will refer to longer proportion looking times (to singer of familiar song) during the test phase compared to the baseline phase.\n\npaired_sample_df &lt;- cbind(paired_sample_df, \n                          differences = (paired_sample_df$Test-\n                                           paired_sample_df$Baseline))\nknitr::kable(paired_sample_df)\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\n\n\n\n\n1\n0.44\n0.60\n0.16\n\n\n2\n0.41\n0.68\n0.27\n\n\n3\n0.75\n0.72\n-0.03\n\n\n4\n0.44\n0.28\n-0.16\n\n\n5\n0.47\n0.50\n0.03\n\n\n\n\n\nThere we have it, the difference scores. The first thing we can do here is look at the difference scores, and ask how many infants showed the effect of interest. Specifically, how many infants showed a positive difference score. We can see that three of five infants showed a positive difference (they looked more at the singer of the familiar song during the test than baseline phase), and two the infants showed the opposite effect (negative difference, they looked more at the singer of the familiar song during baseline than test).\n\n\n\nAs we have been discussing, the effect of interest in this study is the mean difference between the baseline and test phase proportion looking times. We can calculate the mean difference, by finding the mean of the difference scores. Let’s do that, in fact, for fun let’s calculate the mean of the baseline scores, the test scores, and the difference scores.\n\npaired_sample_df &lt;- paired_sample_df %&gt;%\n   rbind(c(\"Sums\",colSums(paired_sample_df[1:5,2:4]))) %&gt;%\n   rbind(c(\"Means\",colMeans(paired_sample_df[1:5,2:4])))\n  \nknitr::kable(paired_sample_df)\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\n\n\n\n\n1\n0.44\n0.6\n0.16\n\n\n2\n0.41\n0.68\n0.27\n\n\n3\n0.75\n0.72\n-0.03\n\n\n4\n0.44\n0.28\n-0.16\n\n\n5\n0.47\n0.5\n0.03\n\n\nSums\n2.51\n2.78\n0.27\n\n\nMeans\n0.502\n0.556\n0.054\n\n\n\n\n\nWe can see there was a positive mean difference of 0.054, between the test and baseline phases.\nCan we rush to judgment and conclude that infants are more socially attracted to individuals who have sung them a familiar song? I would hope not based on this very small sample. First, the difference in proportion looking isn’t very large, and of course we recognize that this difference could have been produced by chance.\nWe will more formally evaluate whether this difference could have been caused by chance with the paired-samples t-test. But, before we do that, let’s again calculate \\(t\\) and discuss what \\(t\\) tells us over and above what our measure of the mean of the difference scores tells us.\n\n\n\nOK, so how do we calculate \\(t\\) for a paired-samples \\(t\\)-test? Surprise, we use the one-sample t-test formula that you already learned about! Specifically, we use the one-sample \\(t\\)-test formula on the difference scores. We have one sample of difference scores (you can see they are in one column), so we can use the one-sample \\(t\\)-test on the difference scores. Specifically, we are interested in comparing whether the mean of our difference scores came from a distribution with mean difference = 0. This is a special distribution we refer to as the null distribution. It is the distribution no differences. Of course, this null distribution can produce differences due to to sampling error, but those differences are not caused by any experimental manipulation, they caused by the random sampling process.\nWe calculate \\(t\\) in a moment. Let’s now consider again why we want to calculate \\(t\\)? Why don’t we just stick with the mean difference we already have?\nRemember, the whole concept behind \\(t\\), is that it gives an indication of how confident we should be in our mean. Remember, \\(t\\) involves a measure of the mean in the numerator, divided by a measure of variation (standard error of the sample mean) in the denominator. The resulting \\(t\\) value is small when the mean difference is small, or when the variation is large. So small \\(t\\)-values tell us that we shouldn’t be that confident in the estimate of our mean difference. Large \\(t\\)-values occur when the mean difference is large and/or when the measure of variation is small. So, large \\(t\\)-values tell us that we can be more confident in the estimate of our mean difference. Let’s find \\(t\\) for the mean difference scores. We use the same formulas as we did last time:\n\n\n\n\n\n\n\n\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\ndiff_from_mean\nSquared_differences\n\n\n\n\n1\n0.44\n0.6\n0.16\n0.106\n0.011236\n\n\n2\n0.41\n0.68\n0.27\n0.216\n0.046656\n\n\n3\n0.75\n0.72\n-0.03\n-0.084\n0.00705600000000001\n\n\n4\n0.44\n0.28\n-0.16\n-0.214\n0.045796\n\n\n5\n0.47\n0.5\n0.03\n-0.024\n0.000575999999999999\n\n\nSums\n2.51\n2.78\n0.27\n0\n0.11132\n\n\nMeans\n0.502\n0.556\n0.054\n0\n0.022264\n\n\n\n\n\n\nsd\n0.167\n\n\n\n\n\n\nSEM\n0.075\n\n\n\n\n\n\nt\n0.72\n\n\n\n\n\nIf we did this test using R, we would obtain almost the same numbers (there is a little bit of rounding in the table).\n\nt.test(differences,mu=0)\n\n\n    One Sample t-test\n\ndata:  differences\nt = 0.72381, df = 4, p-value = 0.5092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.1531384  0.2611384\nsample estimates:\nmean of x \n    0.054 \n\n\nHere is a quick write up of our t-test results, t(4) = .72, p = .509.\nWhat does all of that tell us? There’s a few things we haven’t gotten into much yet. For example, the 4 represents degrees of freedom, which we discuss later. The important part, the \\(t\\) value should start to be a little bit more meaningful. We got a kind of small t-value didn’t we. It’s .72. What can we tell from this value? First, it is positive, so we know the mean difference is positive. The sign of the \\(t\\)-value is always the same as the sign of the mean difference (ours was +0.054). We can also see that the p-value was .509. We’ve seen p-values before. This tells us that our \\(t\\) value or larger, occurs about 50.9% of the time… Actually it means more than this. And, to understand it, we need to talk about the concept of two-tailed and one-tailed tests.\n\n\n\nRemember what it is we are doing here. We are evaluating whether our sample data could have come from a particular kind of distribution. The null distribution of no differences. This is the distribution of \\(t\\)-values that would occur for samples of size 5, with a mean difference of 0, and a standard error of the sample mean of .075 (this is the SEM that we calculated from our sample). We can see what this particular null-distribution looks like in Figure 5.\n\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  scale_x_continuous(breaks = (seq(-3, 3, .5))) +\n  geom_label(data = data.frame(x = -.7, y = .1, label = \"50% \\n (-)\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .7, y = .1, label = \"50% \\n (+)\"), aes(x = x, y = y, label = label)) +\n  geom_vline(xintercept = 0)+\n  theme_classic(base_size = 20)\n\n\n\n\n\n\n\nFigure 5: A distribution of \\(t\\)-values that can occur by chance alone, when there is no difference between the sample and a population\n\n\n\n\n\nThe \\(t\\)-distribution above shows us the kinds of values \\(t\\) will will take by chance alone, when we measure the mean differences for pairs of 5 samples (like our current). \\(t\\) is most likely to be zero, which is good, because we are looking at the distribution of no-differences, which should most often be 0! But, sometimes, due to sampling error, we can get \\(t\\)s that are bigger than 0, either in the positive or negative direction. Notice the distribution is symmetrical, a \\(t\\) from the null-distribution will be positive half of the time, and negative half of the time, that is what we would expect by chance.\nSo, what kind of information do we want know when we find a particular \\(t\\) value from our sample? We want to know how likely the \\(t\\) value like the one we found occurs just by chance. This is actually a subtly nuanced kind of question. For example, any particular \\(t\\) value doesn’t have a specific probability of occurring. When we talk about probabilities, we are talking about ranges of probabilities. Let’s consider some probabilities. We will use the letter \\(p\\), to talk about the probabilities of particular \\(t\\) values.\n\nWhat is the probability that \\(t\\) is zero or positive or negative? The answer is p=1, or 100%. We will always have a \\(t\\) value that is zero or non-zero…Actually, if we can’t compute the t-value, for example when the standard deviation is undefined, I guess then we would have a non-number. But, assuming we can calculate \\(t\\), then it will always be 0 or positive or negative.\nWhat is the probability of \\(t\\) = 0 or greater than 0? The answer is p=.5, or 50%. 50% of \\(t\\)-values are 0 or greater.\nWhat is the of \\(t\\) = 0 or smaller than 0? The answer is p=.5, or 50%. 50% of \\(t\\)-values are 0 or smaller.\n\nWe can answer all of those questions just by looking at our t-distribution, and dividing it into two equal regions, the left side (containing 50% of the \\(t\\) values), and the right side containing 50% of the \\(t\\)-values).\nWhat if we wanted to take a more fine-grained approach, let’s say we were interested in regions of 10%. What kinds of \\(t\\)s occur 10% of the time. We would apply lines like the following. Notice, the likelihood of bigger numbers (positive or negative) gets smaller, so we have to increase the width of the bars for each of the intervals between the bars to contain 10% of the \\(t\\)-values, it looks like Figure 6.\n\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = t_ps) +\n  theme_classic(base_size = 15) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  scale_x_continuous(breaks = round(t_ps, digits = 1)) +\n  geom_label(data = data.frame(x = -2.5, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -1.3, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 1.3, y = .2, label = \"10%\"), aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\nFigure 6: Splitting the t distribution up into regions each containing 10% of the \\(t\\)-values. The width between the bars narrows as they approach the center of the distribution, where there are more \\(t\\)-values.\n\n\n\n\n\nConsider the probabilities (\\(p\\)) of \\(t\\) for the different ranges.\n\n\\(t\\) &lt;= -1.5 (\\(t\\) is less than or equal to -1.5), \\(p\\) = 10%\n-1.5 &gt;= \\(t\\) &lt;= -0.9 (\\(t\\) is equal to or between -1.5 and -.9), \\(p\\) = 10%\n-.9 &gt;= \\(t\\) &lt;= -0.6 (\\(t\\) is equal to or between -.9 and -.6), \\(p\\) = 10%\n\\(t\\) &gt;= 1.5 (\\(t\\) is greater than or equal to 1.5), \\(p\\) = 10%\n\nNotice, that the \\(p\\)s are always 10%. \\(t\\)s occur in these ranges with 10% probability.\n\n\n\nYou might be wondering where I am getting some of these values from. For example, how do I know that 10% of \\(t\\) values (for this null distribution) have a value of approximately 1.5 or greater than 1.5? The answer is I used R to tell me.\nIn most statistics textbooks the answer would be: there is a table at the back of the book where you can look these things up…This textbook has no such table. We could make one for you. And, we might do that. But, we didn’t do that yet…\nSo, where do these values come from, how can you figure out what they are? The complicated answer is that we are not going to explain the math behind finding these values because, 1) the authors (some of us) admittedly don’t know the math well enough to explain it, and 2) it would sidetrack us to much, 3) you will learn how to get these numbers in the lab with software, 4) you will learn how to get these numbers in lab without the math, just by doing a simulation, and 5) you can do it in R, or excel, or you can use an online calculator.\nThis is all to say that you can find the \\(t\\)s and their associated \\(p\\)s using software. But, the software won’t tell you what these values mean. That’s we are doing here. You will also see that software wants to know a few more things from you, such as the degrees of freedom for the test, and whether the test is one-tailed or two tailed. We haven’t explained any of these things yet. That’s what we are going to do now. Note, we explain degrees of freedom last. First, we start with a one-tailed test.\n\n\n\nA one-tailed test is sometimes also called a directional test. It is called a directional test, because a researcher might have a hypothesis in mind suggesting that the difference they observe in their means is going to have a particular direction, either a positive difference, or a negative difference.\nTypically, a researcher would set an alpha criterion. The alpha criterion describes a line in the sand for the researcher. Often, the alpha criterion is set at \\(p = .05\\). What does this mean? Figure 7 shows the \\(t\\)-distribution and the alpha criterion.\n\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = qt(.95, 4, lower.tail = TRUE)) +\n  ggtitle(\"Critical t for one-tailed test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = qt(.95, 4, lower.tail = TRUE),\n    xmax = 3,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.95, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\nFigure 7: The critical value of t for an alpha criterion of 0.05. 5% of all ts are at this value or larger\n\n\n\n\n\nThe figure shows that \\(t\\) values of +2.13 or greater occur 5% of the time. Because the t-distribution is symmetrical, we also know that \\(t\\) values of -2.13 or smaller also occur 5% of the time. Both of these properties are true under the null distribution of no differences. This means, that when there really are no differences, a researcher can expect to find \\(t\\) values of 2.13 or larger 5% of the time.\nLet’s review and connect some of the terms:\n\nalpha criterion: the criterion set by the researcher to make decisions about whether they believe chance did or did not cause the difference. The alpha criterion here is set to \\(p = .05\\).\nCritical \\(t\\). The critical \\(t\\) is the \\(t\\)-value associated with the alpha-criterion. In this case for a one-tailed test, it is the \\(t\\) value where 5% of all \\(t\\)s are this number or greater. In our example, the critical \\(t\\) is 2.13. 5% of all \\(t\\) values (with degrees of freedom = 4) are +2.13, or greater than +2.13.\nObserved \\(t\\). The observed \\(t\\) is the one that you calculated from your sample. In our example about the infants, the observed \\(t\\) was \\(t\\) (4) = 0.72.\np-value. The \\(p\\)-value is the probability of obtaining the observed \\(t\\) value or larger. Now, you could look back at our previous example, and find that the \\(p\\)-value for \\(t\\) (4) = .72, was \\(p = .509\\) . HOWEVER, this p-value was not calculated for a one-directional test…(we talk about what .509 means in the next section).\n\nFigure 8 shows what the \\(p\\)-value for \\(t\\) (4) = .72 using a one-directional test would would look like:\n\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = .72) +\n  geom_vline(xintercept = qt(.95, 4, lower.tail = TRUE)) +\n  ggtitle(\"t value and p-range for one-directional test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = .72,\n    xmax = 3,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = qt(.95, 4, lower.tail = TRUE),\n    xmax = 3,\n    ymin = 0,\n    ymax = .25,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.95, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .72, y = .1,\n                               label = \"Observed t\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .72, y = .05,\n                               label = \".72, p=\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = 1.5,\n    y = .05,\n    label = round(pt(.72, 4, lower.tail = FALSE), digits =\n                    3)\n  ),\n  aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\nFigure 8: A case where the observed value of t is much less than the critical value for a one-directional t-test.\n\n\n\n\n\nLet’s take this one step at a time. We have located the observed \\(t\\) of .72 on the graph. We shaded the right region all grey. What we see is that the grey region represents .256 or 25.6% of all \\(t\\) values. In other words, 25.6% of \\(t\\) values are .72 or larger than .72. You could expect, by chance alone, to a find a \\(t\\) value of .72 or larger, 25.6% of the time. That’s fairly often. We did find a \\(t\\) value of .72. Now that you know this kind of \\(t\\) value or larger occurs 25.6% of the time, would you be confident that the mean difference was not due to chance? Probably not, given that chance can produce this difference fairly often.\nFollowing the “standard” decision making procedure, we would claim that our \\(t\\) value was not statistically significant, because it was not large enough. If our observed value was larger than the critical \\(t\\) (larger than 2.13), defined by our alpha criterion, then we would claim that our \\(t\\) value was statistically significant. This would be equivalent to saying that we believe it is unlikely that the difference we observed was due to chance. In general, for any observed \\(t\\) value, the associated \\(p\\)-value tells you how likely a \\(t\\) of the observed size or larger would be observed. The \\(p\\)-value always refers to a range of \\(t\\)-values, never to a single \\(t\\)-value. Researchers use the alpha criterion of .05, as a matter of convenience and convention. There are other ways to interpret these values that do not rely on a strict (significant versus not) dichotomy.\n\n\n\nOK, so that was one-tailed tests… What are two tailed tests? The \\(p\\)-value that we originally calculated from our paired-samples \\(t\\)-test was for a 2-tailed test. Often, the default is that the \\(p\\)-value is for a two-tailed test.\nThe two-tailed test, is asking a more general question about whether a difference is likely to have been produced by chance. The question is: what is probability of any difference. It is also called a non-directional test, because here we don’t care about the direction or sign of the difference (positive or negative), we just care if there is any kind of difference.\nThe same basic things as before are involved. We define an alpha criterion (\\(\\alpha = 0.05\\)). And, we say that any observed \\(t\\) value that has a probability of \\(p\\) &lt;.05 (\\(p\\) is less than .05) will be called statistically significant, and ones that are more likely (\\(p\\) &gt;.05, \\(p\\) is greater than .05) will be called null-results, or not statistically significant. The only difference is how we draw the alpha range. Before it was on the right side of the \\(t\\) distribution (we were conducting a one-sided test remember, so we were only interested in one side).\nFigure 9 shows what the most extreme 5% of the \\(t\\)-values are when we ignore their sign (whether they are positive or negative).\n\nrange &lt;- seq(-4, 4, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = qt(.975, 4, lower.tail = TRUE)) +\n  geom_vline(xintercept = qt(.025, 4, lower.tail = TRUE)) +\n  ggtitle(\"Critical ts for two-tailed test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = qt(.975, 4, lower.tail = TRUE),\n    xmax = 4,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 3.5, y = .2, label = \"2.5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.975, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.975, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.975, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label)) +\n  annotate(\n    \"rect\",\n    xmin = -4,\n    xmax = qt(.025, 4, lower.tail = TRUE),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = -3.5, y = .2, label = \"2.5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.025, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.025, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.025, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\nFigure 9: Critical values for a two-tailed test. Each line represents the location where 2.5% of all \\(t\\)s are larger or smaller than critical value. The total for both tails is 5%\n\n\n\n\n\nHere is what we are seeing. A distribution of no differences (the null, which is what we are looking at), will produce \\(t\\)s that are 2.78 or greater 2.5% of the time, and \\(t\\)s that are -2.78 or smaller 2.5% of the time. 2.5% + 2.5% is a total of 5% of the time. We could also say that \\(t\\)s larger than +/- 2.78 occur 5% of the time.\nAs a result, the critical \\(t\\) value is (+/-) 2.78 for a two-tailed test. As you can see, the two-tailed test is blind to the direction or sign of the difference. Because of this, the critical \\(t\\) value is also higher for a two-tailed test, than for the one-tailed test that we did earlier. Hopefully, now you can see why it is called a two-tailed test. There are two tails of the distribution, one on the left and right, both shaded in green.\n\n\n\nNow that you know there are two kinds of tests, one-tailed, and two-tailed, which one should you use? There is some conventional wisdom on this, but also some debate. In the end, it is up to you to be able to justify your choice and why it is appropriate for you data. That is the real answer.\nThe conventional answer is that you use a one-tailed test when you have a theory or hypothesis that is making a directional prediction (the theory predicts that the difference will be positive, or negative). Similarly, use a two-tailed test when you are looking for any difference, and you don’t have a theory that makes a directional prediction (it just makes the prediction that there will be a difference, either positive or negative).\nAlso, people appear to choose one or two-tailed tests based on how risky they are as researchers. If you always ran one-tailed tests, your critical \\(t\\) values for your set alpha criterion would always be smaller than the critical \\(t\\)s for a two-tailed test. Over the long run, you would make more type I errors, because the criterion to detect an effect is a lower bar for one than two tailed tests.\n\nRemember type 1 errors occur when you reject the idea that chance could have caused your difference. You often never know when you make this error. It happens anytime that sampling error was the actual cause of the difference, but a researcher dismisses that possibility and concludes that their manipulation caused the difference.\n\nSimilarly, if you always ran two-tailed tests, even when you had a directional prediction, you would make fewer type I errors over the long run, because the \\(t\\) for a two-tailed test is higher than the \\(t\\) for a one-tailed test. It seems quite common for researchers to use a more conservative two-tailed test, even when they are making a directional prediction based on theory. In practice, researchers tend to adopt a standard for reporting that is common in their field. Whether or not the practice is justifiable can sometimes be an open question. The important task for any researcher, or student learning statistics, is to be able to justify their choice of test.\n\n\n\nBefore we finish up with paired-samples \\(t\\)-tests, we should talk about degrees of freedom. Our sense is that students don’t really understand degrees of freedom very well. If you are reading this textbook, you are probably still wondering what is degrees of freedom, seeing as we haven’t really talked about it all.\nFor the \\(t\\)-test, there is a formula for degrees of freedom. For the one-sample and paired sample \\(t\\)-tests, the formula is:\n\\(\\text{Degrees of Freedom} = \\text{df} = n-1\\). Where n is the number of samples in the test.\nIn our paired \\(t\\)-test example, there were 5 infants. Therefore, degrees of freedom = 5-1 = 4.\nOK, that’s a formula. Who cares about degrees of freedom, what does the number mean? And why do we report it when we report a \\(t\\)-test… you’ve probably noticed the number in parentheses e.g., \\(t\\)(4)=.72, the 4 is the \\(df\\), or degrees of freedom.\nDegrees of freedom is both a concept, and a correction. The concept is that if you estimate a property of the numbers, and you use this estimate, you will be forcing some constraints on your numbers.\nConsider the numbers: 1, 2, 3. The mean of these numbers is 2. Now, let’s say I told you that the mean of three numbers is 2. Then, how many of these three numbers have freedom? Funny question right. What we mean is, how many of the three numbers could be any number, or have the freedom to be any number.\nThe first two numbers could be any number. But, once those two numbers are set, the final number (the third number), MUST be a particular number that makes the mean 2. The first two numbers have freedom. The third number has no freedom.\nTo illustrate. Let’s freely pick two numbers: 51 and -3. I used my personal freedom to pick those two numbers. Now, if our three numbers are 51, -3, and x, and the mean of these three numbers is 2. There is only one solution, x has to be -42, otherwise the mean won’t be 2. This is one way to think about degrees of freedom. The degrees of freedom for these three numbers is n-1 = 3-1= 2, because 2 of the numbers can be free, but the last number has no freedom, it becomes fixed after the first two are decided.\nNow, statisticians often apply degrees of freedom to their calculations, especially when a second calculation relies on an estimated value. For example, when we calculate the standard deviation of a sample, we first calculate the mean of the sample right! By estimating the mean, we are fixing an aspect of our sample, and so, our sample now has n-1 degrees of freedom when we calculate the standard deviation (remember for the sample standard deviation, we divide by n-1…there’s that n-1 again.)\n\n\nThere are at least two ways to think the degrees of freedom for a \\(t\\)-test. For example, if you want to use math to compute aspects of the \\(t\\) distribution, then you need the degrees of freedom to plug in to the formula… If you want to see the formulas I’m talking about, scroll down on the \\(t\\)-test wikipedia page and look for the probability density or cumulative distribution functions…We think that is quite scary for most people, and one reason why degrees of freedom are not well-understood.\nIf we wanted to simulate the \\(t\\) distribution we could more easily see what influence degrees of freedom has on the shape of the distribution. Remember, \\(t\\) is a sample statistic, it is something we measure from the sample. So, we could simulate the process of measuring \\(t\\) from many different samples, then plot the histogram of \\(t\\) to show us the simulated \\(t\\) distribution.\n\nts &lt;- c(rt(10000, 4), rt(10000, 100))\ndfs &lt;- as.factor(rep(c(4, 100), each = 10000))\n\nt_df &lt;- data.frame(dfs, ts)\nt_df &lt;- t_df[abs(t_df$ts) &lt; 5, ]\n\nggplot(t_df, aes(x = ts, group = dfs, color = dfs)) +\n  geom_histogram() +\n  theme_classic() +\n  facet_wrap( ~ dfs) +\n  theme_classic(base_size=15)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 10: The width of the t distribution shrinks as sample size and degrees of freedom (from 4 to 100) increases.\n\n\n\n\n\nIn Figure 10 notice that the red distribution for \\(df = 4\\), is a little bit shorter, and a little bit wider than the bluey-green distribution for \\(df = 100\\). As degrees of freedom increase the \\(t\\) distribution gets taller (in the middle), and narrower in the range. It get’s more peaky. Can you guess the reason for this? Remember, we are estimating a sample statistic, and degrees of freedom is really just a number that refers to the number of subjects (well minus one). And, we already know that as we increase \\(n\\), our sample statistics become better estimates (less variance) of the distributional parameters they are estimating. So, \\(t\\) becomes a better estimate of it’s “true” value as sample size increase, resulting in a more narrow distribution of \\(t\\)s.\nThere is a slightly different \\(t\\) distribution for every degrees of freedom, and the critical regions associated with 5% of the extreme values are thus slightly different every time. This is why we report the degrees of freedom for each t-test, they define the distribution of \\(t\\) values for the sample-size in question. Why do we use n-1 and not n? Well, we calculate \\(t\\) using the sample standard deviation to estimate the standard error or the mean, that estimate uses n-1 in the denominator, so our \\(t\\) distribution is built assuming n-1. That’s enough for degrees of freedom…\n\n\n\n\n\nYou must be wondering if we will ever be finished talking about paired samples t-tests… why are we doing round 2, oh no! Don’t worry, we’re just going to 1) remind you about what we were doing with the infant study, and 2) do a paired samples t-test on the entire data set and discuss.\nRemember, we were wondering if the infants would look longer toward the singer who sang the familiar song during the test phase compared to the baseline phase. We showed you data from 5 infants, and walked through the computations for the \\(t\\)-test. As a reminder, it looked like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\ndiff_from_mean\nSquared_differences\n\n\n\n\n1\n0.44\n0.6\n0.16\n0.106\n0.011236\n\n\n2\n0.41\n0.68\n0.27\n0.216\n0.046656\n\n\n3\n0.75\n0.72\n-0.03\n-0.084\n0.00705600000000001\n\n\n4\n0.44\n0.28\n-0.16\n-0.214\n0.045796\n\n\n5\n0.47\n0.5\n0.03\n-0.024\n0.000575999999999999\n\n\nSums\n2.51\n2.78\n0.27\n0\n0.11132\n\n\nMeans\n0.502\n0.556\n0.054\n0\n0.022264\n\n\n\n\n\n\nsd\n0.167\n\n\n\n\n\n\nSEM\n0.075\n\n\n\n\n\n\nt\n0.72\n\n\n\n\n\n\n    One Sample t-test\n\ndata:  round(differences, digits = 2)\nt = 0.72381, df = 4, p-value = 0.5092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.1531384  0.2611384\nsample estimates:\nmean of x \n    0.054 \n\n\nLet’s write down the finding one more time: The mean difference was 0.054, \\(t\\)(4) = .72, \\(p\\) =.509. We can also now confirm, that the \\(p\\)-value was from a two-tailed test. So, what does this all really mean.\nWe can say that a \\(t\\) value with an absolute of .72 or larger occurs 50.9% of the time. More precisely, the distribution of no differences (the null), will produce a \\(t\\) value this large or larger 50.9% of the time. In other words, chance alone good have easily produced the \\(t\\) value from our sample, and the mean difference we observed or .054, could easily have been a result of chance.\nLet’s quickly put all of the data in the \\(t\\)-test, and re-run the test using all of the infant subjects.\n\npaired_sample_df &lt;-  data.frame(infant=1:32, \n                               Baseline = round(experiment_one$Baseline_Proportion_Gaze_to_Singer[1:32],digits=2), \n                               Test = round(experiment_one$Test_Proportion_Gaze_to_Singer[1:32], digits=2))\n\ndifferences &lt;-  paired_sample_df$Test-paired_sample_df$Baseline\nt.test(differences,mu=0)\n\n\n    One Sample t-test\n\ndata:  differences\nt = 2.4388, df = 31, p-value = 0.02066\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.01192088 0.13370412\nsample estimates:\nmean of x \n0.0728125 \n\n\nNow we get a very different answer. We would summarize the results saying the mean difference was .073, t(31) = 2.44, p = 0.020. How many total infants were their? Well the degrees of freedom was 31, so there must have been 32 infants in the study. Now we see a much smaller \\(p\\)-value. This was also a two-tailed test, so we that observing a \\(t\\) value of 2.4 or greater (absolute value) only occurs 2% of the time. In other words, the distribution of no differences will produce the observed t-value very rarely. So, it is unlikely that the observed mean difference of .073 was due to chance (it could have been due to chance, but that is very unlikely). As a result, we can be somewhat confident in concluding that something about seeing and hearing a unfamiliar person sing a familiar song, causes an infant to draw their attention toward the singer, and this potentially benefits social learning on the part of the infant.\n\n\n\nIf you’ve been following the Star Wars references, we are on last movie (of the original trilogy)… the independent t-test. This is were basically the same story plays out as before, only slightly different.\nRemember there are different \\(t\\)-tests for different kinds of research designs. When your design is a between-subjects design, you use an independent samples t-test. Between-subjects design involve different people or subjects in each experimental condition. If there are two conditions, and 10 people in each, then there are 20 total people. And, there are no paired scores, because every single person is measured once, not twice, no repeated measures. Because there are no repeated measures we can’t look at the difference scores between conditions one and two. The scores are not paired in any meaningful way, to it doesn’t make sense to subtract them. So what do we do?\nThe logic of the independent samples t-test is the very same as the other \\(t\\)-tests. We calculated the means for each group, then we find the difference. That goes into the numerator of the t formula. Then we get an estimate of the variation for the denominator. We divide the mean difference by the estimate of the variation, and we get \\(t\\). It’s the same as before.\nThe only wrinkle here is what goes into the denominator? How should we calculate the estimate of the variance? It would be nice if we could do something very straightforward like this, say for an experiment with two groups A and B:\n\\(t = \\frac{\\bar{A}-\\bar{B}}{(\\frac{SEM_A+SEM_B}{2})}\\)\nIn plain language, this is just:\n\nFind the mean difference for the top part\nCompute the SEM (standard error of the mean) for each group, and average them together to make a single estimate, pooling over both samples.\n\nThis would be nice, but unfortunately, it turns out that finding the average of two standard errors of the mean is not the best way to do it. This would create a biased estimator of the variation for the hypothesized distribution of no differences. We won’t go into the math here, but instead of the above formula, we an use a different one that gives as an unbiased estimate of the pooled standard error of the sample mean. Our new and improved \\(t\\) formula would look like this:\n\\(t = \\frac{\\bar{X_A}-\\bar{X_B}}{s_p * \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_B}}}\\)\nand, \\(s_p\\), which is the pooled sample standard deviation is defined as, note the $s$es in the formula are variances:\n\\(s_p = \\sqrt{\\frac{(n_A-1)s_A^2 + (n_B-1)s^2_B}{n_A +n_B -2}}\\)\nBelieve you me, that is so much more formula than I wanted to type out. Shall we do one independent \\(t\\)-test example by hand, just to see the computations? Let’s do it…but in a slightly different way than you expect. I show the steps using R. I made some fake scores for groups A and B. Then, I followed all of the steps from the formula, but made R do each of the calculations. This shows you the needed steps by following the code. At the end, I print the \\(t\\)-test values I computed “by hand”, and then the \\(t\\)-test value that the R software outputs using the \\(t\\)-test function. You should be able to get the same values for \\(t\\), if you were brave enough to compute \\(t\\) by hand.\n\n## By \"hand\" using R r code\na &lt;- c(1,2,3,4,5)\nb &lt;- c(3,5,4,7,9)\n\nmean_difference &lt;- mean(a)-mean(b) # compute mean difference\n\nvariance_a &lt;- var(a) # compute variance for A\nvariance_b &lt;- var(b) # compute variance for B\n\n# Compute top part and bottom part of sp formula\n\nsp_numerator &lt;- (4*variance_a + 4* variance_b) \nsp_denominator &lt;- 5+5-2\nsp &lt;- sqrt(sp_numerator/sp_denominator) # compute sp\n\n\n# compute t following formulat\n\nt &lt;- mean_difference / ( sp * sqrt( (1/5) +(1/5) ) )\n\nt # print results\n\n[1] -2.017991\n\n# using the R function t.test\nt.test(a,b, paired=FALSE, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  a and b\nt = -2.018, df = 8, p-value = 0.0783\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -5.5710785  0.3710785\nsample estimates:\nmean of x mean of y \n      3.0       5.6 \n\n\n\n\n\nAn “advanced” topic for \\(t\\)-tests is the idea of using R to conduct simulations for \\(t\\)-tests.\nIf you recall, \\(t\\) is a property of a sample. We calculate \\(t\\) from our sample. The \\(t\\) distribution is the hypothetical behavior of our sample. That is, if we had taken thousands upon thousands of samples, and calculated \\(t\\) for each one, and then looked at the distribution of those \\(t\\)’s, we would have the sampling distribution of \\(t\\)!\nIt can be very useful to get in the habit of using R to simulate data under certain conditions, to see how your sample data, and things like \\(t\\) behave. Why is this useful? It mainly prepares you with some intuitions about how sampling error (random chance) can influence your results, given specific parameters of your design, such as sample-size, the size of the mean difference you expect to find in your data, and the amount of variation you might find. These methods can be used formally to conduct power-analyses. Or more informally for data sense.\n\n\nHere are the steps you might follow to simulate data for a one sample \\(t\\)-test.\n\nMake some assumptions about what your sample (that you might be planning to collect) might look like. For example, you might be planning to collect 30 subjects worth of data. The scores of those data points might come from a normal distribution (mean = 50, sd = 10).\nsample simulated numbers from the distribution, then conduct a \\(t\\)-test on the simulated numbers. Save the statistics you want (such as \\(t\\)s and \\(p\\)s), and then see how things behave.\n\nLet’s do this a couple different times. First, let’s simulate samples with N = 30, taken from a normal (mean= 50, sd = 25). We’ll do a simulation with 1000 simulations. For each simulation, we will compare the sample mean with a population mean of 50. There should be no difference on average here. Figure 11 is the null distribution that we are simulating.\n\n# steps to create fake data from a distribution\n# and conduct t-tests on the simulated data\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor (i in 1:1000) {\n  my_sample &lt;- rnorm(n = 30, mean = 50, sd = 25)\n  t_test &lt;- t.test (my_sample, mu = 50)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n#plot histograms of t and p values for 1000 simulations\nhist(save_ts)\n\n\n\n\n\n\n\nFigure 11: The distribution of \\(t\\)-values under the null. These are the \\(t\\) values that are produced by chance alone.\n\n\n\n\n\n\nhist(save_ps)\n\n\n\n\n\n\n\nFigure 12: The distribution of \\(p\\)-values that are observed is flat under the null.\n\n\n\n\n\nNeat. We see both a \\(t\\) distribution, that looks like \\(t\\) distribution as it should. And we see the \\(p\\) distribution. This shows us how often we get \\(t\\) values of particular sizes. You may find it interesting that the \\(p\\)-distribution is flat under the null, which we are simulating here. This means that you have the same chances of a getting a \\(t\\) with a p-value between 0 and 0.05, as you would for getting a \\(t\\) with a p-value between .90 and .95. Those ranges are both ranges of 5%, so there are an equal amount of \\(t\\) values in them by definition.\nHere’s another way to do the same simulation in R, using the replicate function, instead a for loop:\n\nsimulated_ts &lt;- replicate(1000,\n                          t.test(rnorm(30, 50, 25), mu = 50)$statistic)\nhist(simulated_ts)\n\n\n\n\n\n\n\nFigure 13: Simulating \\(t\\)s in R.\n\n\n\n\n\n\nsimulated_ps &lt;- replicate(1000,\n                          t.test(rnorm(30, 50, 25), mu = 50)$p.value)\nhist(simulated_ps)\n\n\n\n\n\n\n\nFigure 14: Simulating \\(p\\)s in R.\n\n\n\n\n\n\n\n\nThe code below is set up to sample 10 scores for condition A and B from the same normal distribution. The simulation is conducted 1000 times, and the \\(t\\)s and \\(p\\)s are saved and plotted for each.\n\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(10,10,5)\n  condition_B &lt;- rnorm(10,10,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\nhist(save_ts)\n\n\n\n\n\n\n\nFigure 15: 1000 simulated ts from the null distribution\n\n\n\n\n\n\nhist(save_ps)\n\n\n\n\n\n\n\nFigure 16: 1000 simulated ps from the null distribution\n\n\n\n\n\nAccording to the simulation. When there are no differences between the conditions, and the samples are being pulled from the very same distribution, you get these two distributions for \\(t\\) and \\(p\\). These again show how the null distribution of no differences behaves.\nFor any of these simulations, if you rejected the null-hypothesis (that your difference was only due to chance), you would be making a type I error. If you set your alpha criteria to \\(\\alpha = .05\\), we can ask how many type I errors were made in these 1000 simulations. The answer is:\n\nlength(save_ps[save_ps&lt;.05])\n\n[1] 52\n\nlength(save_ps[save_ps&lt;.05])/1000\n\n[1] 0.052\n\n\nWe happened to make 52. The expectation over the long run is 5% type I error rates (if your alpha is .05).\nWhat happens if there actually is a difference in the simulated data, let’s set one condition to have a larger mean than the other:\n\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(10,10,5)\n  condition_B &lt;- rnorm(10,13,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\nhist(save_ts)\n\n\n\n\n\n\n\nFigure 17: 1000 ts when there is a true difference\n\n\n\n\n\n\nhist(save_ps)\n\n\n\n\n\n\n\nFigure 18: 1000 ps when there is a true difference\n\n\n\n\n\nNow you can see that the \\(p\\)-value distribution is skewed to the left. This is because when there is a true effect, you will get p-values that are less than .05 more often. Or, rather, you get larger \\(t\\) values than you normally would if there were no differences.\nIn this case, we wouldn’t be making a type I error if we rejected the null when p was smaller than .05. How many times would we do that out of our 1000 experiments?\n\nlength(save_ps[save_ps&lt;.05])\n\n[1] 250\n\nlength(save_ps[save_ps&lt;.05])/1000\n\n[1] 0.25\n\n\nWe happened to get 250 simulations where p was less than .05, that’s only 0.25 experiments. If you were the researcher, would you want to run an experiment that would be successful only 0.25 of the time? I wouldn’t. I would run a better experiment.\nHow would you run a better simulated experiment? Well, you could increase \\(n\\), the number of subjects in the experiment. Let’s increase \\(n\\) from 10 to 100, and see what happens to the number of “significant” simulated experiments.\n\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(100,10,5)\n  condition_B &lt;- rnorm(100,13,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\nhist(save_ts)\n\n\n\n\n\n\n\nFigure 19: 1000 ts for n =100, when there is a true effect\n\n\n\n\n\n\nhist(save_ps)\n\n\nlength(save_ps[save_ps&lt;.05])\n\n[1] 991\n\nlength(save_ps[save_ps&lt;.05])/1000\n\n[1] 0.991\n\n\n\n\n\n\n\n\nFigure 20: 1000 ps for n =100, when there is a true effect\n\n\n\n\n\nCool, now almost all of the experiments show a \\(p\\)-value of less than .05 (using a two-tailed test, that’s the default in R). See, you could use this simulation process to determine how many subjects you need to reliably find your effect.\n\n\n\nJust change the t.test function like so… this is for the null, assuming no difference between groups.\n\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  group_A &lt;- rnorm(10,10,5)\n  group_B &lt;- rnorm(10,10,5)\n  t_test &lt;- t.test(group_A, group_B, paired=FALSE, var.equal=TRUE)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\nhist(save_ts)\n\n\n\n\n\n\n\nFigure 21: 1000 ts for n =100, when there is a true effect\n\n\n\n\n\n\nhist(save_ps)\n\n\nlength(save_ps[save_ps&lt;.05])\n\n[1] 50\n\nlength(save_ps[save_ps&lt;.05])/1000\n\n[1] 0.05\n\n\n\n\n\n\n\n\nFigure 22: 1000 ps for n =100, when there is a true effect",
    "crumbs": [
      "Week01",
      "06-ttests"
    ]
  },
  {
    "objectID": "week01/06-ttests.html#check-your-confidence-in-your-mean",
    "href": "week01/06-ttests.html#check-your-confidence-in-your-mean",
    "title": "06-ttests",
    "section": "",
    "text": "We’ve talked about getting a sample of data. We know we can find the mean, we know we can find the standard deviation. We know we can look at the data in a histogram. These are all useful things to do for us to learn something about the properties of our data.\nYou might be thinking of the mean and standard deviation as very different things that we would not put together. The mean is about central tendency (where most of the data is), and the standard deviation is about variance (where most of the data isn’t). Yes, they are different things, but we can use them together to create useful new things.\nWhat if I told you my sample mean was 50, and I told you nothing else about my sample. Would you be confident that most of the numbers were near 50? Would you wonder if there was a lot of variability in the sample, and many of the numbers were very different from 50. You should wonder all of those things. The mean alone, just by itself, doesn’t tell you anything about well the mean represents all of the numbers in the sample.\nIt could be a representative number, when the standard deviation is very small, and all the numbers are close to 50. It could be a non-representative number, when the standard deviation is large, and many of the numbers are not near 50. You need to know the standard deviation in order to be confident in how well the mean represents the data.\nHow can we put the mean and the standard deviation together, to give us a new number that tells us about confidence in the mean?\nWe can do this using a ratio:\n\\(\\frac{mean}{\\text{standard deviation}}\\)\nThink about what happens here. We are dividing a number by a number. Look at what happens:\n\\(\\frac{number}{\\text{same number}} = 1\\)\n\\(\\frac{number}{\\text{smaller number}} = \\text{big number}\\)\ncompared to:\n\\(\\frac{number}{\\text{bigger number}} = \\text{smaller number}\\)\nImagine we have a mean of 50, and a truly small standard deviation of 1. What do we get with our formula?\n\\(\\frac{50}{1} = 50\\)\nImagine we have a mean of 50, and a big standard deviation of 100. What do we get with our formula?\n\\(\\frac{50}{100} = 0.5\\)\nNotice, when we have a mean paired with a small standard deviation, our formula gives us a big number, like 50. When we have a mean paired with a large standard deviation, our formula gives us a small number, like 0.5. These numbers can tell us something about confidence in our mean, in a general way. We can be 50 confident in our mean in the first case, and only 0.5 (not at a lot) confident in the second case.\nWhat did we do here? We created a descriptive statistic by dividing the mean by the standard deviation. And, we have a sense of how to interpret this number, when it’s big we’re more confident that the mean represents all of the numbers, when it’s small we are less confident. This is a useful kind of number, a ratio between what we think about our sample (the mean), and the variability in our sample (the standard deviation). Get used to this idea. Almost everything that follows in this textbook is based on this kind of ratio. We will see that our ratio turns into different kinds of “statistics”, and the ratios will look like this in general:\n\\(\\text{name of statistic} = \\frac{\\text{measure of what we know}}{\\text{measure of what we don't know}}\\)\nor, to say it using different words:\n\\(\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\nIn fact, this is the general formula for the t-test. Big surprise!",
    "crumbs": [
      "Week01",
      "06-ttests"
    ]
  },
  {
    "objectID": "week01/06-ttests.html#one-sample-t-test-a-new-t-test",
    "href": "week01/06-ttests.html#one-sample-t-test-a-new-t-test",
    "title": "06-ttests",
    "section": "",
    "text": "Now we are ready to talk about t-test. We will talk about three of them. We start with the one-sample t-test.\nCommonly, the one-sample t-test is used to estimate the chances that your sample came from a particular population. Specifically, you might want to know whether the mean that you found from your sample, could have come from a particular population having a particular mean.\nStraight away, the one-sample t-test becomes a little confusing (and I haven’t even described it yet). Officially, it uses known parameters from the population, like the mean of the population and the standard deviation of the population. However, most times you don’t know those parameters of the population! So, you have to estimate them from your sample. Remember from the chapters on descriptive statistics and sampling, our sample mean is an unbiased estimate of the population mean. And, our sample standard deviation (the one where we divide by n-1) is an unbiased estimate of the population standard deviation. When Gosset developed the t-test, he recognized that he could use these estimates from his samples, to make the t-test. Here is the formula for the one sample t-test, we first use words, and then become more specific:\n\n\n\\(\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\n\\(\\text{t} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\n\\(\\text{t} = \\frac{\\text{Mean difference}}{\\text{standard error}}\\)\n\\(\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}}\\)\n\\(\\text{t} = \\frac{\\text{Sample Mean  - Population Mean}}{\\text{Sample Standard Error}}\\)\n\\(\\text{Estimated Standard Error} = \\text{Standard Error of Sample} = \\frac{s}{\\sqrt{N}}\\)\nWhere, s is the sample standard deviation.\nSome of you may have gone cross-eyed looking at all of this. Remember, we’ve seen it before when we divided our mean by the standard deviation in the first bit. The t-test is just a measure of a sample mean, divided by the standard error of the sample mean. That is it.\n\n\n\n\\(t\\) gives us a measure of confidence, just like our previous ratio for dividing the mean by a standard deviations. The only difference with \\(t\\), is that we divide by the standard error of mean (remember, this is also a standard deviation, it is the standard deviation of the sampling distribution of the mean)\n\n\n\n\n\n\nNote\n\n\n\nWhat does the t in t-test stand for? Apparently nothing. Gosset originally labelled it z. And, Fisher later called it t, perhaps because t comes after s, which is often used for the sample standard deviation.\n\n\n\\(t\\) is a property of the data that you collect. You compute it with a sample mean, and a sample standard error (there’s one more thing in the one-sample formula, the population mean, which we get to in a moment). This is why we call \\(t\\), a sample-statistic. It’s a statistic we compute from the sample.\nWhat kinds of numbers should we expect to find for these \\(ts\\)? How could we figure that out?\nLet’s start small and work through some examples. Imagine your sample mean is 5. You want to know if it came from a population that also has a mean of 5. In this case, what would \\(t\\) be? It would be zero: we first subtract the sample mean from the population mean, \\(5-5=0\\). Because the numerator is 0, \\(t\\) will be zero. So, \\(t\\) = 0, occurs, when there is no difference.\nLet’s say you take another sample, do you think the mean will be 5 every time, probably not. Let’s say the mean is 6. So, what can \\(t\\) be here? It will be a positive number, because 6-5= +1. But, will \\(t\\) be +1? That depends on the standard error of the sample. If the standard error of the sample is 1, then \\(t\\) could be 1, because 1/1 = 1.\nIf the sample standard error is smaller than 1, what happens to \\(t\\)? It get’s bigger right? For example, 1 divided by 0.5 = 2. If the sample standard error was 0.5, \\(t\\) would be 2. And, what could we do with this information? Well, it be like a measure of confidence. As \\(t\\) get’s bigger we could be more confident in the mean difference we are measuring.\nCan \\(t\\) be smaller than 1? Sure, it can. If the sample standard error is big, say like 2, then \\(t\\) will be smaller than one (in our case), e.g., 1/2 = .5. The direction of the difference between the sample mean and population mean, can also make the \\(t\\) become negative. What if our sample mean was 4. Well, then \\(t\\) will be negative, because the mean difference in the numerator will be negative, and the number in the bottom (denominator) will always be positive (remember why, it’s the standard error, computed from the sample standard deviation, which is always positive because of the squaring that we did.).\nSo, that is some intuitions about what the kinds of values t can take. \\(t\\) can be positive or negative, and big or small.\nLet’s do one more thing to build our intuitions about what \\(t\\) can look like. How about we sample some numbers and then measure the sample mean and the standard error of the mean, and then plot those two things against each each. This will show us how a sample mean typically varies with respect to the standard error of the mean.\nIn Figure 1, I pulled 1,000 samples of \\(N = 10\\) from a normal distribution (mean = 0, sd = 1). Each time I measured the mean and standard error of the sample. That gave two descriptive statistics for each sample, letting us plot each sample as dot in a scatter plot.\n\nsample_mean &lt;- length(1000)\nsample_se &lt;- length(1000)\n\nfor (i in 1:1000) {\n  s &lt;- rnorm(10, 0, 1)\n  sample_mean[i] &lt;- mean(s)\n  sample_se[i] &lt;- sd(s) / sqrt(length(s))\n}\n\nplot(sample_mean, sample_se)\n\n\n\n\n\n\n\nFigure 1: A scatter plot with sample mean on the x-axis, and standard error of the mean on the y-axis\n\n\n\n\n\nWhat we get is a cloud of dots. You might notice the cloud has a circular quality. There’s more dots in the middle, and fewer dots as they radiate out from the middle. The dot cloud shows us the general range of the sample mean, for example most of the dots are in between -1 and 1. Similarly, the range for the sample standard error is roughly between .2 and .5. Remember, each dot represents one sample.\nWe can look at the same data a different way. For example, rather than using a scatter plot, we can divide the mean for each dot by the standard error for each dot. Figure 2 shows the result in a histogram.\n\nhist(sample_mean/sample_se, breaks=30)\n\n\n\n\n\n\n\nFigure 2: A histogram of the sample means divided by the sample standard errors, this is a t-distribution.\n\n\n\n\n\nInteresting, we can see the histogram is shaped like a normal curve. It is centered on 0, which is the most common value. As values become more extreme, they become less common. If you remember, our formula for \\(t\\), was the mean divided by the standard error of the mean. That’s what we did here. This histogram is showing you a \\(t\\)-distribution.\n\n\n\nLet’s briefly calculate a t-value from a small sample. Let’s say we had 10 students do a true/false quiz with 5 questions on it. There’s a 50% chance of getting each answer correct.\nEvery student completes the 5 questions, we grade them, and then we find their performance (mean percent correct). What we want to know is whether the students were guessing. If they were all guessing, then the sample mean should be about 50%, it shouldn’t be different from chance, which is 50%. Let’s look at Table 1.\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\nTable 1: Calculating the t-value for a one-sample test.\n\n\n\n\n\n\nstudents\nscores\nmean\nDifference_from_Mean\nSquared_Deviations\n\n\n\n\n1\n50\n61\n-11\n121\n\n\n2\n70\n61\n9\n81\n\n\n3\n60\n61\n-1\n1\n\n\n4\n40\n61\n-21\n441\n\n\n5\n80\n61\n19\n361\n\n\n6\n30\n61\n-31\n961\n\n\n7\n90\n61\n29\n841\n\n\n8\n60\n61\n-1\n1\n\n\n9\n70\n61\n9\n81\n\n\n10\n60\n61\n-1\n1\n\n\nSums\n610\n610\n0\n2890\n\n\nMeans\n61\n61\n0\n289\n\n\n\n\n\nsd\n17.92\n\n\n\n\n\nSEM\n5.67\n\n\n\n\n\nt\n1.94003527336861\n\n\n\n\n\n\n\n\nYou can see the scores column has all of the test scores for each of the 10 students. We did the things we need to do to compute the standard deviation.\nRemember the sample standard deviation is the square root of the sample variance, or:\n\\(\\text{sample standard deviation} = \\sqrt{\\frac{\\sum_{i}^{n}({x_{i}-\\bar{x})^2}}{N-1}}\\)\n\\(\\text{sd} = \\sqrt{\\frac{2890}{10-1}} = 17.92\\)\nThe standard error of the mean, is the standard deviation divided by the square root of N\n\\(\\text{SEM} = \\frac{s}{\\sqrt{N}} = \\frac{17.92}{10} = 5.67\\)\n\\(t\\) is the difference between our sample mean (61), and our population mean (50, assuming chance), divided by the standard error of the mean.\n\\(\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}} = \\frac{\\bar{X}-u}{SEM} = \\frac{61-50}{5.67} = 1.94\\)\nAnd, that is you how calculate \\(t\\), by hand. It’s a pain. I was annoyed doing it this way. In the lab, you learn how to calculate \\(t\\) using software, so it will just spit out \\(t\\). For example in R, all you have to do is this:\n\nt.test(scores, mu=50)\n\n\n    One Sample t-test\n\ndata:  scores\nt = 1.9412, df = 9, p-value = 0.08415\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 48.18111 73.81889\nsample estimates:\nmean of x \n       61 \n\n\n\n\n\nIf \\(t\\) is just a number that we can compute from our sample (it is), what can we do with it? How can we use \\(t\\) for statistical inference?\nRemember back to the chapter on sampling and distributions, that’s where we discussed the sampling distribution of the sample mean. Remember, we made a lot of samples, then computed the mean for each sample, then we plotted a histogram of the sample means. Later, in that same section, we mentioned that we could generate sampling distributions for any statistic. For each sample, we could compute the mean, the standard deviation, the standard error, and now even \\(t\\), if we wanted to. We could generate 10,000 samples, and draw four histograms, one for each sampling distribution for each statistic.\nThis is exactly what I did, and the results are shown in the four panels of Figure 3 below. I used a sample size of 20, and drew random observations for each sample from a normal distribution, with mean = 0, and standard deviation = 1. Let’s look at the sampling distributions for each of the statistics. \\(t\\) was computed assuming with the population mean assumed to be 0.\n\nall_df &lt;- data.frame()\nfor (i in 1:10000) {\n  sample &lt;- rnorm(20, 0, 1)\n  sample_mean &lt;- mean(sample)\n  sample_sd &lt;- sd(sample)\n  sample_se &lt;- sd(sample) / sqrt(length(sample))\n  sample_t &lt;- as.numeric(t.test(sample, mu = 0)$statistic)\n  t_df &lt;- data.frame(i, sample_mean, sample_sd, sample_se, sample_t)\n  all_df &lt;- rbind(all_df, t_df)\n}\n\nlibrary(ggpubr)\na &lt;- ggplot(all_df, aes(x = sample_mean)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nb &lt;- ggplot(all_df, aes(x = sample_sd)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nc &lt;- ggplot(all_df, aes(x = sample_se)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nd &lt;- ggplot(all_df, aes(x = sample_t)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\n\nggarrange(a, b, c, d,\n          ncol = 2, nrow = 2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 3: Sampling distributions for the mean, standard deviation, standard error of the mean, and \\(t\\).\n\n\n\n\n\nWe see four sampling distributions. This is how statistical summaries of these summaries behave. We have used the word chance windows before. These are four chance windows, measuring different aspects of the sample. In this case, all of the samples came from the same normal distribution. Because of sampling error, each sample is not identical. The means are not identical, the standard deviations are not identical, sample standard error of the means are not identical, and the \\(t\\)s of the samples are not identical. They all have some variation, as shown by the histograms. This is how samples of size 20 behave.\nWe can see straight away, that in this case, we are unlikely to get a sample mean of 2. That’s way outside the window. The range for the sampling distribution of the mean is around -.5 to +.5, and is centered on 0 (the population mean, would you believe!).\nWe are unlikely to get sample standard deviations of between .6 and 1.5, that is a different range, specific to the sample standard deviation.\nSame thing with the sample standard error of the mean, the range here is even smaller, mostly between .1, and .3. You would rarely find a sample with a standard error of the mean greater than .3. Virtually never would you find one of say 1 (for this situation).\nNow, look at \\(t\\). It’s range is basically between -3 and +3 here. 3s barely happen at all. You pretty much never see a 5 or -5 in this situation.\nAll of these sampling windows are chance windows, and they can all be used in the same way as we have used similar sampling distributions before (e.g., Crump Test, and Randomization Test) for statistical inference. For all of them we would follow the same process:\n\nGenerate these distributions\nLook at your sample statistics for the data you have (mean, SD, SEM, and \\(t\\))\nFind the likelihood of obtaining that value or greater\nObtain that probability\nSee if you think your sample statistics were probable or improbable.\n\nWe’ll formalize this in a second. I just want you to know that what you will be doing is something that you have already done before. For example, in the Crump test and the Randomization test we focused on the distribution of mean differences. We could do that again here, but instead, we will focus on the distribution of \\(t\\) values. We then apply the same kinds of decision rules to the \\(t\\) distribution, as we did for the other distributions. Below you will see a graph you have already seen, except this time it is a distribution of \\(t\\)s, not mean differences:\nRemember, if we obtained a single \\(t\\) from one sample we collected, we could consult the chance window in Figure 4 below to find out whether the \\(t\\) we obtained from the sample was likely or unlikely to occur by chance.\n\nsample_t &lt;- all_df$sample_t\n\nggplot(all_df, aes(x = sample_t)) +\n  annotate(\n    \"rect\",\n    xmin = min(sample_t),\n    xmax = max(sample_t),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"red\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = min(sample_t),\n    xmax = -1.94,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = 1.94,\n    xmax = max(sample_t),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  geom_rect(aes(\n    xmin = -Inf,\n    xmax = min(sample_t),\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_rect(aes(\n    xmin = max(sample_t),\n    xmax = Inf,\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_histogram(bins = 50, color = \"white\") +\n  theme_classic() +\n  geom_vline(xintercept = min(sample_t)) +\n  geom_vline(xintercept = max(sample_t)) +\n  geom_vline(xintercept = -1.94) +\n  geom_vline(xintercept = 1.94) +\n  xlim(-8, 8) +\n  geom_label(data = data.frame(x = 0, y = 250, label = \"CHANCE\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -7, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 7, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  #  geom_label(data = data.frame(x = min(sample_t), y = 600,\n  #                              label = paste0(\"min \\n\",round(min(sample_t)))),\n  #                             aes(x = x, y = y, label = label))+\n  #geom_label(data = data.frame(x = max(sample_t), y = 600,\n  #                            label = paste0(\"max \\n\",round(max(sample_t)))),\n  #                           aes(x = x, y = y, label = label))+\n  geom_label(data = data.frame(x = -4, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 4, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  xlab(\"mean sample_t\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\nFigure 4: Applying decision criteria to the \\(t\\)-distribution. Histogram of \\(t\\)s from samples (n=20) drawn from the same normal distribution (u=0, sd=1)\n\n\n\n\n\n\n\n\nFrom our early example involving the TRUE/FALSE quizzes, we are now ready to make some kind of decision about what happened there. We found a mean difference of 11. We found a \\(t\\) = 1.9411765. The probability of this \\(t\\) or larger occurring is \\(p\\) = 0.0841503. We were testing the idea that our sample mean of 61 could have come from a normal distribution with mean = 50. The \\(t\\) test tells us that the \\(t\\) for our sample, or a larger one, would happen with p = 0.0841503. In other words, chance can do it a kind of small amount of time, but not often. In English, this means that all of the students could have been guessing, but it wasn’t that likely that were just guessing.\nThe next \\(t\\)-test is called a paired samples t-test. And, spoiler alert, we will find out that a paired samples t-test is actually a one-sample t-test in disguise (WHAT!), yes it is. If the one-sample \\(t\\)-test didn’t make sense to you, read the next section.",
    "crumbs": [
      "Week01",
      "06-ttests"
    ]
  },
  {
    "objectID": "week01/06-ttests.html#paired-samples-t-test",
    "href": "week01/06-ttests.html#paired-samples-t-test",
    "title": "06-ttests",
    "section": "",
    "text": "For me (Crump), many analyses often boil down to a paired samples t-test. It just happens that many things I do reduce down to a test like this.\nI am a cognitive psychologist, I conduct research about how people do things like remember, pay attention, and learn skills. There are lots of Psychologists like me, who do very similar things.\nWe all often conduct the same kinds of experiments. They go like this, and they are called repeated measures designs. They are called repeated measures designs, because we measure how one person does something more than once, we repeat the measure.\nSo, I might measure somebody doing something in condition A, and measure the same person doing something in Condition B, and then I see that same person does different things in the two conditions. I repeatedly measure the same person in both conditions. I am interested in whether the experimental manipulation changes something about how people perform the task in question.\n\n\nWe will introduce the paired-samples t-test with an example using real data, from a real study. @mehr20165 were interested in whether singing songs to infants helps infants become more sensitive to social cues. For example, infants might need to learn to direct their attention toward people as a part of learning how to interact socially with people. Perhaps singing songs to infants aids this process of directing attention. When an infant hears a familiar song, they might start to pay more attention to the person singing that song, even after they are done singing the song. The person who sang the song might become more socially important to the infant. You will learn more about this study in the lab for this week. This example, prepares you for the lab activities. Here is a brief summary of what they did.\nFirst, parents were trained to sing a song to their infants. After many days of singing this song to the infants, a parent came into the lab with their infant. In the first session, parents sat with their infants on their knees, so the infant could watch two video presentations. There were two videos. Each video involved two unfamiliar new people the infant had never seen before. Each new person in the video (the singers) sang one song to the infant. One singer sang the “familiar” song the infant had learned from their parents. The other singer sang an “unfamiliar” song the infant had not hear before.\nThere were two really important measurement phases: the baseline phase, and the test phase.\nThe baseline phase occurred before the infants saw and heard each singer sing a song. During the baseline phase, the infants watched a video of both singers at the same time. The researchers recorded the proportion of time that the infant looked at each singer. The baseline phase was conducted to determine whether infants had a preference to look at either person (who would later sing them a song).\nThe test phase occurred after infants saw and heard each song, sung by each singer. During the test phase, each infant had an opportunity to watch silent videos of both singers. The researchers measured the proportion of time the infants spent looking at each person. The question of interest, was whether the infants would spend a greater proportion of time looking at the singer who sang the familiar song, compared to the singer who sang the unfamiliar song.\nThere is more than one way to describe the design of this study. We will describe it like this. It was a repeated measures design, with one independent (manipulation) variable called Viewing phase: Baseline versus Test. There was one dependent variable (the measurement), which was proportion looking time (to singer who sung familiar song). This was a repeated measures design because the researchers measured proportion looking time twice (they repeated the measure), once during baseline (before infants heard each singer sing a song), and again during test (after infants head each singer sing a song).\nThe important question was whether infants would change their looking time, and look more at the singer who sang the familiar song during the test phase, than they did during the baseline phase. This is a question about a change within individual infants. In general, the possible outcomes for the study are:\n\nNo change: The difference between looking time toward the singer of the familiar song during baseline and test is zero, no difference.\nPositive change: Infants will look longer toward the singer of the familiar song during the test phase (after they saw and heard the singers), compared to the baseline phase (before they saw and heard the singers). This is a positive difference if we use the formula: Test Phase Looking time - Baseline phase looking time (to familiar song singer).\nNegative change: Infants will look longer toward the singer of the unfamiliar song during the test phase (after they saw and heard the singers), compared to the baseline phase (before they saw and heard the singers). This is a negative difference if we use the same formula: Test Phase Looking time - Baseline phase looking time (to familiar song singer).\n\n\n\n\nLet’s take a look at the data for the first 5 infants in the study. This will help us better understand some properties of the data before we analyze it. We will see that the data is structured in a particular way that we can take advantage of with a paired samples t-test. Note, we look at the first 5 infants to show how the computations work. The results of the paired-samples t-test change when we use all of the data from the study.\nHere is a table of the data:\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\n\n\n\ninfant\nBaseline\nTest\n\n\n\n\n1\n0.44\n0.60\n\n\n2\n0.41\n0.68\n\n\n3\n0.75\n0.72\n\n\n4\n0.44\n0.28\n\n\n5\n0.47\n0.50\n\n\n\n\n\nThe table shows proportion looking times toward the singer of the familiar song during the Baseline and Test phases. Notice there are five different infants, (1 to 5). Each infant is measured twice, once during the Baseline phase, and once during the Test phase. To repeat from before, this is a repeated-measures design, because the infants are measured repeatedly (twice in this case). Or, this kind of design is also called a paired-samples design. Why? because each participant comes with a pair of samples (two samples), one for each level of the design.\nGreat, so what are we really interested in here? We want to know if the mean looking time toward the singer of the familiar song for the Test phase is higher than the Baseline phase. We are comparing the two sample means against each other and looking for a difference. We already know that differences could be obtained by chance alone, simply because we took two sets of samples, and we know that samples can be different. So, we are interested in knowing whether chance was likely or unlikely to have produced any difference we might observe.\nIn other words, we are interested in looking at the difference scores between the baseline and test phase for each infant. The question here is, for each infant, did their proportion looking time to the singer of the familiar song, increase during the test phase as compared to the baseline phase.\n\n\n\nLet’s add the difference scores to the table of data so it is easier to see what we are talking about. The first step in creating difference scores is to decide how you will take the difference, there are two options:\n\nTest phase score - Baseline Phase Score\nBaseline phase score - Test Phase score\n\nLet’s use the first formula. Why? Because it will give us positive differences when the test phase score is higher than the baseline phase score. This makes a positive score meaningful with respect to the study design, we know (because we defined it to be this way), that positive scores will refer to longer proportion looking times (to singer of familiar song) during the test phase compared to the baseline phase.\n\npaired_sample_df &lt;- cbind(paired_sample_df, \n                          differences = (paired_sample_df$Test-\n                                           paired_sample_df$Baseline))\nknitr::kable(paired_sample_df)\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\n\n\n\n\n1\n0.44\n0.60\n0.16\n\n\n2\n0.41\n0.68\n0.27\n\n\n3\n0.75\n0.72\n-0.03\n\n\n4\n0.44\n0.28\n-0.16\n\n\n5\n0.47\n0.50\n0.03\n\n\n\n\n\nThere we have it, the difference scores. The first thing we can do here is look at the difference scores, and ask how many infants showed the effect of interest. Specifically, how many infants showed a positive difference score. We can see that three of five infants showed a positive difference (they looked more at the singer of the familiar song during the test than baseline phase), and two the infants showed the opposite effect (negative difference, they looked more at the singer of the familiar song during baseline than test).\n\n\n\nAs we have been discussing, the effect of interest in this study is the mean difference between the baseline and test phase proportion looking times. We can calculate the mean difference, by finding the mean of the difference scores. Let’s do that, in fact, for fun let’s calculate the mean of the baseline scores, the test scores, and the difference scores.\n\npaired_sample_df &lt;- paired_sample_df %&gt;%\n   rbind(c(\"Sums\",colSums(paired_sample_df[1:5,2:4]))) %&gt;%\n   rbind(c(\"Means\",colMeans(paired_sample_df[1:5,2:4])))\n  \nknitr::kable(paired_sample_df)\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\n\n\n\n\n1\n0.44\n0.6\n0.16\n\n\n2\n0.41\n0.68\n0.27\n\n\n3\n0.75\n0.72\n-0.03\n\n\n4\n0.44\n0.28\n-0.16\n\n\n5\n0.47\n0.5\n0.03\n\n\nSums\n2.51\n2.78\n0.27\n\n\nMeans\n0.502\n0.556\n0.054\n\n\n\n\n\nWe can see there was a positive mean difference of 0.054, between the test and baseline phases.\nCan we rush to judgment and conclude that infants are more socially attracted to individuals who have sung them a familiar song? I would hope not based on this very small sample. First, the difference in proportion looking isn’t very large, and of course we recognize that this difference could have been produced by chance.\nWe will more formally evaluate whether this difference could have been caused by chance with the paired-samples t-test. But, before we do that, let’s again calculate \\(t\\) and discuss what \\(t\\) tells us over and above what our measure of the mean of the difference scores tells us.\n\n\n\nOK, so how do we calculate \\(t\\) for a paired-samples \\(t\\)-test? Surprise, we use the one-sample t-test formula that you already learned about! Specifically, we use the one-sample \\(t\\)-test formula on the difference scores. We have one sample of difference scores (you can see they are in one column), so we can use the one-sample \\(t\\)-test on the difference scores. Specifically, we are interested in comparing whether the mean of our difference scores came from a distribution with mean difference = 0. This is a special distribution we refer to as the null distribution. It is the distribution no differences. Of course, this null distribution can produce differences due to to sampling error, but those differences are not caused by any experimental manipulation, they caused by the random sampling process.\nWe calculate \\(t\\) in a moment. Let’s now consider again why we want to calculate \\(t\\)? Why don’t we just stick with the mean difference we already have?\nRemember, the whole concept behind \\(t\\), is that it gives an indication of how confident we should be in our mean. Remember, \\(t\\) involves a measure of the mean in the numerator, divided by a measure of variation (standard error of the sample mean) in the denominator. The resulting \\(t\\) value is small when the mean difference is small, or when the variation is large. So small \\(t\\)-values tell us that we shouldn’t be that confident in the estimate of our mean difference. Large \\(t\\)-values occur when the mean difference is large and/or when the measure of variation is small. So, large \\(t\\)-values tell us that we can be more confident in the estimate of our mean difference. Let’s find \\(t\\) for the mean difference scores. We use the same formulas as we did last time:\n\n\n\n\n\n\n\n\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\ndiff_from_mean\nSquared_differences\n\n\n\n\n1\n0.44\n0.6\n0.16\n0.106\n0.011236\n\n\n2\n0.41\n0.68\n0.27\n0.216\n0.046656\n\n\n3\n0.75\n0.72\n-0.03\n-0.084\n0.00705600000000001\n\n\n4\n0.44\n0.28\n-0.16\n-0.214\n0.045796\n\n\n5\n0.47\n0.5\n0.03\n-0.024\n0.000575999999999999\n\n\nSums\n2.51\n2.78\n0.27\n0\n0.11132\n\n\nMeans\n0.502\n0.556\n0.054\n0\n0.022264\n\n\n\n\n\n\nsd\n0.167\n\n\n\n\n\n\nSEM\n0.075\n\n\n\n\n\n\nt\n0.72\n\n\n\n\n\nIf we did this test using R, we would obtain almost the same numbers (there is a little bit of rounding in the table).\n\nt.test(differences,mu=0)\n\n\n    One Sample t-test\n\ndata:  differences\nt = 0.72381, df = 4, p-value = 0.5092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.1531384  0.2611384\nsample estimates:\nmean of x \n    0.054 \n\n\nHere is a quick write up of our t-test results, t(4) = .72, p = .509.\nWhat does all of that tell us? There’s a few things we haven’t gotten into much yet. For example, the 4 represents degrees of freedom, which we discuss later. The important part, the \\(t\\) value should start to be a little bit more meaningful. We got a kind of small t-value didn’t we. It’s .72. What can we tell from this value? First, it is positive, so we know the mean difference is positive. The sign of the \\(t\\)-value is always the same as the sign of the mean difference (ours was +0.054). We can also see that the p-value was .509. We’ve seen p-values before. This tells us that our \\(t\\) value or larger, occurs about 50.9% of the time… Actually it means more than this. And, to understand it, we need to talk about the concept of two-tailed and one-tailed tests.\n\n\n\nRemember what it is we are doing here. We are evaluating whether our sample data could have come from a particular kind of distribution. The null distribution of no differences. This is the distribution of \\(t\\)-values that would occur for samples of size 5, with a mean difference of 0, and a standard error of the sample mean of .075 (this is the SEM that we calculated from our sample). We can see what this particular null-distribution looks like in Figure 5.\n\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  scale_x_continuous(breaks = (seq(-3, 3, .5))) +\n  geom_label(data = data.frame(x = -.7, y = .1, label = \"50% \\n (-)\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .7, y = .1, label = \"50% \\n (+)\"), aes(x = x, y = y, label = label)) +\n  geom_vline(xintercept = 0)+\n  theme_classic(base_size = 20)\n\n\n\n\n\n\n\nFigure 5: A distribution of \\(t\\)-values that can occur by chance alone, when there is no difference between the sample and a population\n\n\n\n\n\nThe \\(t\\)-distribution above shows us the kinds of values \\(t\\) will will take by chance alone, when we measure the mean differences for pairs of 5 samples (like our current). \\(t\\) is most likely to be zero, which is good, because we are looking at the distribution of no-differences, which should most often be 0! But, sometimes, due to sampling error, we can get \\(t\\)s that are bigger than 0, either in the positive or negative direction. Notice the distribution is symmetrical, a \\(t\\) from the null-distribution will be positive half of the time, and negative half of the time, that is what we would expect by chance.\nSo, what kind of information do we want know when we find a particular \\(t\\) value from our sample? We want to know how likely the \\(t\\) value like the one we found occurs just by chance. This is actually a subtly nuanced kind of question. For example, any particular \\(t\\) value doesn’t have a specific probability of occurring. When we talk about probabilities, we are talking about ranges of probabilities. Let’s consider some probabilities. We will use the letter \\(p\\), to talk about the probabilities of particular \\(t\\) values.\n\nWhat is the probability that \\(t\\) is zero or positive or negative? The answer is p=1, or 100%. We will always have a \\(t\\) value that is zero or non-zero…Actually, if we can’t compute the t-value, for example when the standard deviation is undefined, I guess then we would have a non-number. But, assuming we can calculate \\(t\\), then it will always be 0 or positive or negative.\nWhat is the probability of \\(t\\) = 0 or greater than 0? The answer is p=.5, or 50%. 50% of \\(t\\)-values are 0 or greater.\nWhat is the of \\(t\\) = 0 or smaller than 0? The answer is p=.5, or 50%. 50% of \\(t\\)-values are 0 or smaller.\n\nWe can answer all of those questions just by looking at our t-distribution, and dividing it into two equal regions, the left side (containing 50% of the \\(t\\) values), and the right side containing 50% of the \\(t\\)-values).\nWhat if we wanted to take a more fine-grained approach, let’s say we were interested in regions of 10%. What kinds of \\(t\\)s occur 10% of the time. We would apply lines like the following. Notice, the likelihood of bigger numbers (positive or negative) gets smaller, so we have to increase the width of the bars for each of the intervals between the bars to contain 10% of the \\(t\\)-values, it looks like Figure 6.\n\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = t_ps) +\n  theme_classic(base_size = 15) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  scale_x_continuous(breaks = round(t_ps, digits = 1)) +\n  geom_label(data = data.frame(x = -2.5, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -1.3, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 1.3, y = .2, label = \"10%\"), aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\nFigure 6: Splitting the t distribution up into regions each containing 10% of the \\(t\\)-values. The width between the bars narrows as they approach the center of the distribution, where there are more \\(t\\)-values.\n\n\n\n\n\nConsider the probabilities (\\(p\\)) of \\(t\\) for the different ranges.\n\n\\(t\\) &lt;= -1.5 (\\(t\\) is less than or equal to -1.5), \\(p\\) = 10%\n-1.5 &gt;= \\(t\\) &lt;= -0.9 (\\(t\\) is equal to or between -1.5 and -.9), \\(p\\) = 10%\n-.9 &gt;= \\(t\\) &lt;= -0.6 (\\(t\\) is equal to or between -.9 and -.6), \\(p\\) = 10%\n\\(t\\) &gt;= 1.5 (\\(t\\) is greater than or equal to 1.5), \\(p\\) = 10%\n\nNotice, that the \\(p\\)s are always 10%. \\(t\\)s occur in these ranges with 10% probability.\n\n\n\nYou might be wondering where I am getting some of these values from. For example, how do I know that 10% of \\(t\\) values (for this null distribution) have a value of approximately 1.5 or greater than 1.5? The answer is I used R to tell me.\nIn most statistics textbooks the answer would be: there is a table at the back of the book where you can look these things up…This textbook has no such table. We could make one for you. And, we might do that. But, we didn’t do that yet…\nSo, where do these values come from, how can you figure out what they are? The complicated answer is that we are not going to explain the math behind finding these values because, 1) the authors (some of us) admittedly don’t know the math well enough to explain it, and 2) it would sidetrack us to much, 3) you will learn how to get these numbers in the lab with software, 4) you will learn how to get these numbers in lab without the math, just by doing a simulation, and 5) you can do it in R, or excel, or you can use an online calculator.\nThis is all to say that you can find the \\(t\\)s and their associated \\(p\\)s using software. But, the software won’t tell you what these values mean. That’s we are doing here. You will also see that software wants to know a few more things from you, such as the degrees of freedom for the test, and whether the test is one-tailed or two tailed. We haven’t explained any of these things yet. That’s what we are going to do now. Note, we explain degrees of freedom last. First, we start with a one-tailed test.\n\n\n\nA one-tailed test is sometimes also called a directional test. It is called a directional test, because a researcher might have a hypothesis in mind suggesting that the difference they observe in their means is going to have a particular direction, either a positive difference, or a negative difference.\nTypically, a researcher would set an alpha criterion. The alpha criterion describes a line in the sand for the researcher. Often, the alpha criterion is set at \\(p = .05\\). What does this mean? Figure 7 shows the \\(t\\)-distribution and the alpha criterion.\n\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = qt(.95, 4, lower.tail = TRUE)) +\n  ggtitle(\"Critical t for one-tailed test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = qt(.95, 4, lower.tail = TRUE),\n    xmax = 3,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.95, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\nFigure 7: The critical value of t for an alpha criterion of 0.05. 5% of all ts are at this value or larger\n\n\n\n\n\nThe figure shows that \\(t\\) values of +2.13 or greater occur 5% of the time. Because the t-distribution is symmetrical, we also know that \\(t\\) values of -2.13 or smaller also occur 5% of the time. Both of these properties are true under the null distribution of no differences. This means, that when there really are no differences, a researcher can expect to find \\(t\\) values of 2.13 or larger 5% of the time.\nLet’s review and connect some of the terms:\n\nalpha criterion: the criterion set by the researcher to make decisions about whether they believe chance did or did not cause the difference. The alpha criterion here is set to \\(p = .05\\).\nCritical \\(t\\). The critical \\(t\\) is the \\(t\\)-value associated with the alpha-criterion. In this case for a one-tailed test, it is the \\(t\\) value where 5% of all \\(t\\)s are this number or greater. In our example, the critical \\(t\\) is 2.13. 5% of all \\(t\\) values (with degrees of freedom = 4) are +2.13, or greater than +2.13.\nObserved \\(t\\). The observed \\(t\\) is the one that you calculated from your sample. In our example about the infants, the observed \\(t\\) was \\(t\\) (4) = 0.72.\np-value. The \\(p\\)-value is the probability of obtaining the observed \\(t\\) value or larger. Now, you could look back at our previous example, and find that the \\(p\\)-value for \\(t\\) (4) = .72, was \\(p = .509\\) . HOWEVER, this p-value was not calculated for a one-directional test…(we talk about what .509 means in the next section).\n\nFigure 8 shows what the \\(p\\)-value for \\(t\\) (4) = .72 using a one-directional test would would look like:\n\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = .72) +\n  geom_vline(xintercept = qt(.95, 4, lower.tail = TRUE)) +\n  ggtitle(\"t value and p-range for one-directional test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = .72,\n    xmax = 3,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = qt(.95, 4, lower.tail = TRUE),\n    xmax = 3,\n    ymin = 0,\n    ymax = .25,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.95, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .72, y = .1,\n                               label = \"Observed t\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .72, y = .05,\n                               label = \".72, p=\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = 1.5,\n    y = .05,\n    label = round(pt(.72, 4, lower.tail = FALSE), digits =\n                    3)\n  ),\n  aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\nFigure 8: A case where the observed value of t is much less than the critical value for a one-directional t-test.\n\n\n\n\n\nLet’s take this one step at a time. We have located the observed \\(t\\) of .72 on the graph. We shaded the right region all grey. What we see is that the grey region represents .256 or 25.6% of all \\(t\\) values. In other words, 25.6% of \\(t\\) values are .72 or larger than .72. You could expect, by chance alone, to a find a \\(t\\) value of .72 or larger, 25.6% of the time. That’s fairly often. We did find a \\(t\\) value of .72. Now that you know this kind of \\(t\\) value or larger occurs 25.6% of the time, would you be confident that the mean difference was not due to chance? Probably not, given that chance can produce this difference fairly often.\nFollowing the “standard” decision making procedure, we would claim that our \\(t\\) value was not statistically significant, because it was not large enough. If our observed value was larger than the critical \\(t\\) (larger than 2.13), defined by our alpha criterion, then we would claim that our \\(t\\) value was statistically significant. This would be equivalent to saying that we believe it is unlikely that the difference we observed was due to chance. In general, for any observed \\(t\\) value, the associated \\(p\\)-value tells you how likely a \\(t\\) of the observed size or larger would be observed. The \\(p\\)-value always refers to a range of \\(t\\)-values, never to a single \\(t\\)-value. Researchers use the alpha criterion of .05, as a matter of convenience and convention. There are other ways to interpret these values that do not rely on a strict (significant versus not) dichotomy.\n\n\n\nOK, so that was one-tailed tests… What are two tailed tests? The \\(p\\)-value that we originally calculated from our paired-samples \\(t\\)-test was for a 2-tailed test. Often, the default is that the \\(p\\)-value is for a two-tailed test.\nThe two-tailed test, is asking a more general question about whether a difference is likely to have been produced by chance. The question is: what is probability of any difference. It is also called a non-directional test, because here we don’t care about the direction or sign of the difference (positive or negative), we just care if there is any kind of difference.\nThe same basic things as before are involved. We define an alpha criterion (\\(\\alpha = 0.05\\)). And, we say that any observed \\(t\\) value that has a probability of \\(p\\) &lt;.05 (\\(p\\) is less than .05) will be called statistically significant, and ones that are more likely (\\(p\\) &gt;.05, \\(p\\) is greater than .05) will be called null-results, or not statistically significant. The only difference is how we draw the alpha range. Before it was on the right side of the \\(t\\) distribution (we were conducting a one-sided test remember, so we were only interested in one side).\nFigure 9 shows what the most extreme 5% of the \\(t\\)-values are when we ignore their sign (whether they are positive or negative).\n\nrange &lt;- seq(-4, 4, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = qt(.975, 4, lower.tail = TRUE)) +\n  geom_vline(xintercept = qt(.025, 4, lower.tail = TRUE)) +\n  ggtitle(\"Critical ts for two-tailed test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = qt(.975, 4, lower.tail = TRUE),\n    xmax = 4,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 3.5, y = .2, label = \"2.5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.975, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.975, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.975, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label)) +\n  annotate(\n    \"rect\",\n    xmin = -4,\n    xmax = qt(.025, 4, lower.tail = TRUE),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = -3.5, y = .2, label = \"2.5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.025, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.025, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.025, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\nFigure 9: Critical values for a two-tailed test. Each line represents the location where 2.5% of all \\(t\\)s are larger or smaller than critical value. The total for both tails is 5%\n\n\n\n\n\nHere is what we are seeing. A distribution of no differences (the null, which is what we are looking at), will produce \\(t\\)s that are 2.78 or greater 2.5% of the time, and \\(t\\)s that are -2.78 or smaller 2.5% of the time. 2.5% + 2.5% is a total of 5% of the time. We could also say that \\(t\\)s larger than +/- 2.78 occur 5% of the time.\nAs a result, the critical \\(t\\) value is (+/-) 2.78 for a two-tailed test. As you can see, the two-tailed test is blind to the direction or sign of the difference. Because of this, the critical \\(t\\) value is also higher for a two-tailed test, than for the one-tailed test that we did earlier. Hopefully, now you can see why it is called a two-tailed test. There are two tails of the distribution, one on the left and right, both shaded in green.\n\n\n\nNow that you know there are two kinds of tests, one-tailed, and two-tailed, which one should you use? There is some conventional wisdom on this, but also some debate. In the end, it is up to you to be able to justify your choice and why it is appropriate for you data. That is the real answer.\nThe conventional answer is that you use a one-tailed test when you have a theory or hypothesis that is making a directional prediction (the theory predicts that the difference will be positive, or negative). Similarly, use a two-tailed test when you are looking for any difference, and you don’t have a theory that makes a directional prediction (it just makes the prediction that there will be a difference, either positive or negative).\nAlso, people appear to choose one or two-tailed tests based on how risky they are as researchers. If you always ran one-tailed tests, your critical \\(t\\) values for your set alpha criterion would always be smaller than the critical \\(t\\)s for a two-tailed test. Over the long run, you would make more type I errors, because the criterion to detect an effect is a lower bar for one than two tailed tests.\n\nRemember type 1 errors occur when you reject the idea that chance could have caused your difference. You often never know when you make this error. It happens anytime that sampling error was the actual cause of the difference, but a researcher dismisses that possibility and concludes that their manipulation caused the difference.\n\nSimilarly, if you always ran two-tailed tests, even when you had a directional prediction, you would make fewer type I errors over the long run, because the \\(t\\) for a two-tailed test is higher than the \\(t\\) for a one-tailed test. It seems quite common for researchers to use a more conservative two-tailed test, even when they are making a directional prediction based on theory. In practice, researchers tend to adopt a standard for reporting that is common in their field. Whether or not the practice is justifiable can sometimes be an open question. The important task for any researcher, or student learning statistics, is to be able to justify their choice of test.\n\n\n\nBefore we finish up with paired-samples \\(t\\)-tests, we should talk about degrees of freedom. Our sense is that students don’t really understand degrees of freedom very well. If you are reading this textbook, you are probably still wondering what is degrees of freedom, seeing as we haven’t really talked about it all.\nFor the \\(t\\)-test, there is a formula for degrees of freedom. For the one-sample and paired sample \\(t\\)-tests, the formula is:\n\\(\\text{Degrees of Freedom} = \\text{df} = n-1\\). Where n is the number of samples in the test.\nIn our paired \\(t\\)-test example, there were 5 infants. Therefore, degrees of freedom = 5-1 = 4.\nOK, that’s a formula. Who cares about degrees of freedom, what does the number mean? And why do we report it when we report a \\(t\\)-test… you’ve probably noticed the number in parentheses e.g., \\(t\\)(4)=.72, the 4 is the \\(df\\), or degrees of freedom.\nDegrees of freedom is both a concept, and a correction. The concept is that if you estimate a property of the numbers, and you use this estimate, you will be forcing some constraints on your numbers.\nConsider the numbers: 1, 2, 3. The mean of these numbers is 2. Now, let’s say I told you that the mean of three numbers is 2. Then, how many of these three numbers have freedom? Funny question right. What we mean is, how many of the three numbers could be any number, or have the freedom to be any number.\nThe first two numbers could be any number. But, once those two numbers are set, the final number (the third number), MUST be a particular number that makes the mean 2. The first two numbers have freedom. The third number has no freedom.\nTo illustrate. Let’s freely pick two numbers: 51 and -3. I used my personal freedom to pick those two numbers. Now, if our three numbers are 51, -3, and x, and the mean of these three numbers is 2. There is only one solution, x has to be -42, otherwise the mean won’t be 2. This is one way to think about degrees of freedom. The degrees of freedom for these three numbers is n-1 = 3-1= 2, because 2 of the numbers can be free, but the last number has no freedom, it becomes fixed after the first two are decided.\nNow, statisticians often apply degrees of freedom to their calculations, especially when a second calculation relies on an estimated value. For example, when we calculate the standard deviation of a sample, we first calculate the mean of the sample right! By estimating the mean, we are fixing an aspect of our sample, and so, our sample now has n-1 degrees of freedom when we calculate the standard deviation (remember for the sample standard deviation, we divide by n-1…there’s that n-1 again.)\n\n\nThere are at least two ways to think the degrees of freedom for a \\(t\\)-test. For example, if you want to use math to compute aspects of the \\(t\\) distribution, then you need the degrees of freedom to plug in to the formula… If you want to see the formulas I’m talking about, scroll down on the \\(t\\)-test wikipedia page and look for the probability density or cumulative distribution functions…We think that is quite scary for most people, and one reason why degrees of freedom are not well-understood.\nIf we wanted to simulate the \\(t\\) distribution we could more easily see what influence degrees of freedom has on the shape of the distribution. Remember, \\(t\\) is a sample statistic, it is something we measure from the sample. So, we could simulate the process of measuring \\(t\\) from many different samples, then plot the histogram of \\(t\\) to show us the simulated \\(t\\) distribution.\n\nts &lt;- c(rt(10000, 4), rt(10000, 100))\ndfs &lt;- as.factor(rep(c(4, 100), each = 10000))\n\nt_df &lt;- data.frame(dfs, ts)\nt_df &lt;- t_df[abs(t_df$ts) &lt; 5, ]\n\nggplot(t_df, aes(x = ts, group = dfs, color = dfs)) +\n  geom_histogram() +\n  theme_classic() +\n  facet_wrap( ~ dfs) +\n  theme_classic(base_size=15)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 10: The width of the t distribution shrinks as sample size and degrees of freedom (from 4 to 100) increases.\n\n\n\n\n\nIn Figure 10 notice that the red distribution for \\(df = 4\\), is a little bit shorter, and a little bit wider than the bluey-green distribution for \\(df = 100\\). As degrees of freedom increase the \\(t\\) distribution gets taller (in the middle), and narrower in the range. It get’s more peaky. Can you guess the reason for this? Remember, we are estimating a sample statistic, and degrees of freedom is really just a number that refers to the number of subjects (well minus one). And, we already know that as we increase \\(n\\), our sample statistics become better estimates (less variance) of the distributional parameters they are estimating. So, \\(t\\) becomes a better estimate of it’s “true” value as sample size increase, resulting in a more narrow distribution of \\(t\\)s.\nThere is a slightly different \\(t\\) distribution for every degrees of freedom, and the critical regions associated with 5% of the extreme values are thus slightly different every time. This is why we report the degrees of freedom for each t-test, they define the distribution of \\(t\\) values for the sample-size in question. Why do we use n-1 and not n? Well, we calculate \\(t\\) using the sample standard deviation to estimate the standard error or the mean, that estimate uses n-1 in the denominator, so our \\(t\\) distribution is built assuming n-1. That’s enough for degrees of freedom…",
    "crumbs": [
      "Week01",
      "06-ttests"
    ]
  },
  {
    "objectID": "week01/06-ttests.html#the-paired-samples-t-test-strikes-back",
    "href": "week01/06-ttests.html#the-paired-samples-t-test-strikes-back",
    "title": "06-ttests",
    "section": "",
    "text": "You must be wondering if we will ever be finished talking about paired samples t-tests… why are we doing round 2, oh no! Don’t worry, we’re just going to 1) remind you about what we were doing with the infant study, and 2) do a paired samples t-test on the entire data set and discuss.\nRemember, we were wondering if the infants would look longer toward the singer who sang the familiar song during the test phase compared to the baseline phase. We showed you data from 5 infants, and walked through the computations for the \\(t\\)-test. As a reminder, it looked like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\ndiff_from_mean\nSquared_differences\n\n\n\n\n1\n0.44\n0.6\n0.16\n0.106\n0.011236\n\n\n2\n0.41\n0.68\n0.27\n0.216\n0.046656\n\n\n3\n0.75\n0.72\n-0.03\n-0.084\n0.00705600000000001\n\n\n4\n0.44\n0.28\n-0.16\n-0.214\n0.045796\n\n\n5\n0.47\n0.5\n0.03\n-0.024\n0.000575999999999999\n\n\nSums\n2.51\n2.78\n0.27\n0\n0.11132\n\n\nMeans\n0.502\n0.556\n0.054\n0\n0.022264\n\n\n\n\n\n\nsd\n0.167\n\n\n\n\n\n\nSEM\n0.075\n\n\n\n\n\n\nt\n0.72\n\n\n\n\n\n\n    One Sample t-test\n\ndata:  round(differences, digits = 2)\nt = 0.72381, df = 4, p-value = 0.5092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.1531384  0.2611384\nsample estimates:\nmean of x \n    0.054 \n\n\nLet’s write down the finding one more time: The mean difference was 0.054, \\(t\\)(4) = .72, \\(p\\) =.509. We can also now confirm, that the \\(p\\)-value was from a two-tailed test. So, what does this all really mean.\nWe can say that a \\(t\\) value with an absolute of .72 or larger occurs 50.9% of the time. More precisely, the distribution of no differences (the null), will produce a \\(t\\) value this large or larger 50.9% of the time. In other words, chance alone good have easily produced the \\(t\\) value from our sample, and the mean difference we observed or .054, could easily have been a result of chance.\nLet’s quickly put all of the data in the \\(t\\)-test, and re-run the test using all of the infant subjects.\n\npaired_sample_df &lt;-  data.frame(infant=1:32, \n                               Baseline = round(experiment_one$Baseline_Proportion_Gaze_to_Singer[1:32],digits=2), \n                               Test = round(experiment_one$Test_Proportion_Gaze_to_Singer[1:32], digits=2))\n\ndifferences &lt;-  paired_sample_df$Test-paired_sample_df$Baseline\nt.test(differences,mu=0)\n\n\n    One Sample t-test\n\ndata:  differences\nt = 2.4388, df = 31, p-value = 0.02066\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.01192088 0.13370412\nsample estimates:\nmean of x \n0.0728125 \n\n\nNow we get a very different answer. We would summarize the results saying the mean difference was .073, t(31) = 2.44, p = 0.020. How many total infants were their? Well the degrees of freedom was 31, so there must have been 32 infants in the study. Now we see a much smaller \\(p\\)-value. This was also a two-tailed test, so we that observing a \\(t\\) value of 2.4 or greater (absolute value) only occurs 2% of the time. In other words, the distribution of no differences will produce the observed t-value very rarely. So, it is unlikely that the observed mean difference of .073 was due to chance (it could have been due to chance, but that is very unlikely). As a result, we can be somewhat confident in concluding that something about seeing and hearing a unfamiliar person sing a familiar song, causes an infant to draw their attention toward the singer, and this potentially benefits social learning on the part of the infant.",
    "crumbs": [
      "Week01",
      "06-ttests"
    ]
  },
  {
    "objectID": "week01/06-ttests.html#independent-samples-t-test-the-return-of-the-t-test",
    "href": "week01/06-ttests.html#independent-samples-t-test-the-return-of-the-t-test",
    "title": "06-ttests",
    "section": "",
    "text": "If you’ve been following the Star Wars references, we are on last movie (of the original trilogy)… the independent t-test. This is were basically the same story plays out as before, only slightly different.\nRemember there are different \\(t\\)-tests for different kinds of research designs. When your design is a between-subjects design, you use an independent samples t-test. Between-subjects design involve different people or subjects in each experimental condition. If there are two conditions, and 10 people in each, then there are 20 total people. And, there are no paired scores, because every single person is measured once, not twice, no repeated measures. Because there are no repeated measures we can’t look at the difference scores between conditions one and two. The scores are not paired in any meaningful way, to it doesn’t make sense to subtract them. So what do we do?\nThe logic of the independent samples t-test is the very same as the other \\(t\\)-tests. We calculated the means for each group, then we find the difference. That goes into the numerator of the t formula. Then we get an estimate of the variation for the denominator. We divide the mean difference by the estimate of the variation, and we get \\(t\\). It’s the same as before.\nThe only wrinkle here is what goes into the denominator? How should we calculate the estimate of the variance? It would be nice if we could do something very straightforward like this, say for an experiment with two groups A and B:\n\\(t = \\frac{\\bar{A}-\\bar{B}}{(\\frac{SEM_A+SEM_B}{2})}\\)\nIn plain language, this is just:\n\nFind the mean difference for the top part\nCompute the SEM (standard error of the mean) for each group, and average them together to make a single estimate, pooling over both samples.\n\nThis would be nice, but unfortunately, it turns out that finding the average of two standard errors of the mean is not the best way to do it. This would create a biased estimator of the variation for the hypothesized distribution of no differences. We won’t go into the math here, but instead of the above formula, we an use a different one that gives as an unbiased estimate of the pooled standard error of the sample mean. Our new and improved \\(t\\) formula would look like this:\n\\(t = \\frac{\\bar{X_A}-\\bar{X_B}}{s_p * \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_B}}}\\)\nand, \\(s_p\\), which is the pooled sample standard deviation is defined as, note the $s$es in the formula are variances:\n\\(s_p = \\sqrt{\\frac{(n_A-1)s_A^2 + (n_B-1)s^2_B}{n_A +n_B -2}}\\)\nBelieve you me, that is so much more formula than I wanted to type out. Shall we do one independent \\(t\\)-test example by hand, just to see the computations? Let’s do it…but in a slightly different way than you expect. I show the steps using R. I made some fake scores for groups A and B. Then, I followed all of the steps from the formula, but made R do each of the calculations. This shows you the needed steps by following the code. At the end, I print the \\(t\\)-test values I computed “by hand”, and then the \\(t\\)-test value that the R software outputs using the \\(t\\)-test function. You should be able to get the same values for \\(t\\), if you were brave enough to compute \\(t\\) by hand.\n\n## By \"hand\" using R r code\na &lt;- c(1,2,3,4,5)\nb &lt;- c(3,5,4,7,9)\n\nmean_difference &lt;- mean(a)-mean(b) # compute mean difference\n\nvariance_a &lt;- var(a) # compute variance for A\nvariance_b &lt;- var(b) # compute variance for B\n\n# Compute top part and bottom part of sp formula\n\nsp_numerator &lt;- (4*variance_a + 4* variance_b) \nsp_denominator &lt;- 5+5-2\nsp &lt;- sqrt(sp_numerator/sp_denominator) # compute sp\n\n\n# compute t following formulat\n\nt &lt;- mean_difference / ( sp * sqrt( (1/5) +(1/5) ) )\n\nt # print results\n\n[1] -2.017991\n\n# using the R function t.test\nt.test(a,b, paired=FALSE, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  a and b\nt = -2.018, df = 8, p-value = 0.0783\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -5.5710785  0.3710785\nsample estimates:\nmean of x mean of y \n      3.0       5.6",
    "crumbs": [
      "Week01",
      "06-ttests"
    ]
  },
  {
    "objectID": "week01/06-ttests.html#simulating-data-for-t-tests",
    "href": "week01/06-ttests.html#simulating-data-for-t-tests",
    "title": "06-ttests",
    "section": "",
    "text": "An “advanced” topic for \\(t\\)-tests is the idea of using R to conduct simulations for \\(t\\)-tests.\nIf you recall, \\(t\\) is a property of a sample. We calculate \\(t\\) from our sample. The \\(t\\) distribution is the hypothetical behavior of our sample. That is, if we had taken thousands upon thousands of samples, and calculated \\(t\\) for each one, and then looked at the distribution of those \\(t\\)’s, we would have the sampling distribution of \\(t\\)!\nIt can be very useful to get in the habit of using R to simulate data under certain conditions, to see how your sample data, and things like \\(t\\) behave. Why is this useful? It mainly prepares you with some intuitions about how sampling error (random chance) can influence your results, given specific parameters of your design, such as sample-size, the size of the mean difference you expect to find in your data, and the amount of variation you might find. These methods can be used formally to conduct power-analyses. Or more informally for data sense.\n\n\nHere are the steps you might follow to simulate data for a one sample \\(t\\)-test.\n\nMake some assumptions about what your sample (that you might be planning to collect) might look like. For example, you might be planning to collect 30 subjects worth of data. The scores of those data points might come from a normal distribution (mean = 50, sd = 10).\nsample simulated numbers from the distribution, then conduct a \\(t\\)-test on the simulated numbers. Save the statistics you want (such as \\(t\\)s and \\(p\\)s), and then see how things behave.\n\nLet’s do this a couple different times. First, let’s simulate samples with N = 30, taken from a normal (mean= 50, sd = 25). We’ll do a simulation with 1000 simulations. For each simulation, we will compare the sample mean with a population mean of 50. There should be no difference on average here. Figure 11 is the null distribution that we are simulating.\n\n# steps to create fake data from a distribution\n# and conduct t-tests on the simulated data\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor (i in 1:1000) {\n  my_sample &lt;- rnorm(n = 30, mean = 50, sd = 25)\n  t_test &lt;- t.test (my_sample, mu = 50)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n#plot histograms of t and p values for 1000 simulations\nhist(save_ts)\n\n\n\n\n\n\n\nFigure 11: The distribution of \\(t\\)-values under the null. These are the \\(t\\) values that are produced by chance alone.\n\n\n\n\n\n\nhist(save_ps)\n\n\n\n\n\n\n\nFigure 12: The distribution of \\(p\\)-values that are observed is flat under the null.\n\n\n\n\n\nNeat. We see both a \\(t\\) distribution, that looks like \\(t\\) distribution as it should. And we see the \\(p\\) distribution. This shows us how often we get \\(t\\) values of particular sizes. You may find it interesting that the \\(p\\)-distribution is flat under the null, which we are simulating here. This means that you have the same chances of a getting a \\(t\\) with a p-value between 0 and 0.05, as you would for getting a \\(t\\) with a p-value between .90 and .95. Those ranges are both ranges of 5%, so there are an equal amount of \\(t\\) values in them by definition.\nHere’s another way to do the same simulation in R, using the replicate function, instead a for loop:\n\nsimulated_ts &lt;- replicate(1000,\n                          t.test(rnorm(30, 50, 25), mu = 50)$statistic)\nhist(simulated_ts)\n\n\n\n\n\n\n\nFigure 13: Simulating \\(t\\)s in R.\n\n\n\n\n\n\nsimulated_ps &lt;- replicate(1000,\n                          t.test(rnorm(30, 50, 25), mu = 50)$p.value)\nhist(simulated_ps)\n\n\n\n\n\n\n\nFigure 14: Simulating \\(p\\)s in R.\n\n\n\n\n\n\n\n\nThe code below is set up to sample 10 scores for condition A and B from the same normal distribution. The simulation is conducted 1000 times, and the \\(t\\)s and \\(p\\)s are saved and plotted for each.\n\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(10,10,5)\n  condition_B &lt;- rnorm(10,10,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\nhist(save_ts)\n\n\n\n\n\n\n\nFigure 15: 1000 simulated ts from the null distribution\n\n\n\n\n\n\nhist(save_ps)\n\n\n\n\n\n\n\nFigure 16: 1000 simulated ps from the null distribution\n\n\n\n\n\nAccording to the simulation. When there are no differences between the conditions, and the samples are being pulled from the very same distribution, you get these two distributions for \\(t\\) and \\(p\\). These again show how the null distribution of no differences behaves.\nFor any of these simulations, if you rejected the null-hypothesis (that your difference was only due to chance), you would be making a type I error. If you set your alpha criteria to \\(\\alpha = .05\\), we can ask how many type I errors were made in these 1000 simulations. The answer is:\n\nlength(save_ps[save_ps&lt;.05])\n\n[1] 52\n\nlength(save_ps[save_ps&lt;.05])/1000\n\n[1] 0.052\n\n\nWe happened to make 52. The expectation over the long run is 5% type I error rates (if your alpha is .05).\nWhat happens if there actually is a difference in the simulated data, let’s set one condition to have a larger mean than the other:\n\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(10,10,5)\n  condition_B &lt;- rnorm(10,13,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\nhist(save_ts)\n\n\n\n\n\n\n\nFigure 17: 1000 ts when there is a true difference\n\n\n\n\n\n\nhist(save_ps)\n\n\n\n\n\n\n\nFigure 18: 1000 ps when there is a true difference\n\n\n\n\n\nNow you can see that the \\(p\\)-value distribution is skewed to the left. This is because when there is a true effect, you will get p-values that are less than .05 more often. Or, rather, you get larger \\(t\\) values than you normally would if there were no differences.\nIn this case, we wouldn’t be making a type I error if we rejected the null when p was smaller than .05. How many times would we do that out of our 1000 experiments?\n\nlength(save_ps[save_ps&lt;.05])\n\n[1] 250\n\nlength(save_ps[save_ps&lt;.05])/1000\n\n[1] 0.25\n\n\nWe happened to get 250 simulations where p was less than .05, that’s only 0.25 experiments. If you were the researcher, would you want to run an experiment that would be successful only 0.25 of the time? I wouldn’t. I would run a better experiment.\nHow would you run a better simulated experiment? Well, you could increase \\(n\\), the number of subjects in the experiment. Let’s increase \\(n\\) from 10 to 100, and see what happens to the number of “significant” simulated experiments.\n\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(100,10,5)\n  condition_B &lt;- rnorm(100,13,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\nhist(save_ts)\n\n\n\n\n\n\n\nFigure 19: 1000 ts for n =100, when there is a true effect\n\n\n\n\n\n\nhist(save_ps)\n\n\nlength(save_ps[save_ps&lt;.05])\n\n[1] 991\n\nlength(save_ps[save_ps&lt;.05])/1000\n\n[1] 0.991\n\n\n\n\n\n\n\n\nFigure 20: 1000 ps for n =100, when there is a true effect\n\n\n\n\n\nCool, now almost all of the experiments show a \\(p\\)-value of less than .05 (using a two-tailed test, that’s the default in R). See, you could use this simulation process to determine how many subjects you need to reliably find your effect.\n\n\n\nJust change the t.test function like so… this is for the null, assuming no difference between groups.\n\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  group_A &lt;- rnorm(10,10,5)\n  group_B &lt;- rnorm(10,10,5)\n  t_test &lt;- t.test(group_A, group_B, paired=FALSE, var.equal=TRUE)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\nhist(save_ts)\n\n\n\n\n\n\n\nFigure 21: 1000 ts for n =100, when there is a true effect\n\n\n\n\n\n\nhist(save_ps)\n\n\nlength(save_ps[save_ps&lt;.05])\n\n[1] 50\n\nlength(save_ps[save_ps&lt;.05])/1000\n\n[1] 0.05\n\n\n\n\n\n\n\n\nFigure 22: 1000 ps for n =100, when there is a true effect",
    "crumbs": [
      "Week01",
      "06-ttests"
    ]
  },
  {
    "objectID": "week01/chapter.html",
    "href": "week01/chapter.html",
    "title": "T-Tests",
    "section": "",
    "text": "::: Content below origially written by Matthew Crump under a CC-BY-NC-SA Licence. Original Content Chapter 6 of https://www.crumplab.com/statistics/06-ttests.html :::\nBack in the day, William Sealy Gosset got a job working for Guinness Breweries. They make the famous Irish stout called Guinness. What happens next went something like this (total fabrication, but mostly on point).\nGuinness wanted all of their beers to be the best beers. No mistakes, no bad beers. They wanted to improve their quality control so that when Guinness was poured anywhere in the world, it would always comes out fantastic: 5 stars out of 5 every time, the best.\nGuinness had some beer tasters, who were super-experts. Every time they tasted a Guinness from the factory that wasn’t 5 out of 5, they knew right away.\nBut, Guinness had a big problem. They would make a keg of beer, and they would want to know if every single pint that would come out would be a 5 out of 5. So, the beer tasters drank pint after pint out of the keg, until it was gone. Some kegs were all 5 out of 5s. Some weren’t, Guinness needed to fix that. But, the biggest problem was that, after the testing, there was no beer left to sell, the testers drank it all (remember I’m making this part up to illustrate a point, they probably still had beer left to sell).\nGuinness had a sampling and population problem. They wanted to know that the entire population of the beers they made were all 5 out of 5 stars. But, if they sampled the entire population, they would drink all of their beer, and wouldn’t have any left to sell.\nEnter William Sealy Gosset. Gosset figured out the solution to the problem. He asked questions like this:\n\nHow many samples do I need to take to know the whole population is 5 out of 5?\nWhat’s the fewest amount of samples I need to take to know the above, that would mean Guinness could test fewer beers for quality, sell more beers for profit, and make the product testing time shorter.\n\nGosset solved those questions, and he invented something called the Student’s t-test. Gosset was working for Guinness, and could be fired for releasing trade-secrets that he invented (the t-test). But, Gosset published the work anyways, under a pseudonym [@Student1908]. He called himself Student, hence Student’s t-test. Now you know the rest of the story.\nIt turns out this was a very nice thing for Gosset to have done. t-tests are used all the time, and they are useful, that’s why they are used. In this chapter we learn how they work.\nYou’ll be surprised to learn that what we’ve already talked about, (the Crump Test, and the Randomization Test), are both very very similar to the t-test. So, in general, you have already been thinking about the things you need to think about to understand t-tests. You’re probably wondering what is this \\(t\\), what does \\(t\\) mean? We will tell you. Before we tell what it means, we first tell you about one more idea.\n\n\nWe’ve talked about getting a sample of data. We know we can find the mean, we know we can find the standard deviation. We know we can look at the data in a histogram. These are all useful things to do for us to learn something about the properties of our data.\nYou might be thinking of the mean and standard deviation as very different things that we would not put together. The mean is about central tendency (where most of the data is), and the standard deviation is about variance (where most of the data isn’t). Yes, they are different things, but we can use them together to create useful new things.\nWhat if I told you my sample mean was 50, and I told you nothing else about my sample. Would you be confident that most of the numbers were near 50? Would you wonder if there was a lot of variability in the sample, and many of the numbers were very different from 50. You should wonder all of those things. The mean alone, just by itself, doesn’t tell you anything about well the mean represents all of the numbers in the sample.\nIt could be a representative number, when the standard deviation is very small, and all the numbers are close to 50. It could be a non-representative number, when the standard deviation is large, and many of the numbers are not near 50. You need to know the standard deviation in order to be confident in how well the mean represents the data.\nHow can we put the mean and the standard deviation together, to give us a new number that tells us about confidence in the mean?\nWe can do this using a ratio:\n\\(\\frac{mean}{\\text{standard deviation}}\\)\nThink about what happens here. We are dividing a number by a number. Look at what happens:\n\\(\\frac{number}{\\text{same number}} = 1\\)\n\\(\\frac{number}{\\text{smaller number}} = \\text{big number}\\)\ncompared to:\n\\(\\frac{number}{\\text{bigger number}} = \\text{smaller number}\\)\nImagine we have a mean of 50, and a truly small standard deviation of 1. What do we get with our formula?\n\\(\\frac{50}{1} = 50\\)\nImagine we have a mean of 50, and a big standard deviation of 100. What do we get with our formula?\n\\(\\frac{50}{100} = 0.5\\)\nNotice, when we have a mean paired with a small standard deviation, our formula gives us a big number, like 50. When we have a mean paired with a large standard deviation, our formula gives us a small number, like 0.5. These numbers can tell us something about confidence in our mean, in a general way. We can be 50 confident in our mean in the first case, and only 0.5 (not at a lot) confident in the second case.\nWhat did we do here? We created a descriptive statistic by dividing the mean by the standard deviation. And, we have a sense of how to interpret this number, when it’s big we’re more confident that the mean represents all of the numbers, when it’s small we are less confident. This is a useful kind of number, a ratio between what we think about our sample (the mean), and the variability in our sample (the standard deviation). Get used to this idea. Almost everything that follows in this textbook is based on this kind of ratio. We will see that our ratio turns into different kinds of “statistics”, and the ratios will look like this in general:\n\\(\\text{name of statistic} = \\frac{\\text{measure of what we know}}{\\text{measure of what we don't know}}\\)\nor, to say it using different words:\n\\(\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\nIn fact, this is the general formula for the t-test. Big surprise!\n\n\n\nNow we are ready to talk about t-test. We will talk about three of them. We start with the one-sample t-test.\nCommonly, the one-sample t-test is used to estimate the chances that your sample came from a particular population. Specifically, you might want to know whether the mean that you found from your sample, could have come from a particular population having a particular mean.\nStraight away, the one-sample t-test becomes a little confusing (and I haven’t even described it yet). Officially, it uses known parameters from the population, like the mean of the population and the standard deviation of the population. However, most times you don’t know those parameters of the population! So, you have to estimate them from your sample. Remember from the chapters on descriptive statistics and sampling, our sample mean is an unbiased estimate of the population mean. And, our sample standard deviation (the one where we divide by n-1) is an unbiased estimate of the population standard deviation. When Gosset developed the t-test, he recognized that he could use these estimates from his samples, to make the t-test. Here is the formula for the one sample t-test, we first use words, and then become more specific:\n\n\n\\(\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\n\\(\\text{t} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\n\\(\\text{t} = \\frac{\\text{Mean difference}}{\\text{standard error}}\\)\n\\(\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}}\\)\n\\(\\text{t} = \\frac{\\text{Sample Mean  - Population Mean}}{\\text{Sample Standard Error}}\\)\n\\(\\text{Estimated Standard Error} = \\text{Standard Error of Sample} = \\frac{s}{\\sqrt{N}}\\)\nWhere, s is the sample standard deviation.\nSome of you may have gone cross-eyed looking at all of this. Remember, we’ve seen it before when we divided our mean by the standard deviation in the first bit. The t-test is just a measure of a sample mean, divided by the standard error of the sample mean. That is it.\n\n\n\n\\(t\\) gives us a measure of confidence, just like our previous ratio for dividing the mean by a standard deviations. The only difference with \\(t\\), is that we divide by the standard error of mean (remember, this is also a standard deviation, it is the standard deviation of the sampling distribution of the mean)\n\n\n\n\n\n\nNote\n\n\n\nWhat does the t in t-test stand for? Apparently nothing. Gosset originally labelled it z. And, Fisher later called it t, perhaps because t comes after s, which is often used for the sample standard deviation.\n\n\n\\(t\\) is a property of the data that you collect. You compute it with a sample mean, and a sample standard error (there’s one more thing in the one-sample formula, the population mean, which we get to in a moment). This is why we call \\(t\\), a sample-statistic. It’s a statistic we compute from the sample.\nWhat kinds of numbers should we expect to find for these \\(ts\\)? How could we figure that out?\nLet’s start small and work through some examples. Imagine your sample mean is 5. You want to know if it came from a population that also has a mean of 5. In this case, what would \\(t\\) be? It would be zero: we first subtract the sample mean from the population mean, \\(5-5=0\\). Because the numerator is 0, \\(t\\) will be zero. So, \\(t\\) = 0, occurs, when there is no difference.\nLet’s say you take another sample, do you think the mean will be 5 every time, probably not. Let’s say the mean is 6. So, what can \\(t\\) be here? It will be a positive number, because 6-5= +1. But, will \\(t\\) be +1? That depends on the standard error of the sample. If the standard error of the sample is 1, then \\(t\\) could be 1, because 1/1 = 1.\nIf the sample standard error is smaller than 1, what happens to \\(t\\)? It get’s bigger right? For example, 1 divided by 0.5 = 2. If the sample standard error was 0.5, \\(t\\) would be 2. And, what could we do with this information? Well, it be like a measure of confidence. As \\(t\\) get’s bigger we could be more confident in the mean difference we are measuring.\nCan \\(t\\) be smaller than 1? Sure, it can. If the sample standard error is big, say like 2, then \\(t\\) will be smaller than one (in our case), e.g., 1/2 = .5. The direction of the difference between the sample mean and population mean, can also make the \\(t\\) become negative. What if our sample mean was 4. Well, then \\(t\\) will be negative, because the mean difference in the numerator will be negative, and the number in the bottom (denominator) will always be positive (remember why, it’s the standard error, computed from the sample standard deviation, which is always positive because of the squaring that we did.).\nSo, that is some intuitions about what the kinds of values t can take. \\(t\\) can be positive or negative, and big or small.\nLet’s do one more thing to build our intuitions about what \\(t\\) can look like. How about we sample some numbers and then measure the sample mean and the standard error of the mean, and then plot those two things against each each. This will show us how a sample mean typically varies with respect to the standard error of the mean.\nIn Figure 1, I pulled 1,000 samples of \\(N = 10\\) from a normal distribution (mean = 0, sd = 1). Each time I measured the mean and standard error of the sample. That gave two descriptive statistics for each sample, letting us plot each sample as dot in a scatter plot.\n\n\nCode\nlibrary(ggplot2)\n\n\n\n\nCode\nsample_mean &lt;- length(1000)\nsample_se &lt;- length(1000)\n\nfor (i in 1:1000) {\n  s &lt;- rnorm(10, 0, 1)\n  sample_mean[i] &lt;- mean(s)\n  sample_se[i] &lt;- sd(s) / sqrt(length(s))\n}\n\nplot(sample_mean, sample_se)\n\n\n\n\n\n\n\n\nFigure 1: A scatter plot with sample mean on the x-axis, and standard error of the mean on the y-axis\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nTake penguins, and then,\nadd new columns for the bill ratio and bill area.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n1penguins |&gt;\n2  mutate(\n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm\n  )\n\n\n\n1\n\nTake penguins, and then,\n\n2\n\nadd new columns for the bill ratio and bill area.\n\n\n\n\n# A tibble: 344 × 10\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 4 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_ratio &lt;dbl&gt;, bill_area &lt;dbl&gt;\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins |&gt;\n  mutate(\n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm\n  )         \n\n\n# A tibble: 344 × 10\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 4 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_ratio &lt;dbl&gt;, bill_area &lt;dbl&gt;\n\n\n\nWhat we get is a cloud of dots. You might notice the cloud has a circular quality. There’s more dots in the middle, and fewer dots as they radiate out from the middle. The dot cloud shows us the general range of the sample mean, for example most of the dots are in between -1 and 1. Similarly, the range for the sample standard error is roughly between .2 and .5. Remember, each dot represents one sample.\nWe can look at the same data a different way. For example, rather than using a scatter plot, we can divide the mean for each dot by the standard error for each dot. Figure 2 shows the result in a histogram.\n\n\nCode\nhist(sample_mean/sample_se, breaks=30)\n\n\n\n\n\n\n\n\nFigure 2: A histogram of the sample means divided by the sample standard errors, this is a t-distribution.\n\n\n\n\n\nInteresting, we can see the histogram is shaped like a normal curve. It is centered on 0, which is the most common value. As values become more extreme, they become less common. If you remember, our formula for \\(t\\), was the mean divided by the standard error of the mean. That’s what we did here. This histogram is showing you a \\(t\\)-distribution.\n\n\n\nLet’s briefly calculate a t-value from a small sample. Let’s say we had 10 students do a true/false quiz with 5 questions on it. There’s a 50% chance of getting each answer correct.\nEvery student completes the 5 questions, we grade them, and then we find their performance (mean percent correct). What we want to know is whether the students were guessing. If they were all guessing, then the sample mean should be about 50%, it shouldn’t be different from chance, which is 50%. Let’s look at Table 1.\n\n\n\n\nTable 1: Calculating the t-value for a one-sample test.\n\n\n\n\n\n\nstudents\nscores\nmean\nDifference_from_Mean\nSquared_Deviations\n\n\n\n\n1\n50\n61\n-11\n121\n\n\n2\n70\n61\n9\n81\n\n\n3\n60\n61\n-1\n1\n\n\n4\n40\n61\n-21\n441\n\n\n5\n80\n61\n19\n361\n\n\n6\n30\n61\n-31\n961\n\n\n7\n90\n61\n29\n841\n\n\n8\n60\n61\n-1\n1\n\n\n9\n70\n61\n9\n81\n\n\n10\n60\n61\n-1\n1\n\n\nSums\n610\n610\n0\n2890\n\n\nMeans\n61\n61\n0\n289\n\n\n\n\n\nsd\n17.92\n\n\n\n\n\nSEM\n5.67\n\n\n\n\n\nt\n1.94003527336861\n\n\n\n\n\n\n\n\nYou can see the scores column has all of the test scores for each of the 10 students. We did the things we need to do to compute the standard deviation.\nRemember the sample standard deviation is the square root of the sample variance, or:\n\\(\\text{sample standard deviation} = \\sqrt{\\frac{\\sum_{i}^{n}({x_{i}-\\bar{x})^2}}{N-1}}\\)\n\\(\\text{sd} = \\sqrt{\\frac{2890}{10-1}} = 17.92\\)\nThe standard error of the mean, is the standard deviation divided by the square root of N\n\\(\\text{SEM} = \\frac{s}{\\sqrt{N}} = \\frac{17.92}{10} = 5.67\\)\n\\(t\\) is the difference between our sample mean (61), and our population mean (50, assuming chance), divided by the standard error of the mean.\n\\(\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}} = \\frac{\\bar{X}-u}{SEM} = \\frac{61-50}{5.67} = 1.94\\)\nAnd, that is you how calculate \\(t\\), by hand. It’s a pain. I was annoyed doing it this way. In the lab, you learn how to calculate \\(t\\) using software, so it will just spit out \\(t\\). For example in R, all you have to do is this:\n\n\nCode\nt.test(scores, mu=50)\n\n\n\n    One Sample t-test\n\ndata:  scores\nt = 1.9412, df = 9, p-value = 0.08415\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 48.18111 73.81889\nsample estimates:\nmean of x \n       61 \n\n\n\n\n\nIf \\(t\\) is just a number that we can compute from our sample (it is), what can we do with it? How can we use \\(t\\) for statistical inference?\nRemember back to the chapter on sampling and distributions, that’s where we discussed the sampling distribution of the sample mean. Remember, we made a lot of samples, then computed the mean for each sample, then we plotted a histogram of the sample means. Later, in that same section, we mentioned that we could generate sampling distributions for any statistic. For each sample, we could compute the mean, the standard deviation, the standard error, and now even \\(t\\), if we wanted to. We could generate 10,000 samples, and draw four histograms, one for each sampling distribution for each statistic.\nThis is exactly what I did, and the results are shown in the four panels of Figure 3 below. I used a sample size of 20, and drew random observations for each sample from a normal distribution, with mean = 0, and standard deviation = 1. Let’s look at the sampling distributions for each of the statistics. \\(t\\) was computed assuming with the population mean assumed to be 0.\n\n\nCode\nall_df &lt;- data.frame()\nfor (i in 1:10000) {\n  sample &lt;- rnorm(20, 0, 1)\n  sample_mean &lt;- mean(sample)\n  sample_sd &lt;- sd(sample)\n  sample_se &lt;- sd(sample) / sqrt(length(sample))\n  sample_t &lt;- as.numeric(t.test(sample, mu = 0)$statistic)\n  t_df &lt;- data.frame(i, sample_mean, sample_sd, sample_se, sample_t)\n  all_df &lt;- rbind(all_df, t_df)\n}\n\nlibrary(ggpubr)\na &lt;- ggplot(all_df, aes(x = sample_mean)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nb &lt;- ggplot(all_df, aes(x = sample_sd)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nc &lt;- ggplot(all_df, aes(x = sample_se)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nd &lt;- ggplot(all_df, aes(x = sample_t)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\n\nggarrange(a, b, c, d,\n          ncol = 2, nrow = 2)\n\n\n\n\n\n\n\n\nFigure 3: Sampling distributions for the mean, standard deviation, standard error of the mean, and \\(t\\).\n\n\n\n\n\nWe see four sampling distributions. This is how statistical summaries of these summaries behave. We have used the word chance windows before. These are four chance windows, measuring different aspects of the sample. In this case, all of the samples came from the same normal distribution. Because of sampling error, each sample is not identical. The means are not identical, the standard deviations are not identical, sample standard error of the means are not identical, and the \\(t\\)s of the samples are not identical. They all have some variation, as shown by the histograms. This is how samples of size 20 behave.\nWe can see straight away, that in this case, we are unlikely to get a sample mean of 2. That’s way outside the window. The range for the sampling distribution of the mean is around -.5 to +.5, and is centered on 0 (the population mean, would you believe!).\nWe are unlikely to get sample standard deviations of between .6 and 1.5, that is a different range, specific to the sample standard deviation.\nSame thing with the sample standard error of the mean, the range here is even smaller, mostly between .1, and .3. You would rarely find a sample with a standard error of the mean greater than .3. Virtually never would you find one of say 1 (for this situation).\nNow, look at \\(t\\). It’s range is basically between -3 and +3 here. 3s barely happen at all. You pretty much never see a 5 or -5 in this situation.\nAll of these sampling windows are chance windows, and they can all be used in the same way as we have used similar sampling distributions before (e.g., Crump Test, and Randomization Test) for statistical inference. For all of them we would follow the same process:\n\nGenerate these distributions\nLook at your sample statistics for the data you have (mean, SD, SEM, and \\(t\\))\nFind the likelihood of obtaining that value or greater\nObtain that probability\nSee if you think your sample statistics were probable or improbable.\n\nWe’ll formalize this in a second. I just want you to know that what you will be doing is something that you have already done before. For example, in the Crump test and the Randomization test we focused on the distribution of mean differences. We could do that again here, but instead, we will focus on the distribution of \\(t\\) values. We then apply the same kinds of decision rules to the \\(t\\) distribution, as we did for the other distributions. Below you will see a graph you have already seen, except this time it is a distribution of \\(t\\)s, not mean differences:\nRemember, if we obtained a single \\(t\\) from one sample we collected, we could consult the chance window in Figure 4 below to find out whether the \\(t\\) we obtained from the sample was likely or unlikely to occur by chance.\n\n\nCode\nsample_t &lt;- all_df$sample_t\n\nggplot(all_df, aes(x = sample_t)) +\n  annotate(\n    \"rect\",\n    xmin = min(sample_t),\n    xmax = max(sample_t),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"red\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = min(sample_t),\n    xmax = -1.94,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = 1.94,\n    xmax = max(sample_t),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  geom_rect(aes(\n    xmin = -Inf,\n    xmax = min(sample_t),\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_rect(aes(\n    xmin = max(sample_t),\n    xmax = Inf,\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_histogram(bins = 50, color = \"white\") +\n  theme_classic() +\n  geom_vline(xintercept = min(sample_t)) +\n  geom_vline(xintercept = max(sample_t)) +\n  geom_vline(xintercept = -1.94) +\n  geom_vline(xintercept = 1.94) +\n  xlim(-8, 8) +\n  geom_label(data = data.frame(x = 0, y = 250, label = \"CHANCE\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -7, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 7, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  #  geom_label(data = data.frame(x = min(sample_t), y = 600,\n  #                              label = paste0(\"min \\n\",round(min(sample_t)))),\n  #                             aes(x = x, y = y, label = label))+\n  #geom_label(data = data.frame(x = max(sample_t), y = 600,\n  #                            label = paste0(\"max \\n\",round(max(sample_t)))),\n  #                           aes(x = x, y = y, label = label))+\n  geom_label(data = data.frame(x = -4, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 4, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  xlab(\"mean sample_t\")\n\n\n\n\n\n\n\n\nFigure 4: Applying decision criteria to the \\(t\\)-distribution. Histogram of \\(t\\)s from samples (n=20) drawn from the same normal distribution (u=0, sd=1)\n\n\n\n\n\n\n\n\nFrom our early example involving the TRUE/FALSE quizzes, we are now ready to make some kind of decision about what happened there. We found a mean difference of 11. We found a \\(t\\) = 1.9411765. The probability of this \\(t\\) or larger occurring is \\(p\\) = 0.0841503. We were testing the idea that our sample mean of 61 could have come from a normal distribution with mean = 50. The \\(t\\) test tells us that the \\(t\\) for our sample, or a larger one, would happen with p = 0.0841503. In other words, chance can do it a kind of small amount of time, but not often. In English, this means that all of the students could have been guessing, but it wasn’t that likely that were just guessing.\nThe next \\(t\\)-test is called a paired samples t-test. And, spoiler alert, we will find out that a paired samples t-test is actually a one-sample t-test in disguise (WHAT!), yes it is. If the one-sample \\(t\\)-test didn’t make sense to you, read the next section.\n\n\n\n\nFor me (Crump), many analyses often boil down to a paired samples t-test. It just happens that many things I do reduce down to a test like this.\nI am a cognitive psychologist, I conduct research about how people do things like remember, pay attention, and learn skills. There are lots of Psychologists like me, who do very similar things.\nWe all often conduct the same kinds of experiments. They go like this, and they are called repeated measures designs. They are called repeated measures designs, because we measure how one person does something more than once, we repeat the measure.\nSo, I might measure somebody doing something in condition A, and measure the same person doing something in Condition B, and then I see that same person does different things in the two conditions. I repeatedly measure the same person in both conditions. I am interested in whether the experimental manipulation changes something about how people perform the task in question.\n\n\nWe will introduce the paired-samples t-test with an example using real data, from a real study. @mehr20165 were interested in whether singing songs to infants helps infants become more sensitive to social cues. For example, infants might need to learn to direct their attention toward people as a part of learning how to interact socially with people. Perhaps singing songs to infants aids this process of directing attention. When an infant hears a familiar song, they might start to pay more attention to the person singing that song, even after they are done singing the song. The person who sang the song might become more socially important to the infant. You will learn more about this study in the lab for this week. This example, prepares you for the lab activities. Here is a brief summary of what they did.\nFirst, parents were trained to sing a song to their infants. After many days of singing this song to the infants, a parent came into the lab with their infant. In the first session, parents sat with their infants on their knees, so the infant could watch two video presentations. There were two videos. Each video involved two unfamiliar new people the infant had never seen before. Each new person in the video (the singers) sang one song to the infant. One singer sang the “familiar” song the infant had learned from their parents. The other singer sang an “unfamiliar” song the infant had not hear before.\nThere were two really important measurement phases: the baseline phase, and the test phase.\nThe baseline phase occurred before the infants saw and heard each singer sing a song. During the baseline phase, the infants watched a video of both singers at the same time. The researchers recorded the proportion of time that the infant looked at each singer. The baseline phase was conducted to determine whether infants had a preference to look at either person (who would later sing them a song).\nThe test phase occurred after infants saw and heard each song, sung by each singer. During the test phase, each infant had an opportunity to watch silent videos of both singers. The researchers measured the proportion of time the infants spent looking at each person. The question of interest, was whether the infants would spend a greater proportion of time looking at the singer who sang the familiar song, compared to the singer who sang the unfamiliar song.\nThere is more than one way to describe the design of this study. We will describe it like this. It was a repeated measures design, with one independent (manipulation) variable called Viewing phase: Baseline versus Test. There was one dependent variable (the measurement), which was proportion looking time (to singer who sung familiar song). This was a repeated measures design because the researchers measured proportion looking time twice (they repeated the measure), once during baseline (before infants heard each singer sing a song), and again during test (after infants head each singer sing a song).\nThe important question was whether infants would change their looking time, and look more at the singer who sang the familiar song during the test phase, than they did during the baseline phase. This is a question about a change within individual infants. In general, the possible outcomes for the study are:\n\nNo change: The difference between looking time toward the singer of the familiar song during baseline and test is zero, no difference.\nPositive change: Infants will look longer toward the singer of the familiar song during the test phase (after they saw and heard the singers), compared to the baseline phase (before they saw and heard the singers). This is a positive difference if we use the formula: Test Phase Looking time - Baseline phase looking time (to familiar song singer).\nNegative change: Infants will look longer toward the singer of the unfamiliar song during the test phase (after they saw and heard the singers), compared to the baseline phase (before they saw and heard the singers). This is a negative difference if we use the same formula: Test Phase Looking time - Baseline phase looking time (to familiar song singer).\n\n\n\n\nLet’s take a look at the data for the first 5 infants in the study. This will help us better understand some properties of the data before we analyze it. We will see that the data is structured in a particular way that we can take advantage of with a paired samples t-test. Note, we look at the first 5 infants to show how the computations work. The results of the paired-samples t-test change when we use all of the data from the study.\nHere is a table of the data:\n\n\n\n\n\ninfant\nBaseline\nTest\n\n\n\n\n1\n0.44\n0.60\n\n\n2\n0.41\n0.68\n\n\n3\n0.75\n0.72\n\n\n4\n0.44\n0.28\n\n\n5\n0.47\n0.50\n\n\n\n\n\nThe table shows proportion looking times toward the singer of the familiar song during the Baseline and Test phases. Notice there are five different infants, (1 to 5). Each infant is measured twice, once during the Baseline phase, and once during the Test phase. To repeat from before, this is a repeated-measures design, because the infants are measured repeatedly (twice in this case). Or, this kind of design is also called a paired-samples design. Why? because each participant comes with a pair of samples (two samples), one for each level of the design.\nGreat, so what are we really interested in here? We want to know if the mean looking time toward the singer of the familiar song for the Test phase is higher than the Baseline phase. We are comparing the two sample means against each other and looking for a difference. We already know that differences could be obtained by chance alone, simply because we took two sets of samples, and we know that samples can be different. So, we are interested in knowing whether chance was likely or unlikely to have produced any difference we might observe.\nIn other words, we are interested in looking at the difference scores between the baseline and test phase for each infant. The question here is, for each infant, did their proportion looking time to the singer of the familiar song, increase during the test phase as compared to the baseline phase.\n\n\n\nLet’s add the difference scores to the table of data so it is easier to see what we are talking about. The first step in creating difference scores is to decide how you will take the difference, there are two options:\n\nTest phase score - Baseline Phase Score\nBaseline phase score - Test Phase score\n\nLet’s use the first formula. Why? Because it will give us positive differences when the test phase score is higher than the baseline phase score. This makes a positive score meaningful with respect to the study design, we know (because we defined it to be this way), that positive scores will refer to longer proportion looking times (to singer of familiar song) during the test phase compared to the baseline phase.\n\n\nCode\npaired_sample_df &lt;- cbind(paired_sample_df, \n                          differences = (paired_sample_df$Test-\n                                           paired_sample_df$Baseline))\nknitr::kable(paired_sample_df)\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\n\n\n\n\n1\n0.44\n0.60\n0.16\n\n\n2\n0.41\n0.68\n0.27\n\n\n3\n0.75\n0.72\n-0.03\n\n\n4\n0.44\n0.28\n-0.16\n\n\n5\n0.47\n0.50\n0.03\n\n\n\n\n\nThere we have it, the difference scores. The first thing we can do here is look at the difference scores, and ask how many infants showed the effect of interest. Specifically, how many infants showed a positive difference score. We can see that three of five infants showed a positive difference (they looked more at the singer of the familiar song during the test than baseline phase), and two the infants showed the opposite effect (negative difference, they looked more at the singer of the familiar song during baseline than test).\n\n\n\nAs we have been discussing, the effect of interest in this study is the mean difference between the baseline and test phase proportion looking times. We can calculate the mean difference, by finding the mean of the difference scores. Let’s do that, in fact, for fun let’s calculate the mean of the baseline scores, the test scores, and the difference scores.\n\n\nCode\npaired_sample_df &lt;- paired_sample_df %&gt;%\n   rbind(c(\"Sums\",colSums(paired_sample_df[1:5,2:4]))) %&gt;%\n   rbind(c(\"Means\",colMeans(paired_sample_df[1:5,2:4])))\n  \nknitr::kable(paired_sample_df)\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\n\n\n\n\n1\n0.44\n0.6\n0.16\n\n\n2\n0.41\n0.68\n0.27\n\n\n3\n0.75\n0.72\n-0.03\n\n\n4\n0.44\n0.28\n-0.16\n\n\n5\n0.47\n0.5\n0.03\n\n\nSums\n2.51\n2.78\n0.27\n\n\nMeans\n0.502\n0.556\n0.054\n\n\n\n\n\nWe can see there was a positive mean difference of 0.054, between the test and baseline phases.\nCan we rush to judgment and conclude that infants are more socially attracted to individuals who have sung them a familiar song? I would hope not based on this very small sample. First, the difference in proportion looking isn’t very large, and of course we recognize that this difference could have been produced by chance.\nWe will more formally evaluate whether this difference could have been caused by chance with the paired-samples t-test. But, before we do that, let’s again calculate \\(t\\) and discuss what \\(t\\) tells us over and above what our measure of the mean of the difference scores tells us.\n\n\n\nOK, so how do we calculate \\(t\\) for a paired-samples \\(t\\)-test? Surprise, we use the one-sample t-test formula that you already learned about! Specifically, we use the one-sample \\(t\\)-test formula on the difference scores. We have one sample of difference scores (you can see they are in one column), so we can use the one-sample \\(t\\)-test on the difference scores. Specifically, we are interested in comparing whether the mean of our difference scores came from a distribution with mean difference = 0. This is a special distribution we refer to as the null distribution. It is the distribution no differences. Of course, this null distribution can produce differences due to to sampling error, but those differences are not caused by any experimental manipulation, they caused by the random sampling process.\nWe calculate \\(t\\) in a moment. Let’s now consider again why we want to calculate \\(t\\)? Why don’t we just stick with the mean difference we already have?\nRemember, the whole concept behind \\(t\\), is that it gives an indication of how confident we should be in our mean. Remember, \\(t\\) involves a measure of the mean in the numerator, divided by a measure of variation (standard error of the sample mean) in the denominator. The resulting \\(t\\) value is small when the mean difference is small, or when the variation is large. So small \\(t\\)-values tell us that we shouldn’t be that confident in the estimate of our mean difference. Large \\(t\\)-values occur when the mean difference is large and/or when the measure of variation is small. So, large \\(t\\)-values tell us that we can be more confident in the estimate of our mean difference. Let’s find \\(t\\) for the mean difference scores. We use the same formulas as we did last time:\n\n\n\n\n\n\n\n\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\ndiff_from_mean\nSquared_differences\n\n\n\n\n1\n0.44\n0.6\n0.16\n0.106\n0.011236\n\n\n2\n0.41\n0.68\n0.27\n0.216\n0.046656\n\n\n3\n0.75\n0.72\n-0.03\n-0.084\n0.00705600000000001\n\n\n4\n0.44\n0.28\n-0.16\n-0.214\n0.045796\n\n\n5\n0.47\n0.5\n0.03\n-0.024\n0.000575999999999999\n\n\nSums\n2.51\n2.78\n0.27\n0\n0.11132\n\n\nMeans\n0.502\n0.556\n0.054\n0\n0.022264\n\n\n\n\n\n\nsd\n0.167\n\n\n\n\n\n\nSEM\n0.075\n\n\n\n\n\n\nt\n0.72\n\n\n\n\n\nIf we did this test using R, we would obtain almost the same numbers (there is a little bit of rounding in the table).\n\n\nCode\nt.test(differences,mu=0)\n\n\n\n    One Sample t-test\n\ndata:  differences\nt = 0.72381, df = 4, p-value = 0.5092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.1531384  0.2611384\nsample estimates:\nmean of x \n    0.054 \n\n\nHere is a quick write up of our t-test results, t(4) = .72, p = .509.\nWhat does all of that tell us? There’s a few things we haven’t gotten into much yet. For example, the 4 represents degrees of freedom, which we discuss later. The important part, the \\(t\\) value should start to be a little bit more meaningful. We got a kind of small t-value didn’t we. It’s .72. What can we tell from this value? First, it is positive, so we know the mean difference is positive. The sign of the \\(t\\)-value is always the same as the sign of the mean difference (ours was +0.054). We can also see that the p-value was .509. We’ve seen p-values before. This tells us that our \\(t\\) value or larger, occurs about 50.9% of the time… Actually it means more than this. And, to understand it, we need to talk about the concept of two-tailed and one-tailed tests.\n\n\n\nRemember what it is we are doing here. We are evaluating whether our sample data could have come from a particular kind of distribution. The null distribution of no differences. This is the distribution of \\(t\\)-values that would occur for samples of size 5, with a mean difference of 0, and a standard error of the sample mean of .075 (this is the SEM that we calculated from our sample). We can see what this particular null-distribution looks like in Figure 5.\n\n\nCode\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  scale_x_continuous(breaks = (seq(-3, 3, .5))) +\n  geom_label(data = data.frame(x = -.7, y = .1, label = \"50% \\n (-)\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .7, y = .1, label = \"50% \\n (+)\"), aes(x = x, y = y, label = label)) +\n  geom_vline(xintercept = 0)+\n  theme_classic(base_size = 20)\n\n\n\n\n\n\n\n\nFigure 5: A distribution of \\(t\\)-values that can occur by chance alone, when there is no difference between the sample and a population\n\n\n\n\n\nThe \\(t\\)-distribution above shows us the kinds of values \\(t\\) will will take by chance alone, when we measure the mean differences for pairs of 5 samples (like our current). \\(t\\) is most likely to be zero, which is good, because we are looking at the distribution of no-differences, which should most often be 0! But, sometimes, due to sampling error, we can get \\(t\\)s that are bigger than 0, either in the positive or negative direction. Notice the distribution is symmetrical, a \\(t\\) from the null-distribution will be positive half of the time, and negative half of the time, that is what we would expect by chance.\nSo, what kind of information do we want know when we find a particular \\(t\\) value from our sample? We want to know how likely the \\(t\\) value like the one we found occurs just by chance. This is actually a subtly nuanced kind of question. For example, any particular \\(t\\) value doesn’t have a specific probability of occurring. When we talk about probabilities, we are talking about ranges of probabilities. Let’s consider some probabilities. We will use the letter \\(p\\), to talk about the probabilities of particular \\(t\\) values.\n\nWhat is the probability that \\(t\\) is zero or positive or negative? The answer is p=1, or 100%. We will always have a \\(t\\) value that is zero or non-zero…Actually, if we can’t compute the t-value, for example when the standard deviation is undefined, I guess then we would have a non-number. But, assuming we can calculate \\(t\\), then it will always be 0 or positive or negative.\nWhat is the probability of \\(t\\) = 0 or greater than 0? The answer is p=.5, or 50%. 50% of \\(t\\)-values are 0 or greater.\nWhat is the of \\(t\\) = 0 or smaller than 0? The answer is p=.5, or 50%. 50% of \\(t\\)-values are 0 or smaller.\n\nWe can answer all of those questions just by looking at our t-distribution, and dividing it into two equal regions, the left side (containing 50% of the \\(t\\) values), and the right side containing 50% of the \\(t\\)-values).\nWhat if we wanted to take a more fine-grained approach, let’s say we were interested in regions of 10%. What kinds of \\(t\\)s occur 10% of the time. We would apply lines like the following. Notice, the likelihood of bigger numbers (positive or negative) gets smaller, so we have to increase the width of the bars for each of the intervals between the bars to contain 10% of the \\(t\\)-values, it looks like Figure 6.\n\n\nCode\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = t_ps) +\n  theme_classic(base_size = 15) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  scale_x_continuous(breaks = round(t_ps, digits = 1)) +\n  geom_label(data = data.frame(x = -2.5, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -1.3, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 1.3, y = .2, label = \"10%\"), aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\n\nFigure 6: Splitting the t distribution up into regions each containing 10% of the \\(t\\)-values. The width between the bars narrows as they approach the center of the distribution, where there are more \\(t\\)-values.\n\n\n\n\n\nConsider the probabilities (\\(p\\)) of \\(t\\) for the different ranges.\n\n\\(t\\) &lt;= -1.5 (\\(t\\) is less than or equal to -1.5), \\(p\\) = 10%\n-1.5 &gt;= \\(t\\) &lt;= -0.9 (\\(t\\) is equal to or between -1.5 and -.9), \\(p\\) = 10%\n-.9 &gt;= \\(t\\) &lt;= -0.6 (\\(t\\) is equal to or between -.9 and -.6), \\(p\\) = 10%\n\\(t\\) &gt;= 1.5 (\\(t\\) is greater than or equal to 1.5), \\(p\\) = 10%\n\nNotice, that the \\(p\\)s are always 10%. \\(t\\)s occur in these ranges with 10% probability.\n\n\n\nYou might be wondering where I am getting some of these values from. For example, how do I know that 10% of \\(t\\) values (for this null distribution) have a value of approximately 1.5 or greater than 1.5? The answer is I used R to tell me.\nIn most statistics textbooks the answer would be: there is a table at the back of the book where you can look these things up…This textbook has no such table. We could make one for you. And, we might do that. But, we didn’t do that yet…\nSo, where do these values come from, how can you figure out what they are? The complicated answer is that we are not going to explain the math behind finding these values because, 1) the authors (some of us) admittedly don’t know the math well enough to explain it, and 2) it would sidetrack us to much, 3) you will learn how to get these numbers in the lab with software, 4) you will learn how to get these numbers in lab without the math, just by doing a simulation, and 5) you can do it in R, or excel, or you can use an online calculator.\nThis is all to say that you can find the \\(t\\)s and their associated \\(p\\)s using software. But, the software won’t tell you what these values mean. That’s we are doing here. You will also see that software wants to know a few more things from you, such as the degrees of freedom for the test, and whether the test is one-tailed or two tailed. We haven’t explained any of these things yet. That’s what we are going to do now. Note, we explain degrees of freedom last. First, we start with a one-tailed test.\n\n\n\nA one-tailed test is sometimes also called a directional test. It is called a directional test, because a researcher might have a hypothesis in mind suggesting that the difference they observe in their means is going to have a particular direction, either a positive difference, or a negative difference.\nTypically, a researcher would set an alpha criterion. The alpha criterion describes a line in the sand for the researcher. Often, the alpha criterion is set at \\(p = .05\\). What does this mean? Figure 7 shows the \\(t\\)-distribution and the alpha criterion.\n\n\nCode\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = qt(.95, 4, lower.tail = TRUE)) +\n  ggtitle(\"Critical t for one-tailed test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = qt(.95, 4, lower.tail = TRUE),\n    xmax = 3,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.95, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\n\nFigure 7: The critical value of t for an alpha criterion of 0.05. 5% of all ts are at this value or larger\n\n\n\n\n\nThe figure shows that \\(t\\) values of +2.13 or greater occur 5% of the time. Because the t-distribution is symmetrical, we also know that \\(t\\) values of -2.13 or smaller also occur 5% of the time. Both of these properties are true under the null distribution of no differences. This means, that when there really are no differences, a researcher can expect to find \\(t\\) values of 2.13 or larger 5% of the time.\nLet’s review and connect some of the terms:\n\nalpha criterion: the criterion set by the researcher to make decisions about whether they believe chance did or did not cause the difference. The alpha criterion here is set to \\(p = .05\\).\nCritical \\(t\\). The critical \\(t\\) is the \\(t\\)-value associated with the alpha-criterion. In this case for a one-tailed test, it is the \\(t\\) value where 5% of all \\(t\\)s are this number or greater. In our example, the critical \\(t\\) is 2.13. 5% of all \\(t\\) values (with degrees of freedom = 4) are +2.13, or greater than +2.13.\nObserved \\(t\\). The observed \\(t\\) is the one that you calculated from your sample. In our example about the infants, the observed \\(t\\) was \\(t\\) (4) = 0.72.\np-value. The \\(p\\)-value is the probability of obtaining the observed \\(t\\) value or larger. Now, you could look back at our previous example, and find that the \\(p\\)-value for \\(t\\) (4) = .72, was \\(p = .509\\) . HOWEVER, this p-value was not calculated for a one-directional test…(we talk about what .509 means in the next section).\n\nFigure 8 shows what the \\(p\\)-value for \\(t\\) (4) = .72 using a one-directional test would would look like:\n\n\nCode\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = .72) +\n  geom_vline(xintercept = qt(.95, 4, lower.tail = TRUE)) +\n  ggtitle(\"t value and p-range for one-directional test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = .72,\n    xmax = 3,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = qt(.95, 4, lower.tail = TRUE),\n    xmax = 3,\n    ymin = 0,\n    ymax = .25,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.95, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .72, y = .1,\n                               label = \"Observed t\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .72, y = .05,\n                               label = \".72, p=\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = 1.5,\n    y = .05,\n    label = round(pt(.72, 4, lower.tail = FALSE), digits =\n                    3)\n  ),\n  aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\n\nFigure 8: A case where the observed value of t is much less than the critical value for a one-directional t-test.\n\n\n\n\n\nLet’s take this one step at a time. We have located the observed \\(t\\) of .72 on the graph. We shaded the right region all grey. What we see is that the grey region represents .256 or 25.6% of all \\(t\\) values. In other words, 25.6% of \\(t\\) values are .72 or larger than .72. You could expect, by chance alone, to a find a \\(t\\) value of .72 or larger, 25.6% of the time. That’s fairly often. We did find a \\(t\\) value of .72. Now that you know this kind of \\(t\\) value or larger occurs 25.6% of the time, would you be confident that the mean difference was not due to chance? Probably not, given that chance can produce this difference fairly often.\nFollowing the “standard” decision making procedure, we would claim that our \\(t\\) value was not statistically significant, because it was not large enough. If our observed value was larger than the critical \\(t\\) (larger than 2.13), defined by our alpha criterion, then we would claim that our \\(t\\) value was statistically significant. This would be equivalent to saying that we believe it is unlikely that the difference we observed was due to chance. In general, for any observed \\(t\\) value, the associated \\(p\\)-value tells you how likely a \\(t\\) of the observed size or larger would be observed. The \\(p\\)-value always refers to a range of \\(t\\)-values, never to a single \\(t\\)-value. Researchers use the alpha criterion of .05, as a matter of convenience and convention. There are other ways to interpret these values that do not rely on a strict (significant versus not) dichotomy.\n\n\n\nOK, so that was one-tailed tests… What are two tailed tests? The \\(p\\)-value that we originally calculated from our paired-samples \\(t\\)-test was for a 2-tailed test. Often, the default is that the \\(p\\)-value is for a two-tailed test.\nThe two-tailed test, is asking a more general question about whether a difference is likely to have been produced by chance. The question is: what is probability of any difference. It is also called a non-directional test, because here we don’t care about the direction or sign of the difference (positive or negative), we just care if there is any kind of difference.\nThe same basic things as before are involved. We define an alpha criterion (\\(\\alpha = 0.05\\)). And, we say that any observed \\(t\\) value that has a probability of \\(p\\) &lt;.05 (\\(p\\) is less than .05) will be called statistically significant, and ones that are more likely (\\(p\\) &gt;.05, \\(p\\) is greater than .05) will be called null-results, or not statistically significant. The only difference is how we draw the alpha range. Before it was on the right side of the \\(t\\) distribution (we were conducting a one-sided test remember, so we were only interested in one side).\nFigure 9 shows what the most extreme 5% of the \\(t\\)-values are when we ignore their sign (whether they are positive or negative).\n\n\nCode\nrange &lt;- seq(-4, 4, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = qt(.975, 4, lower.tail = TRUE)) +\n  geom_vline(xintercept = qt(.025, 4, lower.tail = TRUE)) +\n  ggtitle(\"Critical ts for two-tailed test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = qt(.975, 4, lower.tail = TRUE),\n    xmax = 4,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 3.5, y = .2, label = \"2.5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.975, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.975, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.975, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label)) +\n  annotate(\n    \"rect\",\n    xmin = -4,\n    xmax = qt(.025, 4, lower.tail = TRUE),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = -3.5, y = .2, label = \"2.5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.025, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.025, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.025, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\n\nFigure 9: Critical values for a two-tailed test. Each line represents the location where 2.5% of all \\(t\\)s are larger or smaller than critical value. The total for both tails is 5%\n\n\n\n\n\nHere is what we are seeing. A distribution of no differences (the null, which is what we are looking at), will produce \\(t\\)s that are 2.78 or greater 2.5% of the time, and \\(t\\)s that are -2.78 or smaller 2.5% of the time. 2.5% + 2.5% is a total of 5% of the time. We could also say that \\(t\\)s larger than +/- 2.78 occur 5% of the time.\nAs a result, the critical \\(t\\) value is (+/-) 2.78 for a two-tailed test. As you can see, the two-tailed test is blind to the direction or sign of the difference. Because of this, the critical \\(t\\) value is also higher for a two-tailed test, than for the one-tailed test that we did earlier. Hopefully, now you can see why it is called a two-tailed test. There are two tails of the distribution, one on the left and right, both shaded in green.\n\n\n\nNow that you know there are two kinds of tests, one-tailed, and two-tailed, which one should you use? There is some conventional wisdom on this, but also some debate. In the end, it is up to you to be able to justify your choice and why it is appropriate for you data. That is the real answer.\nThe conventional answer is that you use a one-tailed test when you have a theory or hypothesis that is making a directional prediction (the theory predicts that the difference will be positive, or negative). Similarly, use a two-tailed test when you are looking for any difference, and you don’t have a theory that makes a directional prediction (it just makes the prediction that there will be a difference, either positive or negative).\nAlso, people appear to choose one or two-tailed tests based on how risky they are as researchers. If you always ran one-tailed tests, your critical \\(t\\) values for your set alpha criterion would always be smaller than the critical \\(t\\)s for a two-tailed test. Over the long run, you would make more type I errors, because the criterion to detect an effect is a lower bar for one than two tailed tests.\n\nRemember type 1 errors occur when you reject the idea that chance could have caused your difference. You often never know when you make this error. It happens anytime that sampling error was the actual cause of the difference, but a researcher dismisses that possibility and concludes that their manipulation caused the difference.\n\nSimilarly, if you always ran two-tailed tests, even when you had a directional prediction, you would make fewer type I errors over the long run, because the \\(t\\) for a two-tailed test is higher than the \\(t\\) for a one-tailed test. It seems quite common for researchers to use a more conservative two-tailed test, even when they are making a directional prediction based on theory. In practice, researchers tend to adopt a standard for reporting that is common in their field. Whether or not the practice is justifiable can sometimes be an open question. The important task for any researcher, or student learning statistics, is to be able to justify their choice of test.\n\n\n\nBefore we finish up with paired-samples \\(t\\)-tests, we should talk about degrees of freedom. Our sense is that students don’t really understand degrees of freedom very well. If you are reading this textbook, you are probably still wondering what is degrees of freedom, seeing as we haven’t really talked about it all.\nFor the \\(t\\)-test, there is a formula for degrees of freedom. For the one-sample and paired sample \\(t\\)-tests, the formula is:\n\\(\\text{Degrees of Freedom} = \\text{df} = n-1\\). Where n is the number of samples in the test.\nIn our paired \\(t\\)-test example, there were 5 infants. Therefore, degrees of freedom = 5-1 = 4.\nOK, that’s a formula. Who cares about degrees of freedom, what does the number mean? And why do we report it when we report a \\(t\\)-test… you’ve probably noticed the number in parentheses e.g., \\(t\\)(4)=.72, the 4 is the \\(df\\), or degrees of freedom.\nDegrees of freedom is both a concept, and a correction. The concept is that if you estimate a property of the numbers, and you use this estimate, you will be forcing some constraints on your numbers.\nConsider the numbers: 1, 2, 3. The mean of these numbers is 2. Now, let’s say I told you that the mean of three numbers is 2. Then, how many of these three numbers have freedom? Funny question right. What we mean is, how many of the three numbers could be any number, or have the freedom to be any number.\nThe first two numbers could be any number. But, once those two numbers are set, the final number (the third number), MUST be a particular number that makes the mean 2. The first two numbers have freedom. The third number has no freedom.\nTo illustrate. Let’s freely pick two numbers: 51 and -3. I used my personal freedom to pick those two numbers. Now, if our three numbers are 51, -3, and x, and the mean of these three numbers is 2. There is only one solution, x has to be -42, otherwise the mean won’t be 2. This is one way to think about degrees of freedom. The degrees of freedom for these three numbers is n-1 = 3-1= 2, because 2 of the numbers can be free, but the last number has no freedom, it becomes fixed after the first two are decided.\nNow, statisticians often apply degrees of freedom to their calculations, especially when a second calculation relies on an estimated value. For example, when we calculate the standard deviation of a sample, we first calculate the mean of the sample right! By estimating the mean, we are fixing an aspect of our sample, and so, our sample now has n-1 degrees of freedom when we calculate the standard deviation (remember for the sample standard deviation, we divide by n-1…there’s that n-1 again.)\n\n\nThere are at least two ways to think the degrees of freedom for a \\(t\\)-test. For example, if you want to use math to compute aspects of the \\(t\\) distribution, then you need the degrees of freedom to plug in to the formula… If you want to see the formulas I’m talking about, scroll down on the \\(t\\)-test wikipedia page and look for the probability density or cumulative distribution functions…We think that is quite scary for most people, and one reason why degrees of freedom are not well-understood.\nIf we wanted to simulate the \\(t\\) distribution we could more easily see what influence degrees of freedom has on the shape of the distribution. Remember, \\(t\\) is a sample statistic, it is something we measure from the sample. So, we could simulate the process of measuring \\(t\\) from many different samples, then plot the histogram of \\(t\\) to show us the simulated \\(t\\) distribution.\n\n\nCode\nts &lt;- c(rt(10000, 4), rt(10000, 100))\ndfs &lt;- as.factor(rep(c(4, 100), each = 10000))\n\nt_df &lt;- data.frame(dfs, ts)\nt_df &lt;- t_df[abs(t_df$ts) &lt; 5, ]\n\nggplot(t_df, aes(x = ts, group = dfs, color = dfs)) +\n  geom_histogram() +\n  theme_classic() +\n  facet_wrap( ~ dfs) +\n  theme_classic(base_size=15)\n\n\n\n\n\n\n\n\nFigure 10: The width of the t distribution shrinks as sample size and degrees of freedom (from 4 to 100) increases.\n\n\n\n\n\nIn Figure 10 notice that the red distribution for \\(df = 4\\), is a little bit shorter, and a little bit wider than the bluey-green distribution for \\(df = 100\\). As degrees of freedom increase the \\(t\\) distribution gets taller (in the middle), and narrower in the range. It get’s more peaky. Can you guess the reason for this? Remember, we are estimating a sample statistic, and degrees of freedom is really just a number that refers to the number of subjects (well minus one). And, we already know that as we increase \\(n\\), our sample statistics become better estimates (less variance) of the distributional parameters they are estimating. So, \\(t\\) becomes a better estimate of it’s “true” value as sample size increase, resulting in a more narrow distribution of \\(t\\)s.\nThere is a slightly different \\(t\\) distribution for every degrees of freedom, and the critical regions associated with 5% of the extreme values are thus slightly different every time. This is why we report the degrees of freedom for each t-test, they define the distribution of \\(t\\) values for the sample-size in question. Why do we use n-1 and not n? Well, we calculate \\(t\\) using the sample standard deviation to estimate the standard error or the mean, that estimate uses n-1 in the denominator, so our \\(t\\) distribution is built assuming n-1. That’s enough for degrees of freedom…\n\n\n\n\n\nYou must be wondering if we will ever be finished talking about paired samples t-tests… why are we doing round 2, oh no! Don’t worry, we’re just going to 1) remind you about what we were doing with the infant study, and 2) do a paired samples t-test on the entire data set and discuss.\nRemember, we were wondering if the infants would look longer toward the singer who sang the familiar song during the test phase compared to the baseline phase. We showed you data from 5 infants, and walked through the computations for the \\(t\\)-test. As a reminder, it looked like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\ndiff_from_mean\nSquared_differences\n\n\n\n\n1\n0.44\n0.6\n0.16\n0.106\n0.011236\n\n\n2\n0.41\n0.68\n0.27\n0.216\n0.046656\n\n\n3\n0.75\n0.72\n-0.03\n-0.084\n0.00705600000000001\n\n\n4\n0.44\n0.28\n-0.16\n-0.214\n0.045796\n\n\n5\n0.47\n0.5\n0.03\n-0.024\n0.000575999999999999\n\n\nSums\n2.51\n2.78\n0.27\n0\n0.11132\n\n\nMeans\n0.502\n0.556\n0.054\n0\n0.022264\n\n\n\n\n\n\nsd\n0.167\n\n\n\n\n\n\nSEM\n0.075\n\n\n\n\n\n\nt\n0.72\n\n\n\n\n\n\n    One Sample t-test\n\ndata:  round(differences, digits = 2)\nt = 0.72381, df = 4, p-value = 0.5092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.1531384  0.2611384\nsample estimates:\nmean of x \n    0.054 \n\n\nLet’s write down the finding one more time: The mean difference was 0.054, \\(t\\)(4) = .72, \\(p\\) =.509. We can also now confirm, that the \\(p\\)-value was from a two-tailed test. So, what does this all really mean.\nWe can say that a \\(t\\) value with an absolute of .72 or larger occurs 50.9% of the time. More precisely, the distribution of no differences (the null), will produce a \\(t\\) value this large or larger 50.9% of the time. In other words, chance alone good have easily produced the \\(t\\) value from our sample, and the mean difference we observed or .054, could easily have been a result of chance.\nLet’s quickly put all of the data in the \\(t\\)-test, and re-run the test using all of the infant subjects.\n\n\nCode\npaired_sample_df &lt;-  data.frame(infant=1:32, \n                               Baseline = round(experiment_one$Baseline_Proportion_Gaze_to_Singer[1:32],digits=2), \n                               Test = round(experiment_one$Test_Proportion_Gaze_to_Singer[1:32], digits=2))\n\ndifferences &lt;-  paired_sample_df$Test-paired_sample_df$Baseline\nt.test(differences,mu=0)\n\n\n\n    One Sample t-test\n\ndata:  differences\nt = 2.4388, df = 31, p-value = 0.02066\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.01192088 0.13370412\nsample estimates:\nmean of x \n0.0728125 \n\n\nNow we get a very different answer. We would summarize the results saying the mean difference was .073, t(31) = 2.44, p = 0.020. How many total infants were their? Well the degrees of freedom was 31, so there must have been 32 infants in the study. Now we see a much smaller \\(p\\)-value. This was also a two-tailed test, so we that observing a \\(t\\) value of 2.4 or greater (absolute value) only occurs 2% of the time. In other words, the distribution of no differences will produce the observed t-value very rarely. So, it is unlikely that the observed mean difference of .073 was due to chance (it could have been due to chance, but that is very unlikely). As a result, we can be somewhat confident in concluding that something about seeing and hearing a unfamiliar person sing a familiar song, causes an infant to draw their attention toward the singer, and this potentially benefits social learning on the part of the infant.\n\n\n\nIf you’ve been following the Star Wars references, we are on last movie (of the original trilogy)… the independent t-test. This is were basically the same story plays out as before, only slightly different.\nRemember there are different \\(t\\)-tests for different kinds of research designs. When your design is a between-subjects design, you use an independent samples t-test. Between-subjects design involve different people or subjects in each experimental condition. If there are two conditions, and 10 people in each, then there are 20 total people. And, there are no paired scores, because every single person is measured once, not twice, no repeated measures. Because there are no repeated measures we can’t look at the difference scores between conditions one and two. The scores are not paired in any meaningful way, to it doesn’t make sense to subtract them. So what do we do?\nThe logic of the independent samples t-test is the very same as the other \\(t\\)-tests. We calculated the means for each group, then we find the difference. That goes into the numerator of the t formula. Then we get an estimate of the variation for the denominator. We divide the mean difference by the estimate of the variation, and we get \\(t\\). It’s the same as before.\nThe only wrinkle here is what goes into the denominator? How should we calculate the estimate of the variance? It would be nice if we could do something very straightforward like this, say for an experiment with two groups A and B:\n\\(t = \\frac{\\bar{A}-\\bar{B}}{(\\frac{SEM_A+SEM_B}{2})}\\)\nIn plain language, this is just:\n\nFind the mean difference for the top part\nCompute the SEM (standard error of the mean) for each group, and average them together to make a single estimate, pooling over both samples.\n\nThis would be nice, but unfortunately, it turns out that finding the average of two standard errors of the mean is not the best way to do it. This would create a biased estimator of the variation for the hypothesized distribution of no differences. We won’t go into the math here, but instead of the above formula, we an use a different one that gives as an unbiased estimate of the pooled standard error of the sample mean. Our new and improved \\(t\\) formula would look like this:\n\\(t = \\frac{\\bar{X_A}-\\bar{X_B}}{s_p * \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_B}}}\\)\nand, \\(s_p\\), which is the pooled sample standard deviation is defined as, note the $s$es in the formula are variances:\n\\(s_p = \\sqrt{\\frac{(n_A-1)s_A^2 + (n_B-1)s^2_B}{n_A +n_B -2}}\\)\nBelieve you me, that is so much more formula than I wanted to type out. Shall we do one independent \\(t\\)-test example by hand, just to see the computations? Let’s do it…but in a slightly different way than you expect. I show the steps using R. I made some fake scores for groups A and B. Then, I followed all of the steps from the formula, but made R do each of the calculations. This shows you the needed steps by following the code. At the end, I print the \\(t\\)-test values I computed “by hand”, and then the \\(t\\)-test value that the R software outputs using the \\(t\\)-test function. You should be able to get the same values for \\(t\\), if you were brave enough to compute \\(t\\) by hand.\n\n\nCode\n## By \"hand\" using R r code\na &lt;- c(1,2,3,4,5)\nb &lt;- c(3,5,4,7,9)\n\nmean_difference &lt;- mean(a)-mean(b) # compute mean difference\n\nvariance_a &lt;- var(a) # compute variance for A\nvariance_b &lt;- var(b) # compute variance for B\n\n# Compute top part and bottom part of sp formula\n\nsp_numerator &lt;- (4*variance_a + 4* variance_b) \nsp_denominator &lt;- 5+5-2\nsp &lt;- sqrt(sp_numerator/sp_denominator) # compute sp\n\n\n# compute t following formulat\n\nt &lt;- mean_difference / ( sp * sqrt( (1/5) +(1/5) ) )\n\nt # print results\n\n\n[1] -2.017991\n\n\nCode\n# using the R function t.test\nt.test(a,b, paired=FALSE, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  a and b\nt = -2.018, df = 8, p-value = 0.0783\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -5.5710785  0.3710785\nsample estimates:\nmean of x mean of y \n      3.0       5.6 \n\n\n\n\n\nAn “advanced” topic for \\(t\\)-tests is the idea of using R to conduct simulations for \\(t\\)-tests.\nIf you recall, \\(t\\) is a property of a sample. We calculate \\(t\\) from our sample. The \\(t\\) distribution is the hypothetical behavior of our sample. That is, if we had taken thousands upon thousands of samples, and calculated \\(t\\) for each one, and then looked at the distribution of those \\(t\\)’s, we would have the sampling distribution of \\(t\\)!\nIt can be very useful to get in the habit of using R to simulate data under certain conditions, to see how your sample data, and things like \\(t\\) behave. Why is this useful? It mainly prepares you with some intuitions about how sampling error (random chance) can influence your results, given specific parameters of your design, such as sample-size, the size of the mean difference you expect to find in your data, and the amount of variation you might find. These methods can be used formally to conduct power-analyses. Or more informally for data sense.\n\n\nHere are the steps you might follow to simulate data for a one sample \\(t\\)-test.\n\nMake some assumptions about what your sample (that you might be planning to collect) might look like. For example, you might be planning to collect 30 subjects worth of data. The scores of those data points might come from a normal distribution (mean = 50, sd = 10).\nsample simulated numbers from the distribution, then conduct a \\(t\\)-test on the simulated numbers. Save the statistics you want (such as \\(t\\)s and \\(p\\)s), and then see how things behave.\n\nLet’s do this a couple different times. First, let’s simulate samples with N = 30, taken from a normal (mean= 50, sd = 25). We’ll do a simulation with 1000 simulations. For each simulation, we will compare the sample mean with a population mean of 50. There should be no difference on average here. Figure 11 is the null distribution that we are simulating.\n\n\nCode\n# steps to create fake data from a distribution\n# and conduct t-tests on the simulated data\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor (i in 1:1000) {\n  my_sample &lt;- rnorm(n = 30, mean = 50, sd = 25)\n  t_test &lt;- t.test (my_sample, mu = 50)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\nCode\n#plot histograms of t and p values for 1000 simulations\nhist(save_ts)\n\n\n\n\n\n\n\n\nFigure 11: The distribution of \\(t\\)-values under the null. These are the \\(t\\) values that are produced by chance alone.\n\n\n\n\n\n\n\nCode\nhist(save_ps)\n\n\n\n\n\n\n\n\nFigure 12: The distribution of \\(p\\)-values that are observed is flat under the null.\n\n\n\n\n\nNeat. We see both a \\(t\\) distribution, that looks like \\(t\\) distribution as it should. And we see the \\(p\\) distribution. This shows us how often we get \\(t\\) values of particular sizes. You may find it interesting that the \\(p\\)-distribution is flat under the null, which we are simulating here. This means that you have the same chances of a getting a \\(t\\) with a p-value between 0 and 0.05, as you would for getting a \\(t\\) with a p-value between .90 and .95. Those ranges are both ranges of 5%, so there are an equal amount of \\(t\\) values in them by definition.\nHere’s another way to do the same simulation in R, using the replicate function, instead a for loop:\n\n\nCode\nsimulated_ts &lt;- replicate(1000,\n                          t.test(rnorm(30, 50, 25), mu = 50)$statistic)\nhist(simulated_ts)\n\n\n\n\n\n\n\n\nFigure 13: Simulating \\(t\\)s in R.\n\n\n\n\n\n\n\nCode\nsimulated_ps &lt;- replicate(1000,\n                          t.test(rnorm(30, 50, 25), mu = 50)$p.value)\nhist(simulated_ps)\n\n\n\n\n\n\n\n\nFigure 14: Simulating \\(p\\)s in R.\n\n\n\n\n\n\n\n\nThe code below is set up to sample 10 scores for condition A and B from the same normal distribution. The simulation is conducted 1000 times, and the \\(t\\)s and \\(p\\)s are saved and plotted for each.\n\n\nCode\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(10,10,5)\n  condition_B &lt;- rnorm(10,10,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\nCode\nhist(save_ts)\n\n\n\n\n\n\n\n\nFigure 15: 1000 simulated ts from the null distribution\n\n\n\n\n\n\n\nCode\nhist(save_ps)\n\n\n\n\n\n\n\n\nFigure 16: 1000 simulated ps from the null distribution\n\n\n\n\n\nAccording to the simulation. When there are no differences between the conditions, and the samples are being pulled from the very same distribution, you get these two distributions for \\(t\\) and \\(p\\). These again show how the null distribution of no differences behaves.\nFor any of these simulations, if you rejected the null-hypothesis (that your difference was only due to chance), you would be making a type I error. If you set your alpha criteria to \\(\\alpha = .05\\), we can ask how many type I errors were made in these 1000 simulations. The answer is:\n\n\nCode\nlength(save_ps[save_ps&lt;.05])\n\n\n[1] 49\n\n\nCode\nlength(save_ps[save_ps&lt;.05])/1000\n\n\n[1] 0.049\n\n\nWe happened to make 49. The expectation over the long run is 5% type I error rates (if your alpha is .05).\nWhat happens if there actually is a difference in the simulated data, let’s set one condition to have a larger mean than the other:\n\n\nCode\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(10,10,5)\n  condition_B &lt;- rnorm(10,13,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\nCode\nhist(save_ts)\n\n\n\n\n\n\n\n\nFigure 17: 1000 ts when there is a true difference\n\n\n\n\n\n\n\nCode\nhist(save_ps)\n\n\n\n\n\n\n\n\nFigure 18: 1000 ps when there is a true difference\n\n\n\n\n\nNow you can see that the \\(p\\)-value distribution is skewed to the left. This is because when there is a true effect, you will get p-values that are less than .05 more often. Or, rather, you get larger \\(t\\) values than you normally would if there were no differences.\nIn this case, we wouldn’t be making a type I error if we rejected the null when p was smaller than .05. How many times would we do that out of our 1000 experiments?\n\n\nCode\nlength(save_ps[save_ps&lt;.05])\n\n\n[1] 251\n\n\nCode\nlength(save_ps[save_ps&lt;.05])/1000\n\n\n[1] 0.251\n\n\nWe happened to get 251 simulations where p was less than .05, that’s only 0.251 experiments. If you were the researcher, would you want to run an experiment that would be successful only 0.251 of the time? I wouldn’t. I would run a better experiment.\nHow would you run a better simulated experiment? Well, you could increase \\(n\\), the number of subjects in the experiment. Let’s increase \\(n\\) from 10 to 100, and see what happens to the number of “significant” simulated experiments.\n\n\nCode\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(100,10,5)\n  condition_B &lt;- rnorm(100,13,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\nCode\nhist(save_ts)\n\n\n\n\n\n\n\n\nFigure 19: 1000 ts for n =100, when there is a true effect\n\n\n\n\n\n\n\nCode\nhist(save_ps)\n\n\nlength(save_ps[save_ps&lt;.05])\n\n\n[1] 993\n\n\nCode\nlength(save_ps[save_ps&lt;.05])/1000\n\n\n[1] 0.993\n\n\n\n\n\n\n\n\nFigure 20: 1000 ps for n =100, when there is a true effect\n\n\n\n\n\nCool, now almost all of the experiments show a \\(p\\)-value of less than .05 (using a two-tailed test, that’s the default in R). See, you could use this simulation process to determine how many subjects you need to reliably find your effect.\n\n\n\nJust change the t.test function like so… this is for the null, assuming no difference between groups.\n\n\nCode\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  group_A &lt;- rnorm(10,10,5)\n  group_B &lt;- rnorm(10,10,5)\n  t_test &lt;- t.test(group_A, group_B, paired=FALSE, var.equal=TRUE)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\nCode\nhist(save_ts)\n\n\n\n\n\n\n\n\nFigure 21: 1000 ts for n =100, when there is a true effect\n\n\n\n\n\n\n\nCode\nhist(save_ps)\n\n\nlength(save_ps[save_ps&lt;.05])\n\n\n[1] 35\n\n\nCode\nlength(save_ps[save_ps&lt;.05])/1000\n\n\n[1] 0.035\n\n\n\n\n\n\n\n\nFigure 22: 1000 ps for n =100, when there is a true effect\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGordon Original ## 1. Introduction\nThe t-test is a fundamental statistical method used in psychology to compare the means of two groups. This chapter will explore the theory behind t-tests, demonstrate practical applications using R, and provide a structured write-up in APA style. Whether you’re examining the effectiveness of a new therapy or comparing cognitive performance between groups, the t-test is an essential tool for psychological research.\n\n\n\n\n\n\nA t-test evaluates whether the means of two groups are statistically different from each other. The most common types are the independent samples t-test, paired samples t-test, and one-sample t-test. Each type tests different hypotheses and has unique assumptions.\n\n\nThis test compares the means of two independent groups to ascertain if there is statistical evidence that the associated population means are significantly different.\n\n\n\nThis test compares means from the same group at different times (e.g., before and after a treatment) or matched pairs.\n\n\n\nThis test determines whether the mean of a single group is different from a known mean.\n\n\n\n\n\nIndependence: Observations within each group must be independent.\nNormality: Data in each group should be approximately normally distributed.\nHomogeneity of Variance: Variances in the two groups should be roughly equal (for independent samples t-test).\n\n\n\n\n\nTo conduct a t-test, you need to: 1. Formulate the null (H0) and alternative (H1) hypotheses. 2. Collect and organize data. 3. Check assumptions. 4. Calculate the t-statistic and degrees of freedom. 5. Determine the p-value. 6. Make a decision to reject or fail to reject H0 based on the p-value.\n\n\nResearch Question: Does a new cognitive-behavioral therapy (CBT) improve anxiety levels more than a placebo?\nHypotheses: - H0: There is no difference in anxiety levels between the CBT and placebo groups. - H1: The CBT group has lower anxiety levels than the placebo group.\nMeasures: Anxiety levels are measured using a standardized anxiety inventory.\n\n\n\n\nLet’s use R for an independent samples t-test.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe t.test function provides the t-statistic, degrees of freedom, and p-value. If the p-value is less than the significance level (e.g., 0.05), we reject H0.\n    Welch Two Sample t-test\n\ndata:  anxiety by group\nt = -8.6943, df = 17.564, p-value = 8.922e-08\nalternative hypothesis: true difference in means between group CBT and group Placebo is not equal to 0\n95 percent confidence interval:\n -9.936595 -6.063405\nsample estimates:\n    mean in group CBT mean in group Placebo \n                 21.7                  29.7\n\n\n\n\n\n\nViolating assumptions (e.g., normality, homogeneity of variance) can lead to incorrect conclusions. Robust alternatives or data transformations may be necessary (Wilcox, 2012).\n\n\n\nOverreliance on p-values can lead to misinterpretation of results. It’s crucial to report effect sizes and confidence intervals (Cumming, 2014).\n\n\n\nConducting multiple t-tests increases the risk of Type I errors. Methods like Bonferroni correction adjust for this but can be overly conservative (Perneger, 1998).\n\n\n\n\nT-tests are a vital statistical method in psychological research for comparing means between groups. Understanding their theory, assumptions, and correct application in R can significantly enhance the rigor of your analyses. Always ensure to check assumptions, consider effect sizes, and be mindful of the limitations and debates surrounding t-tests.\n\n\nCumming, G. (2014). The new statistics: Why and how. Psychological Science, 25(1), 7-29.\nPerneger, T. V. (1998). What’s wrong with Bonferroni adjustments. BMJ, 316(7139), 1236-1238.\nWilcox, R. R. (2012). Modern statistics for the social and behavioral sciences: A practical introduction. CRC Press.\nThis chapter provides a comprehensive overview of conducting t-tests in R, essential for first-year psychology students. Future chapters will delve into more advanced statistical methods to further enhance your analytical skills.",
    "crumbs": [
      "Week01",
      "T-Tests"
    ]
  },
  {
    "objectID": "week01/chapter.html#check-your-confidence-in-your-mean",
    "href": "week01/chapter.html#check-your-confidence-in-your-mean",
    "title": "T-Tests",
    "section": "",
    "text": "We’ve talked about getting a sample of data. We know we can find the mean, we know we can find the standard deviation. We know we can look at the data in a histogram. These are all useful things to do for us to learn something about the properties of our data.\nYou might be thinking of the mean and standard deviation as very different things that we would not put together. The mean is about central tendency (where most of the data is), and the standard deviation is about variance (where most of the data isn’t). Yes, they are different things, but we can use them together to create useful new things.\nWhat if I told you my sample mean was 50, and I told you nothing else about my sample. Would you be confident that most of the numbers were near 50? Would you wonder if there was a lot of variability in the sample, and many of the numbers were very different from 50. You should wonder all of those things. The mean alone, just by itself, doesn’t tell you anything about well the mean represents all of the numbers in the sample.\nIt could be a representative number, when the standard deviation is very small, and all the numbers are close to 50. It could be a non-representative number, when the standard deviation is large, and many of the numbers are not near 50. You need to know the standard deviation in order to be confident in how well the mean represents the data.\nHow can we put the mean and the standard deviation together, to give us a new number that tells us about confidence in the mean?\nWe can do this using a ratio:\n\\(\\frac{mean}{\\text{standard deviation}}\\)\nThink about what happens here. We are dividing a number by a number. Look at what happens:\n\\(\\frac{number}{\\text{same number}} = 1\\)\n\\(\\frac{number}{\\text{smaller number}} = \\text{big number}\\)\ncompared to:\n\\(\\frac{number}{\\text{bigger number}} = \\text{smaller number}\\)\nImagine we have a mean of 50, and a truly small standard deviation of 1. What do we get with our formula?\n\\(\\frac{50}{1} = 50\\)\nImagine we have a mean of 50, and a big standard deviation of 100. What do we get with our formula?\n\\(\\frac{50}{100} = 0.5\\)\nNotice, when we have a mean paired with a small standard deviation, our formula gives us a big number, like 50. When we have a mean paired with a large standard deviation, our formula gives us a small number, like 0.5. These numbers can tell us something about confidence in our mean, in a general way. We can be 50 confident in our mean in the first case, and only 0.5 (not at a lot) confident in the second case.\nWhat did we do here? We created a descriptive statistic by dividing the mean by the standard deviation. And, we have a sense of how to interpret this number, when it’s big we’re more confident that the mean represents all of the numbers, when it’s small we are less confident. This is a useful kind of number, a ratio between what we think about our sample (the mean), and the variability in our sample (the standard deviation). Get used to this idea. Almost everything that follows in this textbook is based on this kind of ratio. We will see that our ratio turns into different kinds of “statistics”, and the ratios will look like this in general:\n\\(\\text{name of statistic} = \\frac{\\text{measure of what we know}}{\\text{measure of what we don't know}}\\)\nor, to say it using different words:\n\\(\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\nIn fact, this is the general formula for the t-test. Big surprise!",
    "crumbs": [
      "Week01",
      "T-Tests"
    ]
  },
  {
    "objectID": "week01/chapter.html#one-sample-t-test-a-new-t-test",
    "href": "week01/chapter.html#one-sample-t-test-a-new-t-test",
    "title": "T-Tests",
    "section": "",
    "text": "Now we are ready to talk about t-test. We will talk about three of them. We start with the one-sample t-test.\nCommonly, the one-sample t-test is used to estimate the chances that your sample came from a particular population. Specifically, you might want to know whether the mean that you found from your sample, could have come from a particular population having a particular mean.\nStraight away, the one-sample t-test becomes a little confusing (and I haven’t even described it yet). Officially, it uses known parameters from the population, like the mean of the population and the standard deviation of the population. However, most times you don’t know those parameters of the population! So, you have to estimate them from your sample. Remember from the chapters on descriptive statistics and sampling, our sample mean is an unbiased estimate of the population mean. And, our sample standard deviation (the one where we divide by n-1) is an unbiased estimate of the population standard deviation. When Gosset developed the t-test, he recognized that he could use these estimates from his samples, to make the t-test. Here is the formula for the one sample t-test, we first use words, and then become more specific:\n\n\n\\(\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\n\\(\\text{t} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\n\\(\\text{t} = \\frac{\\text{Mean difference}}{\\text{standard error}}\\)\n\\(\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}}\\)\n\\(\\text{t} = \\frac{\\text{Sample Mean  - Population Mean}}{\\text{Sample Standard Error}}\\)\n\\(\\text{Estimated Standard Error} = \\text{Standard Error of Sample} = \\frac{s}{\\sqrt{N}}\\)\nWhere, s is the sample standard deviation.\nSome of you may have gone cross-eyed looking at all of this. Remember, we’ve seen it before when we divided our mean by the standard deviation in the first bit. The t-test is just a measure of a sample mean, divided by the standard error of the sample mean. That is it.\n\n\n\n\\(t\\) gives us a measure of confidence, just like our previous ratio for dividing the mean by a standard deviations. The only difference with \\(t\\), is that we divide by the standard error of mean (remember, this is also a standard deviation, it is the standard deviation of the sampling distribution of the mean)\n\n\n\n\n\n\nNote\n\n\n\nWhat does the t in t-test stand for? Apparently nothing. Gosset originally labelled it z. And, Fisher later called it t, perhaps because t comes after s, which is often used for the sample standard deviation.\n\n\n\\(t\\) is a property of the data that you collect. You compute it with a sample mean, and a sample standard error (there’s one more thing in the one-sample formula, the population mean, which we get to in a moment). This is why we call \\(t\\), a sample-statistic. It’s a statistic we compute from the sample.\nWhat kinds of numbers should we expect to find for these \\(ts\\)? How could we figure that out?\nLet’s start small and work through some examples. Imagine your sample mean is 5. You want to know if it came from a population that also has a mean of 5. In this case, what would \\(t\\) be? It would be zero: we first subtract the sample mean from the population mean, \\(5-5=0\\). Because the numerator is 0, \\(t\\) will be zero. So, \\(t\\) = 0, occurs, when there is no difference.\nLet’s say you take another sample, do you think the mean will be 5 every time, probably not. Let’s say the mean is 6. So, what can \\(t\\) be here? It will be a positive number, because 6-5= +1. But, will \\(t\\) be +1? That depends on the standard error of the sample. If the standard error of the sample is 1, then \\(t\\) could be 1, because 1/1 = 1.\nIf the sample standard error is smaller than 1, what happens to \\(t\\)? It get’s bigger right? For example, 1 divided by 0.5 = 2. If the sample standard error was 0.5, \\(t\\) would be 2. And, what could we do with this information? Well, it be like a measure of confidence. As \\(t\\) get’s bigger we could be more confident in the mean difference we are measuring.\nCan \\(t\\) be smaller than 1? Sure, it can. If the sample standard error is big, say like 2, then \\(t\\) will be smaller than one (in our case), e.g., 1/2 = .5. The direction of the difference between the sample mean and population mean, can also make the \\(t\\) become negative. What if our sample mean was 4. Well, then \\(t\\) will be negative, because the mean difference in the numerator will be negative, and the number in the bottom (denominator) will always be positive (remember why, it’s the standard error, computed from the sample standard deviation, which is always positive because of the squaring that we did.).\nSo, that is some intuitions about what the kinds of values t can take. \\(t\\) can be positive or negative, and big or small.\nLet’s do one more thing to build our intuitions about what \\(t\\) can look like. How about we sample some numbers and then measure the sample mean and the standard error of the mean, and then plot those two things against each each. This will show us how a sample mean typically varies with respect to the standard error of the mean.\nIn Figure 1, I pulled 1,000 samples of \\(N = 10\\) from a normal distribution (mean = 0, sd = 1). Each time I measured the mean and standard error of the sample. That gave two descriptive statistics for each sample, letting us plot each sample as dot in a scatter plot.\n\n\nCode\nlibrary(ggplot2)\n\n\n\n\nCode\nsample_mean &lt;- length(1000)\nsample_se &lt;- length(1000)\n\nfor (i in 1:1000) {\n  s &lt;- rnorm(10, 0, 1)\n  sample_mean[i] &lt;- mean(s)\n  sample_se[i] &lt;- sd(s) / sqrt(length(s))\n}\n\nplot(sample_mean, sample_se)\n\n\n\n\n\n\n\n\nFigure 1: A scatter plot with sample mean on the x-axis, and standard error of the mean on the y-axis\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nTake penguins, and then,\nadd new columns for the bill ratio and bill area.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n1penguins |&gt;\n2  mutate(\n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm\n  )\n\n\n\n1\n\nTake penguins, and then,\n\n2\n\nadd new columns for the bill ratio and bill area.\n\n\n\n\n# A tibble: 344 × 10\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 4 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_ratio &lt;dbl&gt;, bill_area &lt;dbl&gt;\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins |&gt;\n  mutate(\n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm\n  )         \n\n\n# A tibble: 344 × 10\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 4 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_ratio &lt;dbl&gt;, bill_area &lt;dbl&gt;\n\n\n\nWhat we get is a cloud of dots. You might notice the cloud has a circular quality. There’s more dots in the middle, and fewer dots as they radiate out from the middle. The dot cloud shows us the general range of the sample mean, for example most of the dots are in between -1 and 1. Similarly, the range for the sample standard error is roughly between .2 and .5. Remember, each dot represents one sample.\nWe can look at the same data a different way. For example, rather than using a scatter plot, we can divide the mean for each dot by the standard error for each dot. Figure 2 shows the result in a histogram.\n\n\nCode\nhist(sample_mean/sample_se, breaks=30)\n\n\n\n\n\n\n\n\nFigure 2: A histogram of the sample means divided by the sample standard errors, this is a t-distribution.\n\n\n\n\n\nInteresting, we can see the histogram is shaped like a normal curve. It is centered on 0, which is the most common value. As values become more extreme, they become less common. If you remember, our formula for \\(t\\), was the mean divided by the standard error of the mean. That’s what we did here. This histogram is showing you a \\(t\\)-distribution.\n\n\n\nLet’s briefly calculate a t-value from a small sample. Let’s say we had 10 students do a true/false quiz with 5 questions on it. There’s a 50% chance of getting each answer correct.\nEvery student completes the 5 questions, we grade them, and then we find their performance (mean percent correct). What we want to know is whether the students were guessing. If they were all guessing, then the sample mean should be about 50%, it shouldn’t be different from chance, which is 50%. Let’s look at Table 1.\n\n\n\n\nTable 1: Calculating the t-value for a one-sample test.\n\n\n\n\n\n\nstudents\nscores\nmean\nDifference_from_Mean\nSquared_Deviations\n\n\n\n\n1\n50\n61\n-11\n121\n\n\n2\n70\n61\n9\n81\n\n\n3\n60\n61\n-1\n1\n\n\n4\n40\n61\n-21\n441\n\n\n5\n80\n61\n19\n361\n\n\n6\n30\n61\n-31\n961\n\n\n7\n90\n61\n29\n841\n\n\n8\n60\n61\n-1\n1\n\n\n9\n70\n61\n9\n81\n\n\n10\n60\n61\n-1\n1\n\n\nSums\n610\n610\n0\n2890\n\n\nMeans\n61\n61\n0\n289\n\n\n\n\n\nsd\n17.92\n\n\n\n\n\nSEM\n5.67\n\n\n\n\n\nt\n1.94003527336861\n\n\n\n\n\n\n\n\nYou can see the scores column has all of the test scores for each of the 10 students. We did the things we need to do to compute the standard deviation.\nRemember the sample standard deviation is the square root of the sample variance, or:\n\\(\\text{sample standard deviation} = \\sqrt{\\frac{\\sum_{i}^{n}({x_{i}-\\bar{x})^2}}{N-1}}\\)\n\\(\\text{sd} = \\sqrt{\\frac{2890}{10-1}} = 17.92\\)\nThe standard error of the mean, is the standard deviation divided by the square root of N\n\\(\\text{SEM} = \\frac{s}{\\sqrt{N}} = \\frac{17.92}{10} = 5.67\\)\n\\(t\\) is the difference between our sample mean (61), and our population mean (50, assuming chance), divided by the standard error of the mean.\n\\(\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}} = \\frac{\\bar{X}-u}{SEM} = \\frac{61-50}{5.67} = 1.94\\)\nAnd, that is you how calculate \\(t\\), by hand. It’s a pain. I was annoyed doing it this way. In the lab, you learn how to calculate \\(t\\) using software, so it will just spit out \\(t\\). For example in R, all you have to do is this:\n\n\nCode\nt.test(scores, mu=50)\n\n\n\n    One Sample t-test\n\ndata:  scores\nt = 1.9412, df = 9, p-value = 0.08415\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 48.18111 73.81889\nsample estimates:\nmean of x \n       61 \n\n\n\n\n\nIf \\(t\\) is just a number that we can compute from our sample (it is), what can we do with it? How can we use \\(t\\) for statistical inference?\nRemember back to the chapter on sampling and distributions, that’s where we discussed the sampling distribution of the sample mean. Remember, we made a lot of samples, then computed the mean for each sample, then we plotted a histogram of the sample means. Later, in that same section, we mentioned that we could generate sampling distributions for any statistic. For each sample, we could compute the mean, the standard deviation, the standard error, and now even \\(t\\), if we wanted to. We could generate 10,000 samples, and draw four histograms, one for each sampling distribution for each statistic.\nThis is exactly what I did, and the results are shown in the four panels of Figure 3 below. I used a sample size of 20, and drew random observations for each sample from a normal distribution, with mean = 0, and standard deviation = 1. Let’s look at the sampling distributions for each of the statistics. \\(t\\) was computed assuming with the population mean assumed to be 0.\n\n\nCode\nall_df &lt;- data.frame()\nfor (i in 1:10000) {\n  sample &lt;- rnorm(20, 0, 1)\n  sample_mean &lt;- mean(sample)\n  sample_sd &lt;- sd(sample)\n  sample_se &lt;- sd(sample) / sqrt(length(sample))\n  sample_t &lt;- as.numeric(t.test(sample, mu = 0)$statistic)\n  t_df &lt;- data.frame(i, sample_mean, sample_sd, sample_se, sample_t)\n  all_df &lt;- rbind(all_df, t_df)\n}\n\nlibrary(ggpubr)\na &lt;- ggplot(all_df, aes(x = sample_mean)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nb &lt;- ggplot(all_df, aes(x = sample_sd)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nc &lt;- ggplot(all_df, aes(x = sample_se)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nd &lt;- ggplot(all_df, aes(x = sample_t)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\n\nggarrange(a, b, c, d,\n          ncol = 2, nrow = 2)\n\n\n\n\n\n\n\n\nFigure 3: Sampling distributions for the mean, standard deviation, standard error of the mean, and \\(t\\).\n\n\n\n\n\nWe see four sampling distributions. This is how statistical summaries of these summaries behave. We have used the word chance windows before. These are four chance windows, measuring different aspects of the sample. In this case, all of the samples came from the same normal distribution. Because of sampling error, each sample is not identical. The means are not identical, the standard deviations are not identical, sample standard error of the means are not identical, and the \\(t\\)s of the samples are not identical. They all have some variation, as shown by the histograms. This is how samples of size 20 behave.\nWe can see straight away, that in this case, we are unlikely to get a sample mean of 2. That’s way outside the window. The range for the sampling distribution of the mean is around -.5 to +.5, and is centered on 0 (the population mean, would you believe!).\nWe are unlikely to get sample standard deviations of between .6 and 1.5, that is a different range, specific to the sample standard deviation.\nSame thing with the sample standard error of the mean, the range here is even smaller, mostly between .1, and .3. You would rarely find a sample with a standard error of the mean greater than .3. Virtually never would you find one of say 1 (for this situation).\nNow, look at \\(t\\). It’s range is basically between -3 and +3 here. 3s barely happen at all. You pretty much never see a 5 or -5 in this situation.\nAll of these sampling windows are chance windows, and they can all be used in the same way as we have used similar sampling distributions before (e.g., Crump Test, and Randomization Test) for statistical inference. For all of them we would follow the same process:\n\nGenerate these distributions\nLook at your sample statistics for the data you have (mean, SD, SEM, and \\(t\\))\nFind the likelihood of obtaining that value or greater\nObtain that probability\nSee if you think your sample statistics were probable or improbable.\n\nWe’ll formalize this in a second. I just want you to know that what you will be doing is something that you have already done before. For example, in the Crump test and the Randomization test we focused on the distribution of mean differences. We could do that again here, but instead, we will focus on the distribution of \\(t\\) values. We then apply the same kinds of decision rules to the \\(t\\) distribution, as we did for the other distributions. Below you will see a graph you have already seen, except this time it is a distribution of \\(t\\)s, not mean differences:\nRemember, if we obtained a single \\(t\\) from one sample we collected, we could consult the chance window in Figure 4 below to find out whether the \\(t\\) we obtained from the sample was likely or unlikely to occur by chance.\n\n\nCode\nsample_t &lt;- all_df$sample_t\n\nggplot(all_df, aes(x = sample_t)) +\n  annotate(\n    \"rect\",\n    xmin = min(sample_t),\n    xmax = max(sample_t),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"red\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = min(sample_t),\n    xmax = -1.94,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = 1.94,\n    xmax = max(sample_t),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  geom_rect(aes(\n    xmin = -Inf,\n    xmax = min(sample_t),\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_rect(aes(\n    xmin = max(sample_t),\n    xmax = Inf,\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_histogram(bins = 50, color = \"white\") +\n  theme_classic() +\n  geom_vline(xintercept = min(sample_t)) +\n  geom_vline(xintercept = max(sample_t)) +\n  geom_vline(xintercept = -1.94) +\n  geom_vline(xintercept = 1.94) +\n  xlim(-8, 8) +\n  geom_label(data = data.frame(x = 0, y = 250, label = \"CHANCE\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -7, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 7, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  #  geom_label(data = data.frame(x = min(sample_t), y = 600,\n  #                              label = paste0(\"min \\n\",round(min(sample_t)))),\n  #                             aes(x = x, y = y, label = label))+\n  #geom_label(data = data.frame(x = max(sample_t), y = 600,\n  #                            label = paste0(\"max \\n\",round(max(sample_t)))),\n  #                           aes(x = x, y = y, label = label))+\n  geom_label(data = data.frame(x = -4, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 4, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  xlab(\"mean sample_t\")\n\n\n\n\n\n\n\n\nFigure 4: Applying decision criteria to the \\(t\\)-distribution. Histogram of \\(t\\)s from samples (n=20) drawn from the same normal distribution (u=0, sd=1)\n\n\n\n\n\n\n\n\nFrom our early example involving the TRUE/FALSE quizzes, we are now ready to make some kind of decision about what happened there. We found a mean difference of 11. We found a \\(t\\) = 1.9411765. The probability of this \\(t\\) or larger occurring is \\(p\\) = 0.0841503. We were testing the idea that our sample mean of 61 could have come from a normal distribution with mean = 50. The \\(t\\) test tells us that the \\(t\\) for our sample, or a larger one, would happen with p = 0.0841503. In other words, chance can do it a kind of small amount of time, but not often. In English, this means that all of the students could have been guessing, but it wasn’t that likely that were just guessing.\nThe next \\(t\\)-test is called a paired samples t-test. And, spoiler alert, we will find out that a paired samples t-test is actually a one-sample t-test in disguise (WHAT!), yes it is. If the one-sample \\(t\\)-test didn’t make sense to you, read the next section.",
    "crumbs": [
      "Week01",
      "T-Tests"
    ]
  },
  {
    "objectID": "week01/chapter.html#paired-samples-t-test",
    "href": "week01/chapter.html#paired-samples-t-test",
    "title": "T-Tests",
    "section": "",
    "text": "For me (Crump), many analyses often boil down to a paired samples t-test. It just happens that many things I do reduce down to a test like this.\nI am a cognitive psychologist, I conduct research about how people do things like remember, pay attention, and learn skills. There are lots of Psychologists like me, who do very similar things.\nWe all often conduct the same kinds of experiments. They go like this, and they are called repeated measures designs. They are called repeated measures designs, because we measure how one person does something more than once, we repeat the measure.\nSo, I might measure somebody doing something in condition A, and measure the same person doing something in Condition B, and then I see that same person does different things in the two conditions. I repeatedly measure the same person in both conditions. I am interested in whether the experimental manipulation changes something about how people perform the task in question.\n\n\nWe will introduce the paired-samples t-test with an example using real data, from a real study. @mehr20165 were interested in whether singing songs to infants helps infants become more sensitive to social cues. For example, infants might need to learn to direct their attention toward people as a part of learning how to interact socially with people. Perhaps singing songs to infants aids this process of directing attention. When an infant hears a familiar song, they might start to pay more attention to the person singing that song, even after they are done singing the song. The person who sang the song might become more socially important to the infant. You will learn more about this study in the lab for this week. This example, prepares you for the lab activities. Here is a brief summary of what they did.\nFirst, parents were trained to sing a song to their infants. After many days of singing this song to the infants, a parent came into the lab with their infant. In the first session, parents sat with their infants on their knees, so the infant could watch two video presentations. There were two videos. Each video involved two unfamiliar new people the infant had never seen before. Each new person in the video (the singers) sang one song to the infant. One singer sang the “familiar” song the infant had learned from their parents. The other singer sang an “unfamiliar” song the infant had not hear before.\nThere were two really important measurement phases: the baseline phase, and the test phase.\nThe baseline phase occurred before the infants saw and heard each singer sing a song. During the baseline phase, the infants watched a video of both singers at the same time. The researchers recorded the proportion of time that the infant looked at each singer. The baseline phase was conducted to determine whether infants had a preference to look at either person (who would later sing them a song).\nThe test phase occurred after infants saw and heard each song, sung by each singer. During the test phase, each infant had an opportunity to watch silent videos of both singers. The researchers measured the proportion of time the infants spent looking at each person. The question of interest, was whether the infants would spend a greater proportion of time looking at the singer who sang the familiar song, compared to the singer who sang the unfamiliar song.\nThere is more than one way to describe the design of this study. We will describe it like this. It was a repeated measures design, with one independent (manipulation) variable called Viewing phase: Baseline versus Test. There was one dependent variable (the measurement), which was proportion looking time (to singer who sung familiar song). This was a repeated measures design because the researchers measured proportion looking time twice (they repeated the measure), once during baseline (before infants heard each singer sing a song), and again during test (after infants head each singer sing a song).\nThe important question was whether infants would change their looking time, and look more at the singer who sang the familiar song during the test phase, than they did during the baseline phase. This is a question about a change within individual infants. In general, the possible outcomes for the study are:\n\nNo change: The difference between looking time toward the singer of the familiar song during baseline and test is zero, no difference.\nPositive change: Infants will look longer toward the singer of the familiar song during the test phase (after they saw and heard the singers), compared to the baseline phase (before they saw and heard the singers). This is a positive difference if we use the formula: Test Phase Looking time - Baseline phase looking time (to familiar song singer).\nNegative change: Infants will look longer toward the singer of the unfamiliar song during the test phase (after they saw and heard the singers), compared to the baseline phase (before they saw and heard the singers). This is a negative difference if we use the same formula: Test Phase Looking time - Baseline phase looking time (to familiar song singer).\n\n\n\n\nLet’s take a look at the data for the first 5 infants in the study. This will help us better understand some properties of the data before we analyze it. We will see that the data is structured in a particular way that we can take advantage of with a paired samples t-test. Note, we look at the first 5 infants to show how the computations work. The results of the paired-samples t-test change when we use all of the data from the study.\nHere is a table of the data:\n\n\n\n\n\ninfant\nBaseline\nTest\n\n\n\n\n1\n0.44\n0.60\n\n\n2\n0.41\n0.68\n\n\n3\n0.75\n0.72\n\n\n4\n0.44\n0.28\n\n\n5\n0.47\n0.50\n\n\n\n\n\nThe table shows proportion looking times toward the singer of the familiar song during the Baseline and Test phases. Notice there are five different infants, (1 to 5). Each infant is measured twice, once during the Baseline phase, and once during the Test phase. To repeat from before, this is a repeated-measures design, because the infants are measured repeatedly (twice in this case). Or, this kind of design is also called a paired-samples design. Why? because each participant comes with a pair of samples (two samples), one for each level of the design.\nGreat, so what are we really interested in here? We want to know if the mean looking time toward the singer of the familiar song for the Test phase is higher than the Baseline phase. We are comparing the two sample means against each other and looking for a difference. We already know that differences could be obtained by chance alone, simply because we took two sets of samples, and we know that samples can be different. So, we are interested in knowing whether chance was likely or unlikely to have produced any difference we might observe.\nIn other words, we are interested in looking at the difference scores between the baseline and test phase for each infant. The question here is, for each infant, did their proportion looking time to the singer of the familiar song, increase during the test phase as compared to the baseline phase.\n\n\n\nLet’s add the difference scores to the table of data so it is easier to see what we are talking about. The first step in creating difference scores is to decide how you will take the difference, there are two options:\n\nTest phase score - Baseline Phase Score\nBaseline phase score - Test Phase score\n\nLet’s use the first formula. Why? Because it will give us positive differences when the test phase score is higher than the baseline phase score. This makes a positive score meaningful with respect to the study design, we know (because we defined it to be this way), that positive scores will refer to longer proportion looking times (to singer of familiar song) during the test phase compared to the baseline phase.\n\n\nCode\npaired_sample_df &lt;- cbind(paired_sample_df, \n                          differences = (paired_sample_df$Test-\n                                           paired_sample_df$Baseline))\nknitr::kable(paired_sample_df)\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\n\n\n\n\n1\n0.44\n0.60\n0.16\n\n\n2\n0.41\n0.68\n0.27\n\n\n3\n0.75\n0.72\n-0.03\n\n\n4\n0.44\n0.28\n-0.16\n\n\n5\n0.47\n0.50\n0.03\n\n\n\n\n\nThere we have it, the difference scores. The first thing we can do here is look at the difference scores, and ask how many infants showed the effect of interest. Specifically, how many infants showed a positive difference score. We can see that three of five infants showed a positive difference (they looked more at the singer of the familiar song during the test than baseline phase), and two the infants showed the opposite effect (negative difference, they looked more at the singer of the familiar song during baseline than test).\n\n\n\nAs we have been discussing, the effect of interest in this study is the mean difference between the baseline and test phase proportion looking times. We can calculate the mean difference, by finding the mean of the difference scores. Let’s do that, in fact, for fun let’s calculate the mean of the baseline scores, the test scores, and the difference scores.\n\n\nCode\npaired_sample_df &lt;- paired_sample_df %&gt;%\n   rbind(c(\"Sums\",colSums(paired_sample_df[1:5,2:4]))) %&gt;%\n   rbind(c(\"Means\",colMeans(paired_sample_df[1:5,2:4])))\n  \nknitr::kable(paired_sample_df)\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\n\n\n\n\n1\n0.44\n0.6\n0.16\n\n\n2\n0.41\n0.68\n0.27\n\n\n3\n0.75\n0.72\n-0.03\n\n\n4\n0.44\n0.28\n-0.16\n\n\n5\n0.47\n0.5\n0.03\n\n\nSums\n2.51\n2.78\n0.27\n\n\nMeans\n0.502\n0.556\n0.054\n\n\n\n\n\nWe can see there was a positive mean difference of 0.054, between the test and baseline phases.\nCan we rush to judgment and conclude that infants are more socially attracted to individuals who have sung them a familiar song? I would hope not based on this very small sample. First, the difference in proportion looking isn’t very large, and of course we recognize that this difference could have been produced by chance.\nWe will more formally evaluate whether this difference could have been caused by chance with the paired-samples t-test. But, before we do that, let’s again calculate \\(t\\) and discuss what \\(t\\) tells us over and above what our measure of the mean of the difference scores tells us.\n\n\n\nOK, so how do we calculate \\(t\\) for a paired-samples \\(t\\)-test? Surprise, we use the one-sample t-test formula that you already learned about! Specifically, we use the one-sample \\(t\\)-test formula on the difference scores. We have one sample of difference scores (you can see they are in one column), so we can use the one-sample \\(t\\)-test on the difference scores. Specifically, we are interested in comparing whether the mean of our difference scores came from a distribution with mean difference = 0. This is a special distribution we refer to as the null distribution. It is the distribution no differences. Of course, this null distribution can produce differences due to to sampling error, but those differences are not caused by any experimental manipulation, they caused by the random sampling process.\nWe calculate \\(t\\) in a moment. Let’s now consider again why we want to calculate \\(t\\)? Why don’t we just stick with the mean difference we already have?\nRemember, the whole concept behind \\(t\\), is that it gives an indication of how confident we should be in our mean. Remember, \\(t\\) involves a measure of the mean in the numerator, divided by a measure of variation (standard error of the sample mean) in the denominator. The resulting \\(t\\) value is small when the mean difference is small, or when the variation is large. So small \\(t\\)-values tell us that we shouldn’t be that confident in the estimate of our mean difference. Large \\(t\\)-values occur when the mean difference is large and/or when the measure of variation is small. So, large \\(t\\)-values tell us that we can be more confident in the estimate of our mean difference. Let’s find \\(t\\) for the mean difference scores. We use the same formulas as we did last time:\n\n\n\n\n\n\n\n\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\ndiff_from_mean\nSquared_differences\n\n\n\n\n1\n0.44\n0.6\n0.16\n0.106\n0.011236\n\n\n2\n0.41\n0.68\n0.27\n0.216\n0.046656\n\n\n3\n0.75\n0.72\n-0.03\n-0.084\n0.00705600000000001\n\n\n4\n0.44\n0.28\n-0.16\n-0.214\n0.045796\n\n\n5\n0.47\n0.5\n0.03\n-0.024\n0.000575999999999999\n\n\nSums\n2.51\n2.78\n0.27\n0\n0.11132\n\n\nMeans\n0.502\n0.556\n0.054\n0\n0.022264\n\n\n\n\n\n\nsd\n0.167\n\n\n\n\n\n\nSEM\n0.075\n\n\n\n\n\n\nt\n0.72\n\n\n\n\n\nIf we did this test using R, we would obtain almost the same numbers (there is a little bit of rounding in the table).\n\n\nCode\nt.test(differences,mu=0)\n\n\n\n    One Sample t-test\n\ndata:  differences\nt = 0.72381, df = 4, p-value = 0.5092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.1531384  0.2611384\nsample estimates:\nmean of x \n    0.054 \n\n\nHere is a quick write up of our t-test results, t(4) = .72, p = .509.\nWhat does all of that tell us? There’s a few things we haven’t gotten into much yet. For example, the 4 represents degrees of freedom, which we discuss later. The important part, the \\(t\\) value should start to be a little bit more meaningful. We got a kind of small t-value didn’t we. It’s .72. What can we tell from this value? First, it is positive, so we know the mean difference is positive. The sign of the \\(t\\)-value is always the same as the sign of the mean difference (ours was +0.054). We can also see that the p-value was .509. We’ve seen p-values before. This tells us that our \\(t\\) value or larger, occurs about 50.9% of the time… Actually it means more than this. And, to understand it, we need to talk about the concept of two-tailed and one-tailed tests.\n\n\n\nRemember what it is we are doing here. We are evaluating whether our sample data could have come from a particular kind of distribution. The null distribution of no differences. This is the distribution of \\(t\\)-values that would occur for samples of size 5, with a mean difference of 0, and a standard error of the sample mean of .075 (this is the SEM that we calculated from our sample). We can see what this particular null-distribution looks like in Figure 5.\n\n\nCode\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  scale_x_continuous(breaks = (seq(-3, 3, .5))) +\n  geom_label(data = data.frame(x = -.7, y = .1, label = \"50% \\n (-)\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .7, y = .1, label = \"50% \\n (+)\"), aes(x = x, y = y, label = label)) +\n  geom_vline(xintercept = 0)+\n  theme_classic(base_size = 20)\n\n\n\n\n\n\n\n\nFigure 5: A distribution of \\(t\\)-values that can occur by chance alone, when there is no difference between the sample and a population\n\n\n\n\n\nThe \\(t\\)-distribution above shows us the kinds of values \\(t\\) will will take by chance alone, when we measure the mean differences for pairs of 5 samples (like our current). \\(t\\) is most likely to be zero, which is good, because we are looking at the distribution of no-differences, which should most often be 0! But, sometimes, due to sampling error, we can get \\(t\\)s that are bigger than 0, either in the positive or negative direction. Notice the distribution is symmetrical, a \\(t\\) from the null-distribution will be positive half of the time, and negative half of the time, that is what we would expect by chance.\nSo, what kind of information do we want know when we find a particular \\(t\\) value from our sample? We want to know how likely the \\(t\\) value like the one we found occurs just by chance. This is actually a subtly nuanced kind of question. For example, any particular \\(t\\) value doesn’t have a specific probability of occurring. When we talk about probabilities, we are talking about ranges of probabilities. Let’s consider some probabilities. We will use the letter \\(p\\), to talk about the probabilities of particular \\(t\\) values.\n\nWhat is the probability that \\(t\\) is zero or positive or negative? The answer is p=1, or 100%. We will always have a \\(t\\) value that is zero or non-zero…Actually, if we can’t compute the t-value, for example when the standard deviation is undefined, I guess then we would have a non-number. But, assuming we can calculate \\(t\\), then it will always be 0 or positive or negative.\nWhat is the probability of \\(t\\) = 0 or greater than 0? The answer is p=.5, or 50%. 50% of \\(t\\)-values are 0 or greater.\nWhat is the of \\(t\\) = 0 or smaller than 0? The answer is p=.5, or 50%. 50% of \\(t\\)-values are 0 or smaller.\n\nWe can answer all of those questions just by looking at our t-distribution, and dividing it into two equal regions, the left side (containing 50% of the \\(t\\) values), and the right side containing 50% of the \\(t\\)-values).\nWhat if we wanted to take a more fine-grained approach, let’s say we were interested in regions of 10%. What kinds of \\(t\\)s occur 10% of the time. We would apply lines like the following. Notice, the likelihood of bigger numbers (positive or negative) gets smaller, so we have to increase the width of the bars for each of the intervals between the bars to contain 10% of the \\(t\\)-values, it looks like Figure 6.\n\n\nCode\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = t_ps) +\n  theme_classic(base_size = 15) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  scale_x_continuous(breaks = round(t_ps, digits = 1)) +\n  geom_label(data = data.frame(x = -2.5, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -1.3, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 1.3, y = .2, label = \"10%\"), aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\n\nFigure 6: Splitting the t distribution up into regions each containing 10% of the \\(t\\)-values. The width between the bars narrows as they approach the center of the distribution, where there are more \\(t\\)-values.\n\n\n\n\n\nConsider the probabilities (\\(p\\)) of \\(t\\) for the different ranges.\n\n\\(t\\) &lt;= -1.5 (\\(t\\) is less than or equal to -1.5), \\(p\\) = 10%\n-1.5 &gt;= \\(t\\) &lt;= -0.9 (\\(t\\) is equal to or between -1.5 and -.9), \\(p\\) = 10%\n-.9 &gt;= \\(t\\) &lt;= -0.6 (\\(t\\) is equal to or between -.9 and -.6), \\(p\\) = 10%\n\\(t\\) &gt;= 1.5 (\\(t\\) is greater than or equal to 1.5), \\(p\\) = 10%\n\nNotice, that the \\(p\\)s are always 10%. \\(t\\)s occur in these ranges with 10% probability.\n\n\n\nYou might be wondering where I am getting some of these values from. For example, how do I know that 10% of \\(t\\) values (for this null distribution) have a value of approximately 1.5 or greater than 1.5? The answer is I used R to tell me.\nIn most statistics textbooks the answer would be: there is a table at the back of the book where you can look these things up…This textbook has no such table. We could make one for you. And, we might do that. But, we didn’t do that yet…\nSo, where do these values come from, how can you figure out what they are? The complicated answer is that we are not going to explain the math behind finding these values because, 1) the authors (some of us) admittedly don’t know the math well enough to explain it, and 2) it would sidetrack us to much, 3) you will learn how to get these numbers in the lab with software, 4) you will learn how to get these numbers in lab without the math, just by doing a simulation, and 5) you can do it in R, or excel, or you can use an online calculator.\nThis is all to say that you can find the \\(t\\)s and their associated \\(p\\)s using software. But, the software won’t tell you what these values mean. That’s we are doing here. You will also see that software wants to know a few more things from you, such as the degrees of freedom for the test, and whether the test is one-tailed or two tailed. We haven’t explained any of these things yet. That’s what we are going to do now. Note, we explain degrees of freedom last. First, we start with a one-tailed test.\n\n\n\nA one-tailed test is sometimes also called a directional test. It is called a directional test, because a researcher might have a hypothesis in mind suggesting that the difference they observe in their means is going to have a particular direction, either a positive difference, or a negative difference.\nTypically, a researcher would set an alpha criterion. The alpha criterion describes a line in the sand for the researcher. Often, the alpha criterion is set at \\(p = .05\\). What does this mean? Figure 7 shows the \\(t\\)-distribution and the alpha criterion.\n\n\nCode\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = qt(.95, 4, lower.tail = TRUE)) +\n  ggtitle(\"Critical t for one-tailed test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = qt(.95, 4, lower.tail = TRUE),\n    xmax = 3,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.95, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\n\nFigure 7: The critical value of t for an alpha criterion of 0.05. 5% of all ts are at this value or larger\n\n\n\n\n\nThe figure shows that \\(t\\) values of +2.13 or greater occur 5% of the time. Because the t-distribution is symmetrical, we also know that \\(t\\) values of -2.13 or smaller also occur 5% of the time. Both of these properties are true under the null distribution of no differences. This means, that when there really are no differences, a researcher can expect to find \\(t\\) values of 2.13 or larger 5% of the time.\nLet’s review and connect some of the terms:\n\nalpha criterion: the criterion set by the researcher to make decisions about whether they believe chance did or did not cause the difference. The alpha criterion here is set to \\(p = .05\\).\nCritical \\(t\\). The critical \\(t\\) is the \\(t\\)-value associated with the alpha-criterion. In this case for a one-tailed test, it is the \\(t\\) value where 5% of all \\(t\\)s are this number or greater. In our example, the critical \\(t\\) is 2.13. 5% of all \\(t\\) values (with degrees of freedom = 4) are +2.13, or greater than +2.13.\nObserved \\(t\\). The observed \\(t\\) is the one that you calculated from your sample. In our example about the infants, the observed \\(t\\) was \\(t\\) (4) = 0.72.\np-value. The \\(p\\)-value is the probability of obtaining the observed \\(t\\) value or larger. Now, you could look back at our previous example, and find that the \\(p\\)-value for \\(t\\) (4) = .72, was \\(p = .509\\) . HOWEVER, this p-value was not calculated for a one-directional test…(we talk about what .509 means in the next section).\n\nFigure 8 shows what the \\(p\\)-value for \\(t\\) (4) = .72 using a one-directional test would would look like:\n\n\nCode\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = .72) +\n  geom_vline(xintercept = qt(.95, 4, lower.tail = TRUE)) +\n  ggtitle(\"t value and p-range for one-directional test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = .72,\n    xmax = 3,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = qt(.95, 4, lower.tail = TRUE),\n    xmax = 3,\n    ymin = 0,\n    ymax = .25,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.95, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .72, y = .1,\n                               label = \"Observed t\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .72, y = .05,\n                               label = \".72, p=\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = 1.5,\n    y = .05,\n    label = round(pt(.72, 4, lower.tail = FALSE), digits =\n                    3)\n  ),\n  aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\n\nFigure 8: A case where the observed value of t is much less than the critical value for a one-directional t-test.\n\n\n\n\n\nLet’s take this one step at a time. We have located the observed \\(t\\) of .72 on the graph. We shaded the right region all grey. What we see is that the grey region represents .256 or 25.6% of all \\(t\\) values. In other words, 25.6% of \\(t\\) values are .72 or larger than .72. You could expect, by chance alone, to a find a \\(t\\) value of .72 or larger, 25.6% of the time. That’s fairly often. We did find a \\(t\\) value of .72. Now that you know this kind of \\(t\\) value or larger occurs 25.6% of the time, would you be confident that the mean difference was not due to chance? Probably not, given that chance can produce this difference fairly often.\nFollowing the “standard” decision making procedure, we would claim that our \\(t\\) value was not statistically significant, because it was not large enough. If our observed value was larger than the critical \\(t\\) (larger than 2.13), defined by our alpha criterion, then we would claim that our \\(t\\) value was statistically significant. This would be equivalent to saying that we believe it is unlikely that the difference we observed was due to chance. In general, for any observed \\(t\\) value, the associated \\(p\\)-value tells you how likely a \\(t\\) of the observed size or larger would be observed. The \\(p\\)-value always refers to a range of \\(t\\)-values, never to a single \\(t\\)-value. Researchers use the alpha criterion of .05, as a matter of convenience and convention. There are other ways to interpret these values that do not rely on a strict (significant versus not) dichotomy.\n\n\n\nOK, so that was one-tailed tests… What are two tailed tests? The \\(p\\)-value that we originally calculated from our paired-samples \\(t\\)-test was for a 2-tailed test. Often, the default is that the \\(p\\)-value is for a two-tailed test.\nThe two-tailed test, is asking a more general question about whether a difference is likely to have been produced by chance. The question is: what is probability of any difference. It is also called a non-directional test, because here we don’t care about the direction or sign of the difference (positive or negative), we just care if there is any kind of difference.\nThe same basic things as before are involved. We define an alpha criterion (\\(\\alpha = 0.05\\)). And, we say that any observed \\(t\\) value that has a probability of \\(p\\) &lt;.05 (\\(p\\) is less than .05) will be called statistically significant, and ones that are more likely (\\(p\\) &gt;.05, \\(p\\) is greater than .05) will be called null-results, or not statistically significant. The only difference is how we draw the alpha range. Before it was on the right side of the \\(t\\) distribution (we were conducting a one-sided test remember, so we were only interested in one side).\nFigure 9 shows what the most extreme 5% of the \\(t\\)-values are when we ignore their sign (whether they are positive or negative).\n\n\nCode\nrange &lt;- seq(-4, 4, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"t-values\") +\n  ylab(\"Probability\") +\n  geom_vline(xintercept = qt(.975, 4, lower.tail = TRUE)) +\n  geom_vline(xintercept = qt(.025, 4, lower.tail = TRUE)) +\n  ggtitle(\"Critical ts for two-tailed test\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = qt(.975, 4, lower.tail = TRUE),\n    xmax = 4,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 3.5, y = .2, label = \"2.5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.975, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.975, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.975, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label)) +\n  annotate(\n    \"rect\",\n    xmin = -4,\n    xmax = qt(.025, 4, lower.tail = TRUE),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = -3.5, y = .2, label = \"2.5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.025, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"Critical t\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.025, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.025, 4, lower.tail = TRUE), digits =\n                    2)\n  ),\n  aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\n\nFigure 9: Critical values for a two-tailed test. Each line represents the location where 2.5% of all \\(t\\)s are larger or smaller than critical value. The total for both tails is 5%\n\n\n\n\n\nHere is what we are seeing. A distribution of no differences (the null, which is what we are looking at), will produce \\(t\\)s that are 2.78 or greater 2.5% of the time, and \\(t\\)s that are -2.78 or smaller 2.5% of the time. 2.5% + 2.5% is a total of 5% of the time. We could also say that \\(t\\)s larger than +/- 2.78 occur 5% of the time.\nAs a result, the critical \\(t\\) value is (+/-) 2.78 for a two-tailed test. As you can see, the two-tailed test is blind to the direction or sign of the difference. Because of this, the critical \\(t\\) value is also higher for a two-tailed test, than for the one-tailed test that we did earlier. Hopefully, now you can see why it is called a two-tailed test. There are two tails of the distribution, one on the left and right, both shaded in green.\n\n\n\nNow that you know there are two kinds of tests, one-tailed, and two-tailed, which one should you use? There is some conventional wisdom on this, but also some debate. In the end, it is up to you to be able to justify your choice and why it is appropriate for you data. That is the real answer.\nThe conventional answer is that you use a one-tailed test when you have a theory or hypothesis that is making a directional prediction (the theory predicts that the difference will be positive, or negative). Similarly, use a two-tailed test when you are looking for any difference, and you don’t have a theory that makes a directional prediction (it just makes the prediction that there will be a difference, either positive or negative).\nAlso, people appear to choose one or two-tailed tests based on how risky they are as researchers. If you always ran one-tailed tests, your critical \\(t\\) values for your set alpha criterion would always be smaller than the critical \\(t\\)s for a two-tailed test. Over the long run, you would make more type I errors, because the criterion to detect an effect is a lower bar for one than two tailed tests.\n\nRemember type 1 errors occur when you reject the idea that chance could have caused your difference. You often never know when you make this error. It happens anytime that sampling error was the actual cause of the difference, but a researcher dismisses that possibility and concludes that their manipulation caused the difference.\n\nSimilarly, if you always ran two-tailed tests, even when you had a directional prediction, you would make fewer type I errors over the long run, because the \\(t\\) for a two-tailed test is higher than the \\(t\\) for a one-tailed test. It seems quite common for researchers to use a more conservative two-tailed test, even when they are making a directional prediction based on theory. In practice, researchers tend to adopt a standard for reporting that is common in their field. Whether or not the practice is justifiable can sometimes be an open question. The important task for any researcher, or student learning statistics, is to be able to justify their choice of test.\n\n\n\nBefore we finish up with paired-samples \\(t\\)-tests, we should talk about degrees of freedom. Our sense is that students don’t really understand degrees of freedom very well. If you are reading this textbook, you are probably still wondering what is degrees of freedom, seeing as we haven’t really talked about it all.\nFor the \\(t\\)-test, there is a formula for degrees of freedom. For the one-sample and paired sample \\(t\\)-tests, the formula is:\n\\(\\text{Degrees of Freedom} = \\text{df} = n-1\\). Where n is the number of samples in the test.\nIn our paired \\(t\\)-test example, there were 5 infants. Therefore, degrees of freedom = 5-1 = 4.\nOK, that’s a formula. Who cares about degrees of freedom, what does the number mean? And why do we report it when we report a \\(t\\)-test… you’ve probably noticed the number in parentheses e.g., \\(t\\)(4)=.72, the 4 is the \\(df\\), or degrees of freedom.\nDegrees of freedom is both a concept, and a correction. The concept is that if you estimate a property of the numbers, and you use this estimate, you will be forcing some constraints on your numbers.\nConsider the numbers: 1, 2, 3. The mean of these numbers is 2. Now, let’s say I told you that the mean of three numbers is 2. Then, how many of these three numbers have freedom? Funny question right. What we mean is, how many of the three numbers could be any number, or have the freedom to be any number.\nThe first two numbers could be any number. But, once those two numbers are set, the final number (the third number), MUST be a particular number that makes the mean 2. The first two numbers have freedom. The third number has no freedom.\nTo illustrate. Let’s freely pick two numbers: 51 and -3. I used my personal freedom to pick those two numbers. Now, if our three numbers are 51, -3, and x, and the mean of these three numbers is 2. There is only one solution, x has to be -42, otherwise the mean won’t be 2. This is one way to think about degrees of freedom. The degrees of freedom for these three numbers is n-1 = 3-1= 2, because 2 of the numbers can be free, but the last number has no freedom, it becomes fixed after the first two are decided.\nNow, statisticians often apply degrees of freedom to their calculations, especially when a second calculation relies on an estimated value. For example, when we calculate the standard deviation of a sample, we first calculate the mean of the sample right! By estimating the mean, we are fixing an aspect of our sample, and so, our sample now has n-1 degrees of freedom when we calculate the standard deviation (remember for the sample standard deviation, we divide by n-1…there’s that n-1 again.)\n\n\nThere are at least two ways to think the degrees of freedom for a \\(t\\)-test. For example, if you want to use math to compute aspects of the \\(t\\) distribution, then you need the degrees of freedom to plug in to the formula… If you want to see the formulas I’m talking about, scroll down on the \\(t\\)-test wikipedia page and look for the probability density or cumulative distribution functions…We think that is quite scary for most people, and one reason why degrees of freedom are not well-understood.\nIf we wanted to simulate the \\(t\\) distribution we could more easily see what influence degrees of freedom has on the shape of the distribution. Remember, \\(t\\) is a sample statistic, it is something we measure from the sample. So, we could simulate the process of measuring \\(t\\) from many different samples, then plot the histogram of \\(t\\) to show us the simulated \\(t\\) distribution.\n\n\nCode\nts &lt;- c(rt(10000, 4), rt(10000, 100))\ndfs &lt;- as.factor(rep(c(4, 100), each = 10000))\n\nt_df &lt;- data.frame(dfs, ts)\nt_df &lt;- t_df[abs(t_df$ts) &lt; 5, ]\n\nggplot(t_df, aes(x = ts, group = dfs, color = dfs)) +\n  geom_histogram() +\n  theme_classic() +\n  facet_wrap( ~ dfs) +\n  theme_classic(base_size=15)\n\n\n\n\n\n\n\n\nFigure 10: The width of the t distribution shrinks as sample size and degrees of freedom (from 4 to 100) increases.\n\n\n\n\n\nIn Figure 10 notice that the red distribution for \\(df = 4\\), is a little bit shorter, and a little bit wider than the bluey-green distribution for \\(df = 100\\). As degrees of freedom increase the \\(t\\) distribution gets taller (in the middle), and narrower in the range. It get’s more peaky. Can you guess the reason for this? Remember, we are estimating a sample statistic, and degrees of freedom is really just a number that refers to the number of subjects (well minus one). And, we already know that as we increase \\(n\\), our sample statistics become better estimates (less variance) of the distributional parameters they are estimating. So, \\(t\\) becomes a better estimate of it’s “true” value as sample size increase, resulting in a more narrow distribution of \\(t\\)s.\nThere is a slightly different \\(t\\) distribution for every degrees of freedom, and the critical regions associated with 5% of the extreme values are thus slightly different every time. This is why we report the degrees of freedom for each t-test, they define the distribution of \\(t\\) values for the sample-size in question. Why do we use n-1 and not n? Well, we calculate \\(t\\) using the sample standard deviation to estimate the standard error or the mean, that estimate uses n-1 in the denominator, so our \\(t\\) distribution is built assuming n-1. That’s enough for degrees of freedom…",
    "crumbs": [
      "Week01",
      "T-Tests"
    ]
  },
  {
    "objectID": "week01/chapter.html#the-paired-samples-t-test-strikes-back",
    "href": "week01/chapter.html#the-paired-samples-t-test-strikes-back",
    "title": "T-Tests",
    "section": "",
    "text": "You must be wondering if we will ever be finished talking about paired samples t-tests… why are we doing round 2, oh no! Don’t worry, we’re just going to 1) remind you about what we were doing with the infant study, and 2) do a paired samples t-test on the entire data set and discuss.\nRemember, we were wondering if the infants would look longer toward the singer who sang the familiar song during the test phase compared to the baseline phase. We showed you data from 5 infants, and walked through the computations for the \\(t\\)-test. As a reminder, it looked like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\ndiff_from_mean\nSquared_differences\n\n\n\n\n1\n0.44\n0.6\n0.16\n0.106\n0.011236\n\n\n2\n0.41\n0.68\n0.27\n0.216\n0.046656\n\n\n3\n0.75\n0.72\n-0.03\n-0.084\n0.00705600000000001\n\n\n4\n0.44\n0.28\n-0.16\n-0.214\n0.045796\n\n\n5\n0.47\n0.5\n0.03\n-0.024\n0.000575999999999999\n\n\nSums\n2.51\n2.78\n0.27\n0\n0.11132\n\n\nMeans\n0.502\n0.556\n0.054\n0\n0.022264\n\n\n\n\n\n\nsd\n0.167\n\n\n\n\n\n\nSEM\n0.075\n\n\n\n\n\n\nt\n0.72\n\n\n\n\n\n\n    One Sample t-test\n\ndata:  round(differences, digits = 2)\nt = 0.72381, df = 4, p-value = 0.5092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.1531384  0.2611384\nsample estimates:\nmean of x \n    0.054 \n\n\nLet’s write down the finding one more time: The mean difference was 0.054, \\(t\\)(4) = .72, \\(p\\) =.509. We can also now confirm, that the \\(p\\)-value was from a two-tailed test. So, what does this all really mean.\nWe can say that a \\(t\\) value with an absolute of .72 or larger occurs 50.9% of the time. More precisely, the distribution of no differences (the null), will produce a \\(t\\) value this large or larger 50.9% of the time. In other words, chance alone good have easily produced the \\(t\\) value from our sample, and the mean difference we observed or .054, could easily have been a result of chance.\nLet’s quickly put all of the data in the \\(t\\)-test, and re-run the test using all of the infant subjects.\n\n\nCode\npaired_sample_df &lt;-  data.frame(infant=1:32, \n                               Baseline = round(experiment_one$Baseline_Proportion_Gaze_to_Singer[1:32],digits=2), \n                               Test = round(experiment_one$Test_Proportion_Gaze_to_Singer[1:32], digits=2))\n\ndifferences &lt;-  paired_sample_df$Test-paired_sample_df$Baseline\nt.test(differences,mu=0)\n\n\n\n    One Sample t-test\n\ndata:  differences\nt = 2.4388, df = 31, p-value = 0.02066\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.01192088 0.13370412\nsample estimates:\nmean of x \n0.0728125 \n\n\nNow we get a very different answer. We would summarize the results saying the mean difference was .073, t(31) = 2.44, p = 0.020. How many total infants were their? Well the degrees of freedom was 31, so there must have been 32 infants in the study. Now we see a much smaller \\(p\\)-value. This was also a two-tailed test, so we that observing a \\(t\\) value of 2.4 or greater (absolute value) only occurs 2% of the time. In other words, the distribution of no differences will produce the observed t-value very rarely. So, it is unlikely that the observed mean difference of .073 was due to chance (it could have been due to chance, but that is very unlikely). As a result, we can be somewhat confident in concluding that something about seeing and hearing a unfamiliar person sing a familiar song, causes an infant to draw their attention toward the singer, and this potentially benefits social learning on the part of the infant.",
    "crumbs": [
      "Week01",
      "T-Tests"
    ]
  },
  {
    "objectID": "week01/chapter.html#independent-samples-t-test-the-return-of-the-t-test",
    "href": "week01/chapter.html#independent-samples-t-test-the-return-of-the-t-test",
    "title": "T-Tests",
    "section": "",
    "text": "If you’ve been following the Star Wars references, we are on last movie (of the original trilogy)… the independent t-test. This is were basically the same story plays out as before, only slightly different.\nRemember there are different \\(t\\)-tests for different kinds of research designs. When your design is a between-subjects design, you use an independent samples t-test. Between-subjects design involve different people or subjects in each experimental condition. If there are two conditions, and 10 people in each, then there are 20 total people. And, there are no paired scores, because every single person is measured once, not twice, no repeated measures. Because there are no repeated measures we can’t look at the difference scores between conditions one and two. The scores are not paired in any meaningful way, to it doesn’t make sense to subtract them. So what do we do?\nThe logic of the independent samples t-test is the very same as the other \\(t\\)-tests. We calculated the means for each group, then we find the difference. That goes into the numerator of the t formula. Then we get an estimate of the variation for the denominator. We divide the mean difference by the estimate of the variation, and we get \\(t\\). It’s the same as before.\nThe only wrinkle here is what goes into the denominator? How should we calculate the estimate of the variance? It would be nice if we could do something very straightforward like this, say for an experiment with two groups A and B:\n\\(t = \\frac{\\bar{A}-\\bar{B}}{(\\frac{SEM_A+SEM_B}{2})}\\)\nIn plain language, this is just:\n\nFind the mean difference for the top part\nCompute the SEM (standard error of the mean) for each group, and average them together to make a single estimate, pooling over both samples.\n\nThis would be nice, but unfortunately, it turns out that finding the average of two standard errors of the mean is not the best way to do it. This would create a biased estimator of the variation for the hypothesized distribution of no differences. We won’t go into the math here, but instead of the above formula, we an use a different one that gives as an unbiased estimate of the pooled standard error of the sample mean. Our new and improved \\(t\\) formula would look like this:\n\\(t = \\frac{\\bar{X_A}-\\bar{X_B}}{s_p * \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_B}}}\\)\nand, \\(s_p\\), which is the pooled sample standard deviation is defined as, note the $s$es in the formula are variances:\n\\(s_p = \\sqrt{\\frac{(n_A-1)s_A^2 + (n_B-1)s^2_B}{n_A +n_B -2}}\\)\nBelieve you me, that is so much more formula than I wanted to type out. Shall we do one independent \\(t\\)-test example by hand, just to see the computations? Let’s do it…but in a slightly different way than you expect. I show the steps using R. I made some fake scores for groups A and B. Then, I followed all of the steps from the formula, but made R do each of the calculations. This shows you the needed steps by following the code. At the end, I print the \\(t\\)-test values I computed “by hand”, and then the \\(t\\)-test value that the R software outputs using the \\(t\\)-test function. You should be able to get the same values for \\(t\\), if you were brave enough to compute \\(t\\) by hand.\n\n\nCode\n## By \"hand\" using R r code\na &lt;- c(1,2,3,4,5)\nb &lt;- c(3,5,4,7,9)\n\nmean_difference &lt;- mean(a)-mean(b) # compute mean difference\n\nvariance_a &lt;- var(a) # compute variance for A\nvariance_b &lt;- var(b) # compute variance for B\n\n# Compute top part and bottom part of sp formula\n\nsp_numerator &lt;- (4*variance_a + 4* variance_b) \nsp_denominator &lt;- 5+5-2\nsp &lt;- sqrt(sp_numerator/sp_denominator) # compute sp\n\n\n# compute t following formulat\n\nt &lt;- mean_difference / ( sp * sqrt( (1/5) +(1/5) ) )\n\nt # print results\n\n\n[1] -2.017991\n\n\nCode\n# using the R function t.test\nt.test(a,b, paired=FALSE, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  a and b\nt = -2.018, df = 8, p-value = 0.0783\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -5.5710785  0.3710785\nsample estimates:\nmean of x mean of y \n      3.0       5.6",
    "crumbs": [
      "Week01",
      "T-Tests"
    ]
  },
  {
    "objectID": "week01/chapter.html#simulating-data-for-t-tests",
    "href": "week01/chapter.html#simulating-data-for-t-tests",
    "title": "T-Tests",
    "section": "",
    "text": "An “advanced” topic for \\(t\\)-tests is the idea of using R to conduct simulations for \\(t\\)-tests.\nIf you recall, \\(t\\) is a property of a sample. We calculate \\(t\\) from our sample. The \\(t\\) distribution is the hypothetical behavior of our sample. That is, if we had taken thousands upon thousands of samples, and calculated \\(t\\) for each one, and then looked at the distribution of those \\(t\\)’s, we would have the sampling distribution of \\(t\\)!\nIt can be very useful to get in the habit of using R to simulate data under certain conditions, to see how your sample data, and things like \\(t\\) behave. Why is this useful? It mainly prepares you with some intuitions about how sampling error (random chance) can influence your results, given specific parameters of your design, such as sample-size, the size of the mean difference you expect to find in your data, and the amount of variation you might find. These methods can be used formally to conduct power-analyses. Or more informally for data sense.\n\n\nHere are the steps you might follow to simulate data for a one sample \\(t\\)-test.\n\nMake some assumptions about what your sample (that you might be planning to collect) might look like. For example, you might be planning to collect 30 subjects worth of data. The scores of those data points might come from a normal distribution (mean = 50, sd = 10).\nsample simulated numbers from the distribution, then conduct a \\(t\\)-test on the simulated numbers. Save the statistics you want (such as \\(t\\)s and \\(p\\)s), and then see how things behave.\n\nLet’s do this a couple different times. First, let’s simulate samples with N = 30, taken from a normal (mean= 50, sd = 25). We’ll do a simulation with 1000 simulations. For each simulation, we will compare the sample mean with a population mean of 50. There should be no difference on average here. Figure 11 is the null distribution that we are simulating.\n\n\nCode\n# steps to create fake data from a distribution\n# and conduct t-tests on the simulated data\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor (i in 1:1000) {\n  my_sample &lt;- rnorm(n = 30, mean = 50, sd = 25)\n  t_test &lt;- t.test (my_sample, mu = 50)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\nCode\n#plot histograms of t and p values for 1000 simulations\nhist(save_ts)\n\n\n\n\n\n\n\n\nFigure 11: The distribution of \\(t\\)-values under the null. These are the \\(t\\) values that are produced by chance alone.\n\n\n\n\n\n\n\nCode\nhist(save_ps)\n\n\n\n\n\n\n\n\nFigure 12: The distribution of \\(p\\)-values that are observed is flat under the null.\n\n\n\n\n\nNeat. We see both a \\(t\\) distribution, that looks like \\(t\\) distribution as it should. And we see the \\(p\\) distribution. This shows us how often we get \\(t\\) values of particular sizes. You may find it interesting that the \\(p\\)-distribution is flat under the null, which we are simulating here. This means that you have the same chances of a getting a \\(t\\) with a p-value between 0 and 0.05, as you would for getting a \\(t\\) with a p-value between .90 and .95. Those ranges are both ranges of 5%, so there are an equal amount of \\(t\\) values in them by definition.\nHere’s another way to do the same simulation in R, using the replicate function, instead a for loop:\n\n\nCode\nsimulated_ts &lt;- replicate(1000,\n                          t.test(rnorm(30, 50, 25), mu = 50)$statistic)\nhist(simulated_ts)\n\n\n\n\n\n\n\n\nFigure 13: Simulating \\(t\\)s in R.\n\n\n\n\n\n\n\nCode\nsimulated_ps &lt;- replicate(1000,\n                          t.test(rnorm(30, 50, 25), mu = 50)$p.value)\nhist(simulated_ps)\n\n\n\n\n\n\n\n\nFigure 14: Simulating \\(p\\)s in R.\n\n\n\n\n\n\n\n\nThe code below is set up to sample 10 scores for condition A and B from the same normal distribution. The simulation is conducted 1000 times, and the \\(t\\)s and \\(p\\)s are saved and plotted for each.\n\n\nCode\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(10,10,5)\n  condition_B &lt;- rnorm(10,10,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\nCode\nhist(save_ts)\n\n\n\n\n\n\n\n\nFigure 15: 1000 simulated ts from the null distribution\n\n\n\n\n\n\n\nCode\nhist(save_ps)\n\n\n\n\n\n\n\n\nFigure 16: 1000 simulated ps from the null distribution\n\n\n\n\n\nAccording to the simulation. When there are no differences between the conditions, and the samples are being pulled from the very same distribution, you get these two distributions for \\(t\\) and \\(p\\). These again show how the null distribution of no differences behaves.\nFor any of these simulations, if you rejected the null-hypothesis (that your difference was only due to chance), you would be making a type I error. If you set your alpha criteria to \\(\\alpha = .05\\), we can ask how many type I errors were made in these 1000 simulations. The answer is:\n\n\nCode\nlength(save_ps[save_ps&lt;.05])\n\n\n[1] 49\n\n\nCode\nlength(save_ps[save_ps&lt;.05])/1000\n\n\n[1] 0.049\n\n\nWe happened to make 49. The expectation over the long run is 5% type I error rates (if your alpha is .05).\nWhat happens if there actually is a difference in the simulated data, let’s set one condition to have a larger mean than the other:\n\n\nCode\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(10,10,5)\n  condition_B &lt;- rnorm(10,13,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\nCode\nhist(save_ts)\n\n\n\n\n\n\n\n\nFigure 17: 1000 ts when there is a true difference\n\n\n\n\n\n\n\nCode\nhist(save_ps)\n\n\n\n\n\n\n\n\nFigure 18: 1000 ps when there is a true difference\n\n\n\n\n\nNow you can see that the \\(p\\)-value distribution is skewed to the left. This is because when there is a true effect, you will get p-values that are less than .05 more often. Or, rather, you get larger \\(t\\) values than you normally would if there were no differences.\nIn this case, we wouldn’t be making a type I error if we rejected the null when p was smaller than .05. How many times would we do that out of our 1000 experiments?\n\n\nCode\nlength(save_ps[save_ps&lt;.05])\n\n\n[1] 251\n\n\nCode\nlength(save_ps[save_ps&lt;.05])/1000\n\n\n[1] 0.251\n\n\nWe happened to get 251 simulations where p was less than .05, that’s only 0.251 experiments. If you were the researcher, would you want to run an experiment that would be successful only 0.251 of the time? I wouldn’t. I would run a better experiment.\nHow would you run a better simulated experiment? Well, you could increase \\(n\\), the number of subjects in the experiment. Let’s increase \\(n\\) from 10 to 100, and see what happens to the number of “significant” simulated experiments.\n\n\nCode\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(100,10,5)\n  condition_B &lt;- rnorm(100,13,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\nCode\nhist(save_ts)\n\n\n\n\n\n\n\n\nFigure 19: 1000 ts for n =100, when there is a true effect\n\n\n\n\n\n\n\nCode\nhist(save_ps)\n\n\nlength(save_ps[save_ps&lt;.05])\n\n\n[1] 993\n\n\nCode\nlength(save_ps[save_ps&lt;.05])/1000\n\n\n[1] 0.993\n\n\n\n\n\n\n\n\nFigure 20: 1000 ps for n =100, when there is a true effect\n\n\n\n\n\nCool, now almost all of the experiments show a \\(p\\)-value of less than .05 (using a two-tailed test, that’s the default in R). See, you could use this simulation process to determine how many subjects you need to reliably find your effect.\n\n\n\nJust change the t.test function like so… this is for the null, assuming no difference between groups.\n\n\nCode\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  group_A &lt;- rnorm(10,10,5)\n  group_B &lt;- rnorm(10,10,5)\n  t_test &lt;- t.test(group_A, group_B, paired=FALSE, var.equal=TRUE)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\nCode\nhist(save_ts)\n\n\n\n\n\n\n\n\nFigure 21: 1000 ts for n =100, when there is a true effect\n\n\n\n\n\n\n\nCode\nhist(save_ps)\n\n\nlength(save_ps[save_ps&lt;.05])\n\n\n[1] 35\n\n\nCode\nlength(save_ps[save_ps&lt;.05])/1000\n\n\n[1] 0.035\n\n\n\n\n\n\n\n\nFigure 22: 1000 ps for n =100, when there is a true effect",
    "crumbs": [
      "Week01",
      "T-Tests"
    ]
  },
  {
    "objectID": "week01/chapter.html#videos",
    "href": "week01/chapter.html#videos",
    "title": "T-Tests",
    "section": "",
    "text": "Gordon Original ## 1. Introduction\nThe t-test is a fundamental statistical method used in psychology to compare the means of two groups. This chapter will explore the theory behind t-tests, demonstrate practical applications using R, and provide a structured write-up in APA style. Whether you’re examining the effectiveness of a new therapy or comparing cognitive performance between groups, the t-test is an essential tool for psychological research.",
    "crumbs": [
      "Week01",
      "T-Tests"
    ]
  },
  {
    "objectID": "week01/chapter.html#main-content",
    "href": "week01/chapter.html#main-content",
    "title": "T-Tests",
    "section": "",
    "text": "A t-test evaluates whether the means of two groups are statistically different from each other. The most common types are the independent samples t-test, paired samples t-test, and one-sample t-test. Each type tests different hypotheses and has unique assumptions.\n\n\nThis test compares the means of two independent groups to ascertain if there is statistical evidence that the associated population means are significantly different.\n\n\n\nThis test compares means from the same group at different times (e.g., before and after a treatment) or matched pairs.\n\n\n\nThis test determines whether the mean of a single group is different from a known mean.\n\n\n\n\n\nIndependence: Observations within each group must be independent.\nNormality: Data in each group should be approximately normally distributed.\nHomogeneity of Variance: Variances in the two groups should be roughly equal (for independent samples t-test).",
    "crumbs": [
      "Week01",
      "T-Tests"
    ]
  },
  {
    "objectID": "week01/chapter.html#methods-and-measures",
    "href": "week01/chapter.html#methods-and-measures",
    "title": "T-Tests",
    "section": "",
    "text": "To conduct a t-test, you need to: 1. Formulate the null (H0) and alternative (H1) hypotheses. 2. Collect and organize data. 3. Check assumptions. 4. Calculate the t-statistic and degrees of freedom. 5. Determine the p-value. 6. Make a decision to reject or fail to reject H0 based on the p-value.\n\n\nResearch Question: Does a new cognitive-behavioral therapy (CBT) improve anxiety levels more than a placebo?\nHypotheses: - H0: There is no difference in anxiety levels between the CBT and placebo groups. - H1: The CBT group has lower anxiety levels than the placebo group.\nMeasures: Anxiety levels are measured using a standardized anxiety inventory.",
    "crumbs": [
      "Week01",
      "T-Tests"
    ]
  },
  {
    "objectID": "week01/chapter.html#analysis-with-r-code",
    "href": "week01/chapter.html#analysis-with-r-code",
    "title": "T-Tests",
    "section": "",
    "text": "Let’s use R for an independent samples t-test.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe t.test function provides the t-statistic, degrees of freedom, and p-value. If the p-value is less than the significance level (e.g., 0.05), we reject H0.\n    Welch Two Sample t-test\n\ndata:  anxiety by group\nt = -8.6943, df = 17.564, p-value = 8.922e-08\nalternative hypothesis: true difference in means between group CBT and group Placebo is not equal to 0\n95 percent confidence interval:\n -9.936595 -6.063405\nsample estimates:\n    mean in group CBT mean in group Placebo \n                 21.7                  29.7",
    "crumbs": [
      "Week01",
      "T-Tests"
    ]
  },
  {
    "objectID": "week01/chapter.html#debates-and-controversies",
    "href": "week01/chapter.html#debates-and-controversies",
    "title": "T-Tests",
    "section": "",
    "text": "Violating assumptions (e.g., normality, homogeneity of variance) can lead to incorrect conclusions. Robust alternatives or data transformations may be necessary (Wilcox, 2012).\n\n\n\nOverreliance on p-values can lead to misinterpretation of results. It’s crucial to report effect sizes and confidence intervals (Cumming, 2014).\n\n\n\nConducting multiple t-tests increases the risk of Type I errors. Methods like Bonferroni correction adjust for this but can be overly conservative (Perneger, 1998).",
    "crumbs": [
      "Week01",
      "T-Tests"
    ]
  },
  {
    "objectID": "week01/chapter.html#conclusion",
    "href": "week01/chapter.html#conclusion",
    "title": "T-Tests",
    "section": "",
    "text": "T-tests are a vital statistical method in psychological research for comparing means between groups. Understanding their theory, assumptions, and correct application in R can significantly enhance the rigor of your analyses. Always ensure to check assumptions, consider effect sizes, and be mindful of the limitations and debates surrounding t-tests.\n\n\nCumming, G. (2014). The new statistics: Why and how. Psychological Science, 25(1), 7-29.\nPerneger, T. V. (1998). What’s wrong with Bonferroni adjustments. BMJ, 316(7139), 1236-1238.\nWilcox, R. R. (2012). Modern statistics for the social and behavioral sciences: A practical introduction. CRC Press.\nThis chapter provides a comprehensive overview of conducting t-tests in R, essential for first-year psychology students. Future chapters will delve into more advanced statistical methods to further enhance your analytical skills.",
    "crumbs": [
      "Week01",
      "T-Tests"
    ]
  },
  {
    "objectID": "week01/lab-slides.html#getting-up",
    "href": "week01/lab-slides.html#getting-up",
    "title": "Lab 01",
    "section": "Getting up",
    "text": "Getting up\n\nTurn off alarm\nGet out of bed",
    "crumbs": [
      "Week01",
      "Lab 01"
    ]
  },
  {
    "objectID": "week01/lab-slides.html#breakfast",
    "href": "week01/lab-slides.html#breakfast",
    "title": "Lab 01",
    "section": "Breakfast",
    "text": "Breakfast\n\nEat eggs\nDrink coffee",
    "crumbs": [
      "Week01",
      "Lab 01"
    ]
  },
  {
    "objectID": "week01/lab-slides.html#dinner",
    "href": "week01/lab-slides.html#dinner",
    "title": "Lab 01",
    "section": "Dinner",
    "text": "Dinner\n\nEat spaghetti\nDrink wine",
    "crumbs": [
      "Week01",
      "Lab 01"
    ]
  },
  {
    "objectID": "week01/lab-slides.html#going-to-sleep",
    "href": "week01/lab-slides.html#going-to-sleep",
    "title": "Lab 01",
    "section": "Going to sleep",
    "text": "Going to sleep\n\nGet in bed\nCount sheep",
    "crumbs": [
      "Week01",
      "Lab 01"
    ]
  },
  {
    "objectID": "week01/lab.html",
    "href": "week01/lab.html",
    "title": "Looking at Data",
    "section": "",
    "text": "ReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{gordon_wright,\n  author = {Gordon Wright, Dr.},\n  title = {Looking at {Data}},\n  url = {http://introrm.littlemonkeylab.com/week01/lab.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGordon Wright, Dr. n.d. “Looking at Data.” http://introrm.littlemonkeylab.com/week01/lab.html.",
    "crumbs": [
      "Week01",
      "Looking at Data"
    ]
  },
  {
    "objectID": "week01/codeoptions.html",
    "href": "week01/codeoptions.html",
    "title": "codeoptions",
    "section": "",
    "text": "Show the code\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n1penguins |&gt;\n2  mutate(\n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm\n  ) \n\n\n\n1\n\nTake penguins, and then,\n\n2\n\nadd new columns for the bill ratio and bill area.\n\n\n\n\n# A tibble: 344 × 10\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 4 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_ratio &lt;dbl&gt;, bill_area &lt;dbl&gt;",
    "crumbs": [
      "Week01",
      "codeoptions"
    ]
  },
  {
    "objectID": "week01/codeoptions.html#version-1---chunk-fold-with-line-numbers",
    "href": "week01/codeoptions.html#version-1---chunk-fold-with-line-numbers",
    "title": "codeoptions",
    "section": "",
    "text": "Show the code\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n1penguins |&gt;\n2  mutate(\n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm\n  ) \n\n\n\n1\n\nTake penguins, and then,\n\n2\n\nadd new columns for the bill ratio and bill area.\n\n\n\n\n# A tibble: 344 × 10\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 4 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_ratio &lt;dbl&gt;, bill_area &lt;dbl&gt;",
    "crumbs": [
      "Week01",
      "codeoptions"
    ]
  },
  {
    "objectID": "week01/codeoptions.html#version-2---standard-tabset-with-code-and-code-yoda",
    "href": "week01/codeoptions.html#version-2---standard-tabset-with-code-and-code-yoda",
    "title": "codeoptions",
    "section": "Version 2 - Standard Tabset with code and Code Yoda",
    "text": "Version 2 - Standard Tabset with code and Code Yoda\n\nCodeCodeYoda (#| eval: false)\n\n\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins |&gt;                                     \n  mutate(                                       \n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm \n  )\n\n# A tibble: 344 × 10\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 4 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_ratio &lt;dbl&gt;, bill_area &lt;dbl&gt;\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n1penguins |&gt;\n2  mutate(\n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm\n  )\n\n\n1\n\nTake penguins, and then,\n\n2\n\nadd new columns for the bill ratio and bill area.",
    "crumbs": [
      "Week01",
      "codeoptions"
    ]
  },
  {
    "objectID": "week01/codeoptions.html#version-3---panelize-and-webr-coatless-and-cant-seem-to-get-annotations-to-work-echofalse-for-r",
    "href": "week01/codeoptions.html#version-3---panelize-and-webr-coatless-and-cant-seem-to-get-annotations-to-work-echofalse-for-r",
    "title": "codeoptions",
    "section": "Version 3 - Panelize and Webr (coatless) and can’t seem to get annotations to work (echo:false for R)",
    "text": "Version 3 - Panelize and Webr (coatless) and can’t seem to get annotations to work (echo:false for R)\n\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins |&gt;                                     \n  mutate(                                       \n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm \n  )         \n\n# A tibble: 344 × 10\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 4 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_ratio &lt;dbl&gt;, bill_area &lt;dbl&gt;",
    "crumbs": [
      "Week01",
      "codeoptions"
    ]
  },
  {
    "objectID": "week01/codeoptions.html#version-4---just-trying-to-get-the-code-annotations-to-work-eval-false",
    "href": "week01/codeoptions.html#version-4---just-trying-to-get-the-code-annotations-to-work-eval-false",
    "title": "codeoptions",
    "section": "Version 4 - Just trying to get the Code annotations to work (#| eval: false)",
    "text": "Version 4 - Just trying to get the Code annotations to work (#| eval: false)\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n1penguins |&gt;\n2  mutate(\n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm\n  )\n\n\n1\n\nTake penguins, and then,\n\n2\n\nadd new columns for the bill ratio and bill area.",
    "crumbs": [
      "Week01",
      "codeoptions"
    ]
  },
  {
    "objectID": "week01/codeoptions.html#native-quarto",
    "href": "week01/codeoptions.html#native-quarto",
    "title": "codeoptions",
    "section": "Native Quarto",
    "text": "Native Quarto\n\n## With size spec",
    "crumbs": [
      "Week01",
      "codeoptions"
    ]
  },
  {
    "objectID": "webexercises.html",
    "href": "webexercises.html",
    "title": "Webexercises",
    "section": "",
    "text": "This is a Web Exercise template created by the psychology teaching team at the University of Glasgow, based on ideas from Software Carpentry. This template shows how instructors can easily create interactive web documents that students can use in self-guided learning.\nThe {webexercises} package provides a number of functions that you use in inline R code or through code chunk options to create HTML widgets (text boxes, pull down menus, buttons that reveal hidden content). Examples are given below. Render this file to HTML to see how it works.\nNOTE: To use the widgets in the compiled HTML file, you need to have a JavaScript-enabled browser.",
    "crumbs": [
      "Webexercises"
    ]
  },
  {
    "objectID": "webexercises.html#example-questions",
    "href": "webexercises.html#example-questions",
    "title": "Webexercises",
    "section": "Example Questions",
    "text": "Example Questions\n\nFill-In-The-Blanks (fitb())\nCreate fill-in-the-blank questions using fitb(), providing the answer as the first argument.\n\n2 + 2 is \n\nYou can also create these questions dynamically, using variables from your R session.\n\nThe square root of 49 is: \n\nThe blanks are case-sensitive; if you don’t care about case, use the argument ignore_case = TRUE.\n\nWhat is the letter after D? \n\nIf you want to ignore differences in whitespace use, use the argument ignore_ws = TRUE (which is the default) and include spaces in your answer anywhere they could be acceptable.\n\nHow do you load the tidyverse package? \n\nYou can set more than one possible correct answer by setting the answers as a vector.\n\nType a vowel: \n\nYou can use regular expressions to test answers against more complex rules.\n\nType any 3 letters: \n\n\n\nMultiple Choice (mcq())\n\n“Never gonna give you up, never gonna: let you goturn you downrun awaylet you down”\n“I bless the rainsguess it rainssense the rain down in Africa” -Toto\n\n\n\nTrue or False (torf())\n\nTrue or False? You can permute values in a vector using sample(). TRUEFALSE\n\n\n\nLonger MCQs (longmcq())\nWhen your answers are very long, sometimes a drop-down select box gets formatted oddly. You can use longmcq() to deal with this. Since the answers are long, It’s probably best to set up the options inside an R chunk with echo=FALSE.\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion\n\nWhat is true about a 95% confidence interval of the mean?\n\n if you repeated the process many times, 95% of intervals calculated in this way contain the true mean 95% of the data fall within this range there is a 95% probability that the true mean lies within this range",
    "crumbs": [
      "Webexercises"
    ]
  },
  {
    "objectID": "webexercises.html#checked-sections",
    "href": "webexercises.html#checked-sections",
    "title": "Webexercises",
    "section": "Checked sections",
    "text": "Checked sections\nCreate sections with the class webex-check to add a button that hides feedback until it is pressed. Add the class webex-box to draw a box around the section (or use your own styles).\n\nI am going to learn a lot: TRUEFALSE\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion",
    "crumbs": [
      "Webexercises"
    ]
  },
  {
    "objectID": "webexercises.html#hidden-solutions-and-hints",
    "href": "webexercises.html#hidden-solutions-and-hints",
    "title": "Webexercises",
    "section": "Hidden solutions and hints",
    "text": "Hidden solutions and hints\nYou can fence off a solution area that will be hidden behind a button using hide() before the solution and unhide() after, each as inline R code. Pass the text you want to appear on the button to the hide() function.\nIf the solution is a code chunk, instead of using hide() and unhide(), simply set the webex.hide chunk option to TRUE, or set it to the string you wish to display on the button.\nRecreate the scatterplot below, using the built-in cars dataset.\n\n\n\n\n\n\n\n\n\n\n\nI need a hint\n\nSee the documentation for plot() (?plot)\n\n\n\n\n\nClick here to see the solution\n\nplot(cars$speed, cars$dist)",
    "crumbs": [
      "Webexercises"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{untitled,\n  author = {},\n  title = {About},\n  url = {http://introrm.littlemonkeylab.com/about.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“About.” n.d. http://introrm.littlemonkeylab.com/about.html.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "week02/lab-slides.html#introduction-15-minutes",
    "href": "week02/lab-slides.html#introduction-15-minutes",
    "title": "Lab 02 - Overview",
    "section": "Introduction (15 minutes)",
    "text": "Introduction (15 minutes)\nWhat is wellbeing?\n\nPhysical, emotional, social, and psychological dimensions\nWhy is it important to measure wellbeing?\n\n🧑‍🏫 Lab Objectives:\n- Explore wellbeing measures\n- Discuss interventions\n- Critically analyze the money-happiness relationship\n\n\n\n\n\n\nNote\n\n\n\nRemember to think about wellbeing in your own AND others’ lives",
    "crumbs": [
      "Week02",
      "Lab 02 - Overview"
    ]
  },
  {
    "objectID": "week02/lab-slides.html#measuring-wellbeing-30-minutes",
    "href": "week02/lab-slides.html#measuring-wellbeing-30-minutes",
    "title": "Lab 02 - Overview",
    "section": "Measuring Wellbeing (30 minutes)",
    "text": "Measuring Wellbeing (30 minutes)\n📝 Activity: Complete multiple wellbeing measures\n\nEmotional Wellbeing Scale\nLife Satisfaction Survey\nFlourishing Scale\n\n🔍 Key Question: How did these measures make you feel? What were your first impressions?",
    "crumbs": [
      "Week02",
      "Lab 02 - Overview"
    ]
  },
  {
    "objectID": "week02/lab-slides.html#designing-interventions-30-minutes",
    "href": "week02/lab-slides.html#designing-interventions-30-minutes",
    "title": "Lab 02 - Overview",
    "section": "Designing Interventions (30 minutes)",
    "text": "Designing Interventions (30 minutes)\nSmall Group Activity: Design a brief wellbeing intervention\n\nBrainstorm in groups of 3-4\nFocus on physical or mental wellbeing\n\n💡 Presentation: Share and discuss each group’s intervention idea\n\n\n\n\n\n\nTip\n\n\n\nEncourage creativity: interventions can be apps, mindfulness practices, or community-based.",
    "crumbs": [
      "Week02",
      "Lab 02 - Overview"
    ]
  },
  {
    "objectID": "week02/lab-slides.html#money-and-happiness-activity-30-minutes",
    "href": "week02/lab-slides.html#money-and-happiness-activity-30-minutes",
    "title": "Lab 02 - Overview",
    "section": "Money and Happiness Activity (30 minutes)",
    "text": "Money and Happiness Activity (30 minutes)\n💰 Does Money Buy Happiness?\n\nReview research on the money-happiness link\nPlot your own hypothesized relationship\n\n📊 Activity: Compare your results with existing research\n\n\n\n\n\n\nImportant\n\n\n\nKey insight: Money increases happiness to an extent—where do students think the threshold is?",
    "crumbs": [
      "Week02",
      "Lab 02 - Overview"
    ]
  },
  {
    "objectID": "week02/lab-slides.html#critical-analysis-15-minutes",
    "href": "week02/lab-slides.html#critical-analysis-15-minutes",
    "title": "Lab 02 - Overview",
    "section": "Critical Analysis (15 minutes)",
    "text": "Critical Analysis (15 minutes)\nEvaluating Wellbeing Measures\n\nWhat are the strengths of the scales you used?\nWhat are the weaknesses?\n\n🛠️ Consider:\n\nReliability and validity\nCultural differences",
    "crumbs": [
      "Week02",
      "Lab 02 - Overview"
    ]
  },
  {
    "objectID": "week02/lab-slides.html#conclusion-and-reflection-15-minutes",
    "href": "week02/lab-slides.html#conclusion-and-reflection-15-minutes",
    "title": "Lab 02 - Overview",
    "section": "Conclusion and Reflection (15 minutes)",
    "text": "Conclusion and Reflection (15 minutes)\n🔮 Discussion: - Implications of wellbeing measurement for research and practice - What surprised you about the findings today?\n✍️ Individual Reflection: How will you apply these insights in your future work?\n\n\n\n\n\n\nTip\n\n\n\nEncourage students to think about ethical considerations in wellbeing interventions.",
    "crumbs": [
      "Week02",
      "Lab 02 - Overview"
    ]
  },
  {
    "objectID": "week02/lab-slides.html#wellbeing-measurement-challenge",
    "href": "week02/lab-slides.html#wellbeing-measurement-challenge",
    "title": "Lab 02 - Overview",
    "section": "",
    "text": "Learning Outcomes:\n\nUnderstand different concepts of wellbeing\nApply wellbeing measurement tools\nCritically assess research on wellbeing and happiness",
    "crumbs": [
      "Week02",
      "Lab 02 - Overview"
    ]
  },
  {
    "objectID": "week02/lab.html",
    "href": "week02/lab.html",
    "title": "Now it’s your turn",
    "section": "",
    "text": "Chapter 1: Wellbeing Measurement Challenge\nIntroduction: Standing on the Shoulders of Giants “If I have seen further, it is by standing on the shoulders of giants.” — Isaac Newton Before diving into your first lab, you’ve had a week to explore some key aspects of wellbeing. The work of pioneers in psychology and wellbeing research, such as Martin Seligman and Ed Diener, has shaped the field you’re about to engage with. In this chapter, we’ll guide you through core concepts, helping you build on their successes while avoiding common pitfalls.\n\nWhat is Wellbeing? (15 minutes) Concept Overview: Wellbeing is multidimensional, involving emotional, psychological, social, and physical components. By understanding these different dimensions, we can more accurately measure and improve wellbeing.\n\nEmotional Wellbeing: Feelings of happiness and satisfaction Psychological Wellbeing: Purpose, self-acceptance, and growth Social Wellbeing: Relationships and community belonging Key Question: Which dimensions of wellbeing have researchers focused on, and why?\nGiants in the Field:\nMartin Seligman’s PERMA Model (Positive Emotion, Engagement, Relationships, Meaning, Accomplishment) Ed Diener’s work on Subjective Wellbeing Takeaway: Success: These models provide a comprehensive framework to explore wellbeing. Caution: Beware of over-simplifying wellbeing—it’s not just about happiness. 2. Measuring Wellbeing (30 minutes) Activity Overview:\nYou will complete several measures that assess different aspects of your wellbeing.\nCommon Wellbeing Measures:\nLife Satisfaction Survey (by Diener) Emotional Wellbeing Scale Flourishing Scale Key Terminology:\nReliability: Does the measure consistently capture what it’s supposed to? Validity: Does it actually measure wellbeing, or is it capturing something else? Reflection: What did you think about the different measures? Did they capture your wellbeing accurately? How did they make you feel?\nWhat the Giants Have Achieved:\nStrength: These scales are scientifically validated, with a strong track record. Potential Pitfalls: They can be too general or culturally biased. What can you add to future measures? Looking Ahead: In future labs, you’ll learn more about psychometrics—the science of measuring psychological traits like wellbeing. You’ll also explore how to test the validity and reliability of these measures in your own research.\n\nDesigning Wellbeing Interventions (30 minutes) Group Activity Overview:\n\nDesign a simple wellbeing intervention that could improve one of the dimensions of wellbeing.\nKey Questions:\nWhat issue is your intervention addressing? Is it mental, physical, or social wellbeing? How will you measure its effectiveness? Example Interventions:\nA daily gratitude practice (Psychological Wellbeing) A social support network for peers (Social Wellbeing) Giants’ Wisdom:\nStrength: Research has shown that small changes can make a big impact, like the power of gratitude (Seligman). Common Pitfalls: Many interventions fail because they are too difficult to implement in the real world. Keep it simple and practical. Looking Ahead: Later in your course, you will learn how to develop and test interventions more formally, using experimental designs and data analysis to prove their effectiveness.\n\nMoney and Happiness: The Relationship (30 minutes) Concept Overview: Research shows that while money can buy happiness to an extent, it has diminishing returns.\n\nKey Terminology:\nDiminishing Marginal Returns: Beyond a certain point, each extra unit of money brings less additional happiness. Activity:\nPlot your own hypothesis of the money-happiness relationship. How do you think income relates to happiness? What Giants Have Found:\nStrength: Research like Kahneman and Deaton’s work shows a clear threshold (~$75,000). Common Error: Focusing too much on income while ignoring other key factors, like relationships or health. Looking Ahead: You’ll soon dive deeper into economic psychology and how subjective factors, like perception and expectations, play a role in happiness and wellbeing.\n\nCritical Analysis of Wellbeing Measures (15 minutes) Activity: Evaluate the strengths and weaknesses of the wellbeing measures you’ve used.\n\nWere they easy to understand? Did they capture the complexity of wellbeing? Would you trust them in your own research? Giants’ Learnings:\nSuccess: Existing scales have been rigorously tested for validity and reliability. Caution: Be critical of whether these scales can be applied to different populations. A measure that works well in the UK might not translate well to other cultures. Looking Ahead: Next, you’ll explore how to conduct your own critical reviews of research, building the skills to assess strengths and weaknesses in published studies.\n\nConclusion and Reflection (15 minutes) Reflect on what you’ve learned about wellbeing, money and happiness, and how to design practical interventions.\n\nKey Points:\nWellbeing is multifaceted and should be approached from multiple angles. Measurement tools are useful but not flawless—be mindful of their limitations. Small interventions can make a big difference, but they must be practical. Next Steps:\nContinue exploring different measures of wellbeing in future labs. Develop your own intervention projects, and learn how to evaluate their success. Apply critical thinking when assessing research and start thinking about your own research designs. Final Reflection Prompt: What insights from today will you take into your own work? How will you use the knowledge of giants in this field to inform your own approach?\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{gordon_wright,\n  author = {Gordon Wright, Dr.},\n  title = {Now It’s Your Turn},\n  url = {http://introrm.littlemonkeylab.com/week02/lab.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGordon Wright, Dr. n.d. “Now It’s Your Turn.” http://introrm.littlemonkeylab.com/week02/lab.html.",
    "crumbs": [
      "Week02",
      "Now it's your turn"
    ]
  },
  {
    "objectID": "week02/dataskills.html",
    "href": "week02/dataskills.html",
    "title": "Expectations",
    "section": "",
    "text": "stuff",
    "crumbs": [
      "Week02",
      "Expectations"
    ]
  },
  {
    "objectID": "week02/dataskills.html#what-we-expect-from-you",
    "href": "week02/dataskills.html#what-we-expect-from-you",
    "title": "Expectations",
    "section": "",
    "text": "stuff",
    "crumbs": [
      "Week02",
      "Expectations"
    ]
  },
  {
    "objectID": "week02/dataskills.html#what-you-can-expect-from-us",
    "href": "week02/dataskills.html#what-you-can-expect-from-us",
    "title": "Expectations",
    "section": "What you can expect from us!",
    "text": "What you can expect from us!",
    "crumbs": [
      "Week02",
      "Expectations"
    ]
  },
  {
    "objectID": "week02/lab-slides.html",
    "href": "week02/lab-slides.html",
    "title": "Lab 02 - Overview",
    "section": "",
    "text": "Learning Outcomes:\n\nUnderstand different concepts of wellbeing\nApply wellbeing measurement tools\nCritically assess research on wellbeing and happiness",
    "crumbs": [
      "Week02",
      "Lab 02 - Overview"
    ]
  },
  {
    "objectID": "week02/lab-slides.html#thank-you",
    "href": "week02/lab-slides.html#thank-you",
    "title": "Lab 02 - Overview",
    "section": "Thank You!",
    "text": "Thank You!\n\nReady to start the challenge?\nFor DataSkills, your group will submit a short:\n\nA key reference (Journal Article or Data)\nA preferred measurement approach (with positive and negatives)\nA potential intervention plan to increase wellbeing\nWhat success would look like and whether you think you will achieve it",
    "crumbs": [
      "Week02",
      "Lab 02 - Overview"
    ]
  },
  {
    "objectID": "week02/chapter.html",
    "href": "week02/chapter.html",
    "title": "Measuring Wellbeing",
    "section": "",
    "text": "ReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{untitled,\n  author = {},\n  title = {Measuring {Wellbeing}},\n  url = {http://introrm.littlemonkeylab.com/week02/chapter.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Measuring Wellbeing.” n.d. http://introrm.littlemonkeylab.com/week02/chapter.html.",
    "crumbs": [
      "Week02",
      "Measuring Wellbeing"
    ]
  },
  {
    "objectID": "week02/lecture-slides.html",
    "href": "week02/lecture-slides.html",
    "title": "Lecture 01",
    "section": "",
    "text": "Turn off alarm\nGet out of bed\n\n\n\n\n\nEat eggs\nDrink coffee",
    "crumbs": [
      "Week02",
      "Lecture 01"
    ]
  },
  {
    "objectID": "week02/lecture-slides.html#getting-up",
    "href": "week02/lecture-slides.html#getting-up",
    "title": "Lecture 01",
    "section": "",
    "text": "Turn off alarm\nGet out of bed",
    "crumbs": [
      "Week02",
      "Lecture 01"
    ]
  },
  {
    "objectID": "week02/lecture-slides.html#breakfast",
    "href": "week02/lecture-slides.html#breakfast",
    "title": "Lecture 01",
    "section": "",
    "text": "Eat eggs\nDrink coffee",
    "crumbs": [
      "Week02",
      "Lecture 01"
    ]
  },
  {
    "objectID": "week02/lecture-slides.html#dinner",
    "href": "week02/lecture-slides.html#dinner",
    "title": "Lecture 01",
    "section": "Dinner",
    "text": "Dinner\n\nEat spaghetti\nDrink wine",
    "crumbs": [
      "Week02",
      "Lecture 01"
    ]
  },
  {
    "objectID": "week02/lecture-slides.html#going-to-sleep",
    "href": "week02/lecture-slides.html#going-to-sleep",
    "title": "Lecture 01",
    "section": "Going to sleep",
    "text": "Going to sleep\n\nGet in bed\nCount sheep",
    "crumbs": [
      "Week02",
      "Lecture 01"
    ]
  }
]